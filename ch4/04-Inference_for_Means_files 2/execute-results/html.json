{
  "hash": "2d25aab75851b02a2abeed682db7b11c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Inference for Mean Vector'\nauthor: 'Pulong Ma'\nformat: \n  html:\n    html-math-method: katex\n    include-in-header:\n      - text: |\n          <script>\n            window.MathJax = {\n              loader: {\n                load: ['[tex]/amsmath', '[tex]/amssymb', '[tex]/amsfonts'],\n                '[tex]/color']\n              },\n              tex: {\n                packages: { '[+]': ['amsmath', 'amssymb', 'amsfonts', 'color'] }\n              }\n            };\n          </script>\n    css: custom.css\n    toc: true\n    toc_depth: 3\n    number-sections: true\n    code-fold: true\n    callout-appearance: default\n    df-print: paged\n    theme: cosmo \nexecute:\n  echo: true\n  warning: false\n  message: false\n---\n\n\n\n\n\n\n\n\n\n\n\n## Motivation\n\nLet's look at the famous `iris` dataset. We'll focus on the `setosa` species and two variables: `Sepal Length` and `Sepal Width`.\n\nA biologist might ask: \"Is the mean vector of sepal measurements for `setosa` flowers equal to a specific standard, say $\\boldsymbol{\\mu}_0 = [5.1, 3.6]'$?\"\n\n::: {.callout-tip title=\"Iris Data\" collapse=\"true\"}\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(DescTools) # For one-sample T2 test\nlibrary(ellipse)   # For plotting confidence ellipses\n\n# Isolate the setosa data for the first two variables\nsetosa_data <- iris[iris$Species == \"setosa\", 1:2]\nnames(setosa_data) <- c(\"Sepal.Length\", \"Sepal.Width\")\nhead(setosa_data)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"Sepal.Length\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Sepal.Width\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"5.1\",\"2\":\"3.5\",\"_rn_\":\"1\"},{\"1\":\"4.9\",\"2\":\"3.0\",\"_rn_\":\"2\"},{\"1\":\"4.7\",\"2\":\"3.2\",\"_rn_\":\"3\"},{\"1\":\"4.6\",\"2\":\"3.1\",\"_rn_\":\"4\"},{\"1\":\"5.0\",\"2\":\"3.6\",\"_rn_\":\"5\"},{\"1\":\"5.4\",\"2\":\"3.9\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# The hypothesized mean vector\nmu_0 <- c(5.1, 3.6)\n\n# Plot the data\nggplot(setosa_data, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point(color = \"purple\", alpha = 0.7) +\n  geom_vline(xintercept = mu_0[1], linetype = \"dashed\", color = \"red\") +\n  geom_hline(yintercept = mu_0[2], linetype = \"dashed\", color = \"red\") +\n  annotate(\"point\", x = mu_0[1], y = mu_0[2], color = \"red\", size = 5, shape = 4, stroke = 1.5) +\n  labs(\n    title = \"Sepal Measurements for Iris Setosa\",\n    subtitle = \"Red crosshairs show the hypothesized mean vector [5.1, 3.6]\",\n    x = \"Sepal Length (cm)\",\n    y = \"Sepal Width (cm)\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](04-Inference_for_Means_files/figure-html/setup-1.png){width=672}\n:::\n:::\n\n\n\n\n\n:::\n\n\n### Why Not Just Run Two t-tests?\n\nThe \"naive\" approach is to test each variable separately.\n\n1.  Test $H_0: \\mu_{\\text{Length}} = 5.1 \\quad \\text{v.s.} \\quad H_1: \\mu_{\\text{Length}} \\neq 5.1$\n2.  Test $H_0: \\mu_{\\text{Width}} = 3.6 \\quad \\text{v.s.} \\quad H_1: \\mu_{\\text{Width}} \\neq 3.6$\n\n::: {.callout-tip title=\"R code: one-sample $t$ test\" collapse=\"true\"}\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Test 1: Sepal Length\nt.test(setosa_data$Sepal.Length, mu = mu_0[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  setosa_data$Sepal.Length\nt = -1.8857, df = 49, p-value = 0.06527\nalternative hypothesis: true mean is not equal to 5.1\n95 percent confidence interval:\n 4.905824 5.106176\nsample estimates:\nmean of x \n    5.006 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Test 2: Sepal Width\nt.test(setosa_data$Sepal.Width, mu = mu_0[2])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  setosa_data$Sepal.Width\nt = -3.2085, df = 49, p-value = 0.002354\nalternative hypothesis: true mean is not equal to 3.6\n95 percent confidence interval:\n 3.320271 3.535729\nsample estimates:\nmean of x \n    3.428 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n:::\n\n\n**Individually, we would reject both null hypotheses at $\\alpha = 0.05$. But this is misleading!**\n\n\n::: {.callout-important title=\"Why do we need a multivariate test?\" collapse=\"true\"}\n\n1.  **The univariate approach doesn't answer the right question.** The research question is about the **mean vector** $\\boldsymbol{\\mu}$, not the individual means. The two t-tests do not provide a single measure of evidence (one p-value) against the single hypothesis $H_0: \\boldsymbol{\\mu} = \\boldsymbol{\\mu}_0$.\n\n2.  **It ignores the data's structure.** Sepal length and petal length are correlated. Taller flowers tend to have longer petals. The separate t-tests treat the variables as independent, which they are not. This ignores crucial information about the joint variability (or covariance) of the data.\n\n3.  **It cannot quantify the joint uncertainty.** The true \"distance\" of the sample mean vector from the hypothesized mean vector can only be properly measured by accounting for the covariance between variables. We need a method that can create a single **confidence region** (an ellipse) for the mean vector, which is impossible with two separate confidence intervals. \n\nThe multivariate approach provides a single, unified test for a single, unified hypothesis, while properly accounting for the relationships between the variables.\n\n:::\n\n\n### The Family-Wise Error Rate Problem\n\nIf we conduct $p$ tests, each at a significance level $\\alpha$, the probability of making *at least one* Type I error (a false positive) skyrockets.\n\n$$ \n\\text{Pr}(\\text{rejecting at least one} \\, H_0: \\mu_j = \\mu_{0j} \\, | \\, \\text{all}\\, H_0\\text{'s are true})= 1 - (1 - \\alpha)^p $$\n\n* For our $p=2$ case with $\\alpha=0.05$, the true error rate is $1 - (1-0.05)^2 = 0.0975$, nearly double our intended $\\alpha$!\n* This approach is very conservative (i.e., tends to reject more null hypotheses than we should).\n* **More importantly, this univariate approach completely ignores the correlation between variables.** Multivariate methods account for this correlation structure. \n\n\n\n\n## Review of Univariate Inference\n\nThe one-sample $t$-test answers a very common question: \"Is the average of my sample significantly different from a known or claimed value?\"\n\nBefore we go multivariate, let's review the one-sample $t$-test for a mean $\\mu$.\n\n:::{.callout-note title=\"$t$-Test\"}\n* **Hypotheses:** $H_0: \\mu = \\mu_0$ \\text{v.s.} $H_1: \\mu \\neq \\mu_0$\n* **Test Statistic:**\n    $$ t = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} \\sim t_{n-1} $$\n* **Confidence Interval:** A $100(1-\\alpha)\\%$ CI for $\\mu$ is:\n    $$ \\bar{x} \\pm t_{n-1, 1-\\alpha/2} \\frac{s}{\\sqrt{n}} $$\n\n:::\n\n:::{.callout-note title=\"Decision approach\" collapse=\"true\"}\nTo make the decision, we could either use the rejection region approach or the p-value approach. \n\n* **Rejection Region**\n\n- If $|t|> |t_{n-1,1-\\alpha/2}|$, we should reject $H_0$ at significance level $\\alpha$; otherwise, we fail to reject $H_0$ at significance level $\\alpha$ and conclude there is no sufficient evidence to detect the difference.\n\n* **P-Value**\n\n- The p-value is the probability of seeing a result as extreme as yours (or more extreme) if the claim (in null hypothesis) were actually true.\n\n- Small p-value (typically < 0.05): This result is very unlikely to happen by random chance alone. We reject the null hypothesis (H_0) and conclude there's a significant difference at level $\\alpha$ (e.g., 0.05).\n- Large p-value (typically â‰¥ 0.05): This result is reasonably likely to happen by random chance. The difference you saw could just be noise. We fail to reject the null hypothesis at level $\\alpha$ (e.g., 0.05). There is no sufficient evidence from the data to indicate the null hypothesis is wrong. \n\n:::\n\n::: {.callout-note title=\"Example: Coffee Roaster\" collapse=\"true\"}\n\nLet's test the coffee roaster's claim. We buy 10 bags and weigh them with data given by . We want to test if the true mean weight is different from the claimed 12 ounces. \n\n- Null hypothesis: $H_0$: ______\n- Alternative hypothesis $H_1$: ______\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Our sample data: weights of 10 coffee bags in ounces\ncoffee_weights <- c(11.8, 12.1, 11.4, 11.7, \n                    12.0, 11.6, 11.8, 12.2, \n                    11.5, 11.8)\n\n# The hypothesized mean from the null hypothesis\nmu0 <- 12\n\n# Perform the one-sample t-test\ntest_result <- t.test(x = coffee_weights, mu = mu0)\n\n# Print the results\nprint(test_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  coffee_weights\nt = -2.5959, df = 9, p-value = 0.02893\nalternative hypothesis: true mean is not equal to 12\n95 percent confidence interval:\n 11.607 11.973\nsample estimates:\nmean of x \n    11.79 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n**Interpreting the output**:\n\n1. $t=$-2.596: The sample mean is 2.596 standard errors *below* the hypothesized mean. That's a moderately strong signal. \n2. Conclusion: Since the p-value = 0.029 is less than the significant level $\\alpha=0.05$, we reject the null hypothesis. We conclude that the true mean weight of the coffee bags is significantly different from 12 ounces at level $\\alpha=0.05$.\n3. 95 percent confidence interval: [11.61, 11.97]: We are 95% confident that the true mean weight of all bags from this roaster is between 11.61 and 11.97 ounces. Notice that this interval does not contain 12, which confirms our decision to reject the null hypothesis.\n\n::: \n\n\n\n## Generalizing to the Multivariate Case\n\nWe upgrade our tools from scalars to vectors and matrices.\n\n* The **sample mean vector** $\\bar{\\mathbf{x}}$ is a vector of the individual sample means.\n    $$ \\bar{\\mathbf{x}} = \\begin{bmatrix} \\bar{x}_1 \\\\ \\vdots \\\\ \\bar{x}_p \\end{bmatrix} $$\n* The **sample covariance matrix** $\\mathbf{S}$ contains the sample variances on its diagonal and the sample covariances on its off-diagonals.\n    $$ \\mathbf{S} = \\frac{1}{n-1} \\sum_{j=1}^{n} (\\mathbf{x}_j - \\bar{\\mathbf{x}})(\\mathbf{x}_j - \\bar{\\mathbf{x}})^\\top \n    $$\n\n\n::: {.callout-tip title=\"R Code\" collapse=\"true\"}\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample size\nn <- nrow(setosa_data)\n# Number of variables\np <- ncol(setosa_data)\n\n# Sample mean vector\nx_bar <- colMeans(setosa_data)\ncat(\"Sample Mean Vector (x_bar):\", x_bar)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSample Mean Vector (x_bar): 5.006 3.428\n```\n\n\n:::\n\n```{.r .cell-code}\n# Sample covariance matrix\nS <- cov(setosa_data)\ncat(\"Sample Covariance Matrix (S):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSample Covariance Matrix (S):\n```\n\n\n:::\n\n```{.r .cell-code}\nS\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Sepal.Length Sepal.Width\nSepal.Length   0.12424898  0.09921633\nSepal.Width    0.09921633  0.14368980\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n:::\n\n\n\n## Hotelling's $T^2$ Test\n\nThis is the multivariate workhorse, analogous to the squared $t$-statistic.\n\n### Hotelling's $T^2$ Statistic\nHotelling's $T^2$ statistic measures the \"distance\" between the sample mean vector $\\bar{\\mathbf{x}}$ and the hypothesized mean vector $\\boldsymbol{\\mu}_0$, accounting for sample size and covariance.\n\n$$ T^2 = n(\\bar{\\mathbf{x}} - \\boldsymbol{\\mu}_0)^\\top \\mathbf{S}^{-1} (\\bar{\\mathbf{x}} - \\boldsymbol{\\mu}_0) $$\n\n### The $F$-Distribution Connection\nThe $T^2$ statistic is not directly used. Instead, it is rescaled to a familiar $F$-statistic. Under $H_0: \\boldsymbol{\\mu} = \\boldsymbol{\\mu}_0$,\n\n$$ \\frac{n-p}{p(n-1)} T^2 \\, \\overset{H_0}\\sim\\, F_{p, n-p} $$\n\nWe reject $H_0$ if our calculated F-statistic (or scaled $T^2$ statistic) is greater than the critical value $F_{p, n-p, 1-\\alpha}$ at significance level $\\alpha$. \n\n### Assumptions\n1.  The observations $\\mathbf{x}_1, \\dots, \\mathbf{x}_n$ are independently drawn from the same population distribution.\n2.  The population distribution is multivariate normal (MVN):  $\\mathbf{x}_i \\overset{iid}{\\sim} N_p(\\boldsymbol{\\mu}, {\\Sigma}), i=1,\\ldots, n$.\n\n### Example: Sweat Data\nA study measured sweat rate, sodium content, and potassium content for 20 healthy females. Let's test if the mean vector is $\\boldsymbol{\\mu}_0 = [4, 50, 10]^\\top$.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# sweat data\nsweat <- data.frame(\n    Rate = c(3.7, 5.7, 3.8, 3.2, 3.1, 4.6, 6.2, 3.4, 4.1, 5.5, 6.5, 4.5, \n             3.9, 4.5, 6.2, 4.1, 5.5, 6.0, 5.2, 4.8),\n    Sodium = c(48.5, 65.1, 47.2, 31.1, 59.8, 37.8, 52.8, 43.2, 45.1,\n               50.3, 58.3, 40.2, 38.9, 48.8, 60.1, 44.5, 55.5, 59.9, \n               57.7, 51.0),\n    Potassium = c(9.3, 11.1, 9.6, 9.8, 8.0, 10.9, 12.2, 8.5, 9.2, \n                  10.4, 11.2, 9.7, 8.8, 10.1, 12.3, 9.5, 11.3, 12.0,\n                  10.8, 10.5)\n)\nmu0_sweat <- c(4, 50, 10)\nnames(sweat) = c(\"x1\", \"x2\", \"x3\")\n```\n:::\n\n\n\n\n\n\n::: {.callout-tip title=\"Step 1: Normality Check\" collapse=\"true\"}\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Q-Q plot\npar(mfrow = c(1, 3))\n\n# Loop through the first 3 column names and create a plot for each\nfor (col_name in colnames(sweat)[1:3]) {\n  qqnorm(sweat[[col_name]], main = col_name)\n  qqline(sweat[[col_name]], col = \"red\", lwd = 2)\n}\n```\n\n::: {.cell-output-display}\n![](04-Inference_for_Means_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\n## Compute Shapiro-Wilk statistic to test normality for each variable\nsapply(colnames(sweat[ ,1:3]), function(x) {\n               shapiro.test(sweat[[x]]) } )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          x1                            x2                           \nstatistic 0.9483909                     0.9757974                    \np.value   0.3433242                     0.8692117                    \nmethod    \"Shapiro-Wilk normality test\" \"Shapiro-Wilk normality test\"\ndata.name \"sweat[[x]]\"                  \"sweat[[x]]\"                 \n          x3                           \nstatistic 0.9759253                    \np.value   0.8714673                    \nmethod    \"Shapiro-Wilk normality test\"\ndata.name \"sweat[[x]]\"                 \n```\n\n\n:::\n\n```{.r .cell-code}\n## Perform multivariate normality test\nlibrary(mvShapiroTest)\nmvShapiro.Test(as.matrix(sweat[ , 1:3]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tGeneralized Shapiro-Wilk test for Multivariate Normality by\n\tVillasenor-Alva and Gonzalez-Estrada\n\ndata:  as.matrix(sweat[, 1:3])\nMVW = 0.96404, p-value = 0.7893\n```\n\n\n:::\n:::\n\n\n\n\n\n\n:::\n\n::: {.callout-tip title=\"Step 2: Calculate Sample Statistics\" collapse=\"true\"}\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Manual Calculation\nn_sweat <- nrow(sweat)\np_sweat <- ncol(sweat)\nx_bar_sweat <- colMeans(sweat)\nS_sweat <- cov(sweat)\nS_inv_sweat <- solve(S_sweat)\n```\n:::\n\n\n\n\n\n\n:::\n\n\n::: {.callout-tip title=\"Step 3: Compute Hotelling's $T^2$ Statistic\" collapse=\"true\"}\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# manual calculation \ndiff_vec <- x_bar_sweat - mu0_sweat\nT2_manual <- n_sweat * t(diff_vec) %*% S_inv_sweat %*% diff_vec\ncat(\"Manually calculated T-squared:\", as.numeric(T2_manual), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nManually calculated T-squared: 43.50518 \n```\n\n\n:::\n\n```{.r .cell-code}\n# 2. Convert to F-statistic or scaled T-square statistic \nF_stat <- as.numeric(T2_manual) * (n_sweat - p_sweat) / \n  (p_sweat * (n_sweat - 1))\ncat(\"Calculated F-statistic:\", F_stat, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCalculated F-statistic: 12.97523 \n```\n\n\n:::\n\n```{.r .cell-code}\n# 3. Find critical value\nalpha <- 0.05\nF_crit <- qf(1 - alpha, p_sweat, n_sweat - p_sweat)\ncat(\"Critical F-value:\", F_crit, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCritical F-value: 3.196777 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Decision:\", \n    ifelse(F_stat > F_crit, \"Reject H0\", \"Do not reject H0\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDecision: Reject H0 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n**Conclusion:** Since our F-statistic (12.98) is much larger than the critical F-value (3.197), we **reject the null hypothesis** at level $\\alpha=0.05$. The mean sweat composition is significantly different from $[4, 50, 10]^\\top$.\n\n:::\n\n::: {.callout-note title=\"Hotelling's $T^2$ Test\"}\nThe manual calculations in **Step 2-3** above can be completed using R package `DescTools` after normality check. If the data follows a multivariate normal distribution, then we can perform the Hotelling's $T^2$ test; if the normality assumption is violated, the conclusion is misleading and this limitation should be acknowledged. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"R code for Hotelling's T2 test\"}\n# install.packages(\"DescTools\")\nlibrary(DescTools)\nht_sweat <- HotellingsT2Test(x=sweat, mu = mu0_sweat)\nprint(ht_sweat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tHotelling's one sample T2-test\n\ndata:  sweat\nT.2 = 12.975, df1 = 3, df2 = 17, p-value = 0.0001177\nalternative hypothesis: true location is not equal to c(4,50,10)\n```\n\n\n:::\n:::\n\n\n\n\n\n\n**Note**: At this point we do not know which of the two hypothesized mean values is not supported by the data. \n\n:::\n\n:::{.callout-warning title=\"R Function `HotellingsT2Test`\"}\nWhen the function `HotellingsT2Test` in the  R package `DescTools` is used to perform the test, its output reports the **scaled** $T^2$ statistic (or $F$ statistic) named *T.2* (which is 12.975 in this example). This scaled $T^2$ statistic should not be confused with the $T^2$ statistic as the scaled $T^2$ statistic is $\\frac{n-p}{p(n-1)}T^2$. \n::: \n\n\n\n\n### Exercise: `Iris` data\n**Background**\n\nRonald Fisher's `iris` dataset is a cornerstone of statistical analysis. Imagine a historical botanical guide from the 1930s describes the \"type specimen\" for the *Iris setosa* species as having a mean **Sepal Length** of **5.1 cm** and a mean **Sepal Width** of **3.6 cm**.\n\nYour task is to determine if the sample of 50 *Iris setosa* flowers from Fisher's dataset is consistent with this historical description. You will use a Hotelling's TÂ² test with a significance level of $\\alpha = 0.05$. \n\n\n\n**1. State the Hypotheses**\n\nWrite the **null hypothesis ($H_0$)** and the **alternative hypothesis ($H_1$)** for this test. Let $\\boldsymbol{\\mu}$ represent the true mean vector of `[Sepal.Length, Sepal.Width]`.\n\n::: {.callout-tip title=\"View Solution\" collapse=\"true\"}\nThe hypotheses are formulated as:\n\n* **Null Hypothesis ($H_0$)**: The true mean sepal measurements are equal to the historical description.\n    $$H_0: \\boldsymbol{\\mu} = \\begin{bmatrix} 5.1 \\\\ 3.6 \\end{bmatrix}$$\n\n* **Alternative Hypothesis ($H_1$)**: The true mean sepal measurements are not equal to the historical description.\n    $$H_1: \\boldsymbol{\\mu} \\neq \\begin{bmatrix} 5.1 \\\\ 3.6 \\end{bmatrix}$$\n:::\n\n\n\n**2. Prepare and Visualize the Data**\n\nLoad the necessary R packages. From the built-in `iris` dataset, create a final data frame containing only the `Sepal.Length` and `Sepal.Width` for the `setosa` species.\n\n::: {.callout-tip title=\"View Solution\" collapse=\"true\"}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"R Code: Data Preparation\"}\n# Load the packages needed for the entire analysis\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Create the final data frame for analysis\nsetosa_data <- iris %>%\n  filter(Species == \"setosa\") %>%\n  dplyr::select(Sepal.Length, Sepal.Width)\n\ndf = setosa_data\n# Display the first few rows of the prepared data\nhead(df)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"Sepal.Length\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Sepal.Width\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"5.1\",\"2\":\"3.5\",\"_rn_\":\"1\"},{\"1\":\"4.9\",\"2\":\"3.0\",\"_rn_\":\"2\"},{\"1\":\"4.7\",\"2\":\"3.2\",\"_rn_\":\"3\"},{\"1\":\"4.6\",\"2\":\"3.1\",\"_rn_\":\"4\"},{\"1\":\"5.0\",\"2\":\"3.6\",\"_rn_\":\"5\"},{\"1\":\"5.4\",\"2\":\"3.9\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code  code-summary=\"R Code: Data Preparation\"}\npairs(df)\n```\n\n::: {.cell-output-display}\n![](04-Inference_for_Means_files/figure-html/data-prep-iris-final-1.png){width=672}\n:::\n:::\n\n\n\n\n\n:::\n\n\n\n**3. Perform the Statistical Test**\n\nRun a one-sample Hotelling's T$^2$ test on your prepared data using the historical description as your hypothesized mean vector.\n\n::: {.callout-tip title=\"View Solution\" collapse=\"true\"}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"R Code: Hotelling's T-squared Test\"}\n# Define the hypothesized mean vector from the historical guide\nhypothesized_mean <- c(5.1, 3.6)\n\n# Perform the test\ntest_result <- DescTools::HotellingsT2Test(\n  x = setosa_data, \n  mu = hypothesized_mean)\n\n# Print the results\nprint(test_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tHotelling's one sample T2-test\n\ndata:  setosa_data\nT.2 = 5.3116, df1 = 2, df2 = 48, p-value = 0.008244\nalternative hypothesis: true location is not equal to c(5.1,3.6)\n```\n\n\n:::\n:::\n\n\n\n\n\n:::\n\n\n\n**4. Interpret the Results**\n\nWhat is the p-value from your test? Based on this and $\\alpha = 0.05$, do you reject or fail to reject the null hypothesis? State your conclusion in the context of the problem.\n\n::: {.callout-tip title=\"View Solution\" collapse=\"true\"}\nThe p-value is 0.008244. Since this p-value is less than our significance level of $\\alpha = 0.05$, we **should reject the null hypothesis at level 0.05**.\n\n**Conclusion:** There is enough statistical evidence to conclude that the mean sepal measurements of the `setosa` flowers in Fisher's dataset are different from the historical description. The benchmark of 5.1 cm length and 4.2 cm width is not a plausible value for the true mean of this sample.\n:::\n\n\n\n**5. Visualize Your Conclusion**\n\nCreate a scatter plot of `Sepal.Width` v.s. `Sepal.Length`. On the plot, mark the sample mean, the hypothesized mean, and add the 95% confidence ellipse. Explain how it supports your conclusion.\n\n::: {.callout-tip title=\"View Solution\" collapse=\"true\"}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"R Code: Visualization\"}\n# Calculate the sample mean and sample covariance \nx_bar = colMeans(df)\nS = cov(df)\n\n# Calculate points for the 95% confidence ellipse\nalpha=0.05\nFvalue <- sqrt(p * (n - 1) / (n - p) * qf(1 - alpha, p, n - p))\n\nconfidence_ellipse <- as.data.frame(ellipse::ellipse(\n  S,\n  centre = x_bar,\n  level = 0.95,\n  t = Fvalue / sqrt(n)\n))\n\n# Create the plot\nggplot(df, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point(color = \"purple\",\n             alpha = 0.6,\n             size = 2.5) +\n  geom_path(\n    data = confidence_ellipse,\n    aes(x = Sepal.Length, y = Sepal.Width),\n    color = \"blue\",\n    linewidth = 1,\n    inherit.aes = FALSE\n  ) +\n  annotate(\n    \"point\",\n    x = x_bar[1],\n    y = x_bar[2],\n    color = \"blue\",\n    size = 5\n  ) +\n  annotate(\n    \"text\",\n    x = x_bar[1],\n    y = x_bar[2] + 0.03,\n    label = \"Sample Mean\",\n    color = \"blue\"\n  ) +\n  annotate(\n    \"point\",\n    x = hypothesized_mean[1],\n    y = hypothesized_mean[2],\n    color = \"red\",\n    size = 5,\n    shape = 4,\n    stroke = 1.5\n  ) +\n  annotate(\n    \"text\",\n    x = hypothesized_mean[1],\n    y = hypothesized_mean[2] - 0.03,\n    label = \"Historical Description\",\n    color = \"red\"\n  ) +\n  labs(\n    title = \"Iris Setosa Sepal Measurements\",\n    subtitle = \"Hotelling's TÂ² test vs. Historical Description\",\n    x = \"Sepal Length (cm)\",\n    y = \"Sepal Width (cm)\"\n  ) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![Comparison of the Iris setosa sample to a historical description.](04-Inference_for_Means_files/figure-html/visualize-result-iris-final-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n**Visual Interpretation:** The plot confirms the test result. The red cross (the historical description) lies **outside** the blue 95% confidence ellipse. Since the hypothesized value is outside the confidence region of plausible values for the true mean, it visually supports our conclusion to  reject the null hypothesis.\n\n:::\n\n\n\n\n## Confidence Regions for $\\boldsymbol{\\mu}$\n\nA confidence interval for a single mean becomes a **confidence region** (an ellipse for $p=2$, an ellipsoid for $p>2$) for a mean vector. This confidence region is also known as **joint confidence**. \n\nA $100(1-\\alpha)\\%$ confidence region for $\\boldsymbol{\\mu}$ is the set of all vectors $\\boldsymbol \\mu$ satisfying:\n$$ n(\\bar{\\mathbf{x}} - \\boldsymbol{\\mu})^\\top {S}^{-1} (\\bar{\\mathbf{x}} - \\boldsymbol{\\mu}) \\le \\frac{p(n-1)}{n-p} F_{p, n-p, 1-\\alpha} $$\n\n\n:::{.callout-note title=\"Confidence Region\"}\nThe confidence region forms an ellipsoid. Its shape and orientation are determined by the eigenvalues and eigenvectors of the sample covariance matrix ${S}$. \n\n* The center of the ellipsoid is $\\bar{\\mathbf{x}}$. \n* The distance from the center of the ellipsoid to the edge of the ellipsoid along the $i$-th axis is\n$$\n\\pm \\sqrt{\\lambda_i} \\sqrt{\\frac{(n-1)p}{n(n-p)}F_{(p, n-p), 1-\\alpha}}.\n$$\n* The directions of the eigenvectors and sizes of the eigenvalues depend on \n  - the relative sizes of the variances of the measured variables\n  - the sizes of the correlations between pairs of variables\n  \n:::\n\n### Plot Confidence Ellipse ($p=2$)\nLet's plot the 95% confidence ellipse for our `setosa` variable\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![95% confidence ellipse for the true mean vector of Setosa sepal measurements. The sample mean is the blue dot, and the hypothesized mean is the red cross.](04-Inference_for_Means_files/figure-html/plot-ellipse-1.png){width=672}\n:::\n:::\n\n\n\n\n\nOur hypothesized mean (red cross) falls outside the 95% confidence ellipse, which is consistent with rejecting the null hypothesis in the Hotelling's $T^2$ test.\n\n\n\n## Simultaneous Confidence Intervals\n\nIf we reject $H_0$, we want to know *which* variables contributed to the rejection. We need CIs that hold simultaneously for all $p$ variables with confidence level at least $1-\\alpha$.\n\n### $T^2$ CIs\nThe $T^2$ confidence intervals are derived from the Hotelling's $T^2$ statistic. For each component $\\mu_i$:\n$$ \\bar{x}_i \\pm \\sqrt{\\frac{p(n-1)}{n-p} F_{p, n-p, 1-\\alpha}} \\sqrt{\\frac{s_{ii}}{n}} $$\nwhere $s_{ii}$ is the $(i,i)$-th entry of ${S}$.\n\n### Bonferroni CIs\nAnother way to construct individual confidence intervals for each $\\mu_i$ is to use the $t$-interval, but the combined set of individual $t$ intervals result in a simulatenous confidence level that is ***less than*** than the normal level $1-\\alpha$. \n\nTo avoid this limitation, one can use the so-called Bonferroni confidence intervals:\n$$ \\bar{x}_i \\pm t_{n-1, 1- \\alpha/(2p)} \\sqrt{\\frac{s_{ii}}{n}} $$\nwhere $p$ is the number of variables to be estimated. \n\n:::{.callout-note title=\"One-At-a-Time $t$ CI\"}\nUsing the univariate approach, we can construct\n$t$-intervals for each of mean differences, and obtain the so-called ***one-at-a-time*** $t$ intervals:\n$$ \n\\bar{x}_i \\pm t_{n-1, 1- \\alpha/2} \\sqrt{\\frac{s_{ii}}{n}} \n$$\nThe one-at-a-time CI does not control the family-wise error at $\\alpha$ or the confidence level at least at the nominal level $1-\\alpha$. Bonferroni CI corrects the one-at-a-time CI and produces a set of CIs that jointly have the confidence level at least $1-\\alpha$. \n\n:::\n\n### Comparison of Confidence Region, $T^2$ CI and Bonferroni CI\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![95% joint confidence region (ellipse) with rectangles for T-squared intervals (red shaped area) and Bonferroni simultaneous intervals (purple shaped area).](04-Inference_for_Means_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n### Example: Microwave Oven Data\nManufacturers of a microwave oven are concerned about radiation emission. They measure radiation with the door closed and open from $n=12$ ovens. Test if the mean radiation levels are $\\boldsymbol{\\mu}_0 = [0.15, 0.20]'$.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Generate some plausible data\nset.seed(4750)\noven_data <- data.frame(\n  Closed = rnorm(12, mean = 0.25, sd = 0.08),\n  Open = rnorm(12, mean = 0.30, sd = 0.10)\n)\ndat = oven_data\nmu0 <- c(0.15, 0.20)\n```\n:::\n\n\n\n\n\n\n\n:::{.callout-tip title=\"State The Hypotheses\" collapse=\"true\"}\nLet $\\boldsymbol \\mu = (\\mu_1, \\mu_2)^\\top$ represents the mean radiation levels corresponding to door closed and door open conditions. The historical standard or hypothesized mean vector is $\\boldsymbol \\mu_0$. The hypotheses are \n$$\nH_0: \\boldsymbol \\mu  = \\boldsymbol \\mu_0 \\quad \\text{v.s.} \\quad H_1: \\boldsymbol \\mu \\neq \\boldsymbol \\mu_0\n$$\n:::\n\n:::{.callout-tip title=\"Compute Test Statistic\" collapse=\"true\"}\n\nIf we do a manual calculation, we need to compute the Hotelling's $T^2$ test\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"R Code: T$^2$ Statistic\"}\nxbar = colMeans(dat)\nS = cov(as.matrix(dat))\nn = nrow(dat)\np = ncol(dat)\nT2 = n*t(xbar-mu0)%*%solve(S)%*%(xbar-mu0)\nscaledT2 = (n-p)/(p*(n-1))*drop(T2)\nF_crit =  qf(0.95, p, n-p)\nif(scaledT2>F_crit){\n  cat(paste0(\" scaled T2=\", signif(scaledT2,3)),\">\", \n      paste0(\"F critical value=\", signif(F_crit,3)), \n      \", thus we reject H0 at level 0.05\")\n}else{\n  cat(paste0(\" scaled T2=\", signif(scaledT2,3)),\"<\", \n      paste0(\"F critical value=\", signif(F_crit,3)), \n      \", thus we fail to reject H0 at level 0.05\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n scaled T2=13.5 > F critical value=4.1 , thus we reject H0 at level 0.05\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"R Code: Use `HotellingsT2Test`\"}\n# First, run the T2 test\nT2result <- HotellingsT2Test(x=dat, mu = mu0)\nprint(T2result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tHotelling's one sample T2-test\n\ndata:  dat\nT.2 = 13.464, df1 = 2, df2 = 10, p-value = 0.001456\nalternative hypothesis: true location is not equal to c(0.15,0.2)\n```\n\n\n:::\n:::\n\n\n\n\n\n\n:::\n\n:::{.callout-tip title=\"Simultaneous $T^2$ CIs\" collapse=\"true\"}\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"R Code: Simultaneous T2 CIs\"}\n# The p-value is tiny, so we reject H0. Let's find out why.\n\ns_ii <- diag(S)\n\n# T2-based intervals\nT2_multiplier <- sqrt(p * (n - 1) / (n - p) * F_crit)\nT2_margins <- T2_multiplier * sqrt(s_ii / n)\nT2_intervals <- data.frame(\n  Variable = names(oven_data),\n  Lower = xbar - T2_margins,\n  Upper = xbar + T2_margins\n)\nprint(\"95% T-squared Simultaneous CIs:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"95% T-squared Simultaneous CIs:\"\n```\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n       Variable     Lower     Upper\nClosed   Closed 0.1829773 0.2941655\nOpen       Open 0.1688177 0.3802530\n```\n\n\n:::\n:::\n\n\n\n\n\n\n:::\n\n:::{.callout-tip title=\"Simultaneous Bonferroni CIs\" collapse=\"true\"}\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"R Code: Simultaneous Bonferroni CIs\"}\n# Bonferroni-based intervals\nbonf_multiplier <- qt(1 - 0.05 / (2 * p), df = n - 1)\nbonf_margins <- bonf_multiplier * sqrt(s_ii / n)\nbonf_intervals <- data.frame(\n  Variable = names(dat),\n  Lower = xbar - bonf_margins,\n  Upper = xbar + bonf_margins\n)\nprint(\"95% Bonferroni Simultaneous CIs:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"95% Bonferroni Simultaneous CIs:\"\n```\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n       Variable     Lower     Upper\nClosed   Closed 0.1905876 0.2865551\nOpen       Open 0.1832896 0.3657812\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n**Conclusion:** Both methods produce very similar intervals. Neither the interval for \"Closed\" nor \"Open\" contains its hypothesized mean (0.15 and 0.20, respectively). Therefore, we conclude that *both* mean radiation levels are significantly higher than the specified standards. Note that the Bonferroni CIs are slightly narrower (more precise), which is often the case.\n\n:::\n\n\n\n\n\n\n## Inference for  Linear Combinations of Means\n\nIn many scientific applications, one is interested in making statistical inference for a linear combination of the mean components (also called *mean contrast*) based on multivariate data. In such applications, we can test hypotheses about any linear combination of the mean components, $H_0: \\mathbf{c}'\\boldsymbol{\\mu} = c_0$. One can still apply previous methods to perform the test.  This is useful for testing things like \"Is the average of all means equal to 5?\" or \"Is the difference between mean 1 and mean 2 equal to zero?\"\n\n:::{.callout-note title=\"Dogs Anesthetics Study (J. \\& W. 2007)\"}\n\n**Background**\n\nA study was conducted on a sample of 19 dogs to evaluate the effect of CO2 pressure and the anesthetic halothane on heart rate. Each dog was then administered carbon dioxide CO2 at each of two pressure levels. Next, halothane (H) was added and then administration of CO2 was repeated. There was a washout period (several weeks) between the use of one anesthetic and the use of another anesthetic.  The response variable is the time in milliseconds between heartbeats. The data were obtained by measuring the response under four treatment combinations: (1) high CO2 pressure, (2) low CO2 pressure, (3) high pressure + halothane, and (4) low pressure + halothane. \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# load data\ndogdat <- read.table(\"dogs.dat\", \n            col.names=c(\"dog\", \"x1\", \"x2\", \"x3\", \"x4\"))\ndogdat = dogdat %>% mutate(across(c(\"x1\", \"x2\", \"x3\", \"x4\"), as.numeric))\nhead(dogdat)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"dog\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"x1\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"x2\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"x3\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"x4\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"426\",\"3\":\"609\",\"4\":\"556\",\"5\":\"600\",\"_rn_\":\"1\"},{\"1\":\"2\",\"2\":\"253\",\"3\":\"236\",\"4\":\"392\",\"5\":\"395\",\"_rn_\":\"2\"},{\"1\":\"3\",\"2\":\"359\",\"3\":\"433\",\"4\":\"349\",\"5\":\"357\",\"_rn_\":\"3\"},{\"1\":\"4\",\"2\":\"432\",\"3\":\"431\",\"4\":\"522\",\"5\":\"600\",\"_rn_\":\"4\"},{\"1\":\"5\",\"2\":\"405\",\"3\":\"426\",\"4\":\"513\",\"5\":\"513\",\"_rn_\":\"5\"},{\"1\":\"6\",\"2\":\"324\",\"3\":\"438\",\"4\":\"507\",\"5\":\"539\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n\n\n\n:::\n\n:::{.callout-tip title=\"Experimental Setup\"} \n\n  - Design: $p=4$ treatments are assigned to the same experimental unit. In total, there are $n= 19$ experimental units. \n  - Objective: our interest is to formally assess whether there is significant difference between the two corresponding mean responses. \n  - Mean responses: Let $\\mu_j$ denote the mean response under treatment $j$, where $j=1,\\ldots, 4$. $\\boldsymbol \\mu = (\\mu_1, \\ldots, \\mu_p)^\\top$ is a vector of mean responses over all $p$ treatments (high CO2, low CO2, high CO2 + halothane, low CO2 + halothane). \n  - Data:  Let $\\mathbf{x}_i=(x_{i1}, x_{i2}, \\ldots, x_{ip})^\\top$ denote the response variable for the $i$-th experimental unit under all the $p$ treatments, where $i=1, \\ldots, n$.\n\n:::\n\n:::{.callout-tip title=\"Research Questions\" collapse=\"true\"}\n\nAs we want to assess whether there is significant difference between the two corresponding mean responses, we are interested in testing three specific effects simultaneously:\n\n1. Is there a main effect of halothane?\n2. Is there a main effect of CO2?\n3. Is there an interaction effect?\n\n:::\n\n\n::: {.callout-tip title=\"Step 1: Prepare and Visualize the Data\" collapse=\"true\"}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX = as.matrix(dogdat[ ,2:5])\n#  Display a scatterplot matrix\npairs(X, panel=function(x, y, ...){points(x, y, ...)\n          abline(lm(y ~ x), col=\"black\")}, cex=1.5)\n```\n\n::: {.cell-output-display}\n![](04-Inference_for_Means_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n:::\n\n\n### Repeated Measures\n\nA **repeated measures** study is a research design where the same participants are measured multiple times under different conditions or over a period of time\n\n**Key characteristics**\n\n- Same participants across conditions or time points\n- Measurements may be taken:\n  - Across treatments (within-subjects design) â€“ e.g., each participant tries all drug dosages.\n  - Over time (longitudinal design) â€“ e.g., measuring blood pressure weekly for 8 weeks.\n- Correlation between measurements \n\n**Advantages**\n\n- Controls for individual differences â†’ reduces variability and increases statistical power.\n- Fewer participants needed compared to a between-subjects design for the same precision.\n- Ability to track changes within individuals over time or across conditions.\n\n**Common Examples**\n\n- **Before-and-After Studies**: This is the classic setup. You measure a group's cholesterol levels (before), put them on a new diet for three months, and measure their levels again (after).\n- **Comparing Multiple Treatments**: Each participant is given several different drugs (with a washout period in between), and their reaction to each drug is measured. The dog anesthetic example from our previous conversation is a perfect case of this.\n- **Longitudinal Studies**: A researcher tracks the same group of children and measures their reading ability at ages 6, 8, and 10 to see how it develops over time.\n\n**Key Challenge: Dependent Data**\n\nWhen the same subject is measured on $p$ different occasions or under $p$ different conditions, the measurements are **not independent**. A person's score at time 1 is related to their score at time 2. Because of this dependency, we cannot directly apply previous methods on the data. However, we can still analyze this data by creating **difference vectors** and performing a one-sample Hotelling's $T^2$ test on these differences.\n\n\n### Hotelling's $T^2$ Test for Contrasts\n\n\n\n:::{.callout-tip title=\"Step 2: State Hypotheses\" collapse=\"true\"}\nThe research questions can be formulated as a hypothesis testing problem to test treatment effects simultaneously: \n\n1. Halothane Effect: The overall effect of halothane, averaging across CO2 pressure levels: $(\\mu_3+\\mu_4) - (\\mu_1+\\mu_2)$.\n2. CO2 Pressure Effect: The overall effect of CO2 pressure, averaging across halothane levels: $(\\mu_1+\\mu_3) - (\\mu_2+\\mu_4)$\n3. Interaction Effect: Whether the effect of CO2 pressure depends on the presence of halothane: $(\\mu_1-\\mu_2)-(\\mu_3-\\mu_4)$\n\n- **Null hypothesis**: The no treatment effect can be written as testing $H_0: C\\boldsymbol{\\mu}=\\mathbf{0}$ for a $q\\times p$ **contrast matrix** $C$.  \n  - $q$ is the number of contrasts. In this example, $q=3$.  \n- **Alternative hypothesis**:  $H_1: C\\boldsymbol \\mu\\neq \\mathbf{0}$\n\n**Question: How to write down the *contrast matrix* for the null hypothesis?** \n\nIf one is interested in testing the hypothesis that all four treatments have the same mean, then \n\\begin{equation*}\nC=\n\\left[\\begin{array}{rrrr} \n-1 & 1 & 0 & 0 \\\\ 0 & -1 & 1 & 0 \\\\ 0 & 0 & -1 & 1 \n\\end{array}\\right] \n\\end{equation*}\n\n:::\n\n\n:::{.callout-tip title=\"Step 3: Define the Contrast Matrix\" collapse=\"true\"}\n\nThe contrast matrix defined by the null hypotheses above is given by  \n\\begin{equation*}\nC=\n\\begin{bmatrix}\n-1 & -1 & 1 & 1 \\\\\n1 & -1 & 1 & -1 \\\\\n1 & -1 & -1 & 1\n\\end{bmatrix}\n\\end{equation*}\nbecause  \n$$\n\\begin{bmatrix} \n(\\mu_3 + \\mu_4) - (\\mu_1 + \\mu_2) \\\\ \n(\\mu_1 + \\mu_3) - (\\mu_2 + \\mu_4) \\\\\n(\\mu_1 - \\mu_2) - (\\mu_3-\\mu_4)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-1 & -1 & 1 & 1 \\\\\n1 & -1 & 1 & -1 \\\\\n1 & -1 & -1 & 1\n\\end{bmatrix}\n\\begin{bmatrix} \n\\mu_1 \\\\ \n\\mu_2 \\\\ \n\\mu_3 \\\\ \n\\mu_4 \\end{bmatrix}\n= {C} \\boldsymbol{\\mu}\n$$\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nC<-matrix( c(1, -1, 1, -1, -1, -1, 1, 1, -1, 1, 1, -1), \n              nrow=3, ncol=4, byrow=T)\nC\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4]\n[1,]    1   -1    1   -1\n[2,]   -1   -1    1    1\n[3,]   -1    1    1   -1\n```\n\n\n:::\n:::\n\n\n\n\n\n\n:::\n\n:::{.callout-tip title=\"Step 4: Transform the Data Using the Contrast Matrix\" collapse=\"true\"}\n\nFor each dog, apply the contrasts defined in matrix C to their four measurements. This converts the original 4 variables into 3 new contrast variables, which will be the subject of our test.\n\nWe use matrix multiplication to transform our $19\n\\times 4$ data matrix into a $19\n\\times 3$ matrix of contrast scores. The formula is $Y = X  C^\\top$.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Transform the original data into contrast data\nY <-  X %*% t(C)\n\n# Rename columns for clarity\ncolnames(Y) <- c(\"Halothane_Effect\", \"CO2_Effect\", \"Interaction_Effect\")\n\nhead(Y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Halothane_Effect CO2_Effect Interaction_Effect\n[1,]             -227        121                139\n[2,]               14        298                -20\n[3,]              -82        -86                 66\n[4,]              -77        259                -79\n[5,]              -21        195                 21\n[6,]             -146        284                 82\n```\n\n\n:::\n\n```{.r .cell-code}\npairs(Y)\n```\n\n::: {.cell-output-display}\n![](04-Inference_for_Means_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n:::\n\n\n::: {.callout-tip title=\"Step 5: Check Assumptions\" collapse=\"true\"}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(2,2),pch=1)     \nfor (i in 1:3){\n  qqnorm(Y[,i],  main=\"Normal Q-Q Plot\") \n  qqline(Y[,i], col=\"red\", lwd=2)\n}\n\napply(Y,2,shapiro.test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$Halothane_Effect\n\n\tShapiro-Wilk normality test\n\ndata:  newX[, i]\nW = 0.97195, p-value = 0.8146\n\n\n$CO2_Effect\n\n\tShapiro-Wilk normality test\n\ndata:  newX[, i]\nW = 0.80247, p-value = 0.001242\n\n\n$Interaction_Effect\n\n\tShapiro-Wilk normality test\n\ndata:  newX[, i]\nW = 0.96055, p-value = 0.5834\n```\n\n\n:::\n\n```{.r .cell-code}\nmvShapiroTest::mvShapiro.Test(Y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tGeneralized Shapiro-Wilk test for Multivariate Normality by\n\tVillasenor-Alva and Gonzalez-Estrada\n\ndata:  Y\nMVW = 0.92197, p-value = 0.04801\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](04-Inference_for_Means_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n**Conclusion**: While the multivariate normality assumption is violated based on the testing results, we will still perform the hypothesis testing in what follows for pedagogical purpose. In practice, one needs to address this issue before applying the Hotelling's $T^2$ test. \n\n:::\n\n\n\n\n:::{.callout-tip title=\"Step 6: Hotelling's $T^2$ Test\" collapse=\"true\"}\n\n- To test $H_0: C\\boldsymbol \\mu = 0$, we can again use $T^2$ statistic:\n$$\nT^2 = n(C\\bar{\\mathbf{x}} - \\mathbf{0})^\\top (CS^{-1}C^\\top)^{-1} (C\\bar{\\mathbf{x}} - \\mathbf{0})\n$$\n- The null hypothesis is rejected at significance level $\\alpha$ if \n$$\nT^2 \\geq \\frac{(n-1)(p-1)}{(n-p+1)} F_{p-1, n-p+1, 1-\\alpha}\n$$\nwhere the numerator degrees of freedom are $p-1$ instead of $p$ because the null hypotheses puts only $p-1$ constraints on the mean responses. \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the hypothesized mean vector (a zero vector)\nmu0 <- c(0, 0, 0)\n\n# Perform the one-sample test on the contrast data\ntest_result = DescTools::HotellingsT2Test(Y, mu=mu0)\nprint(test_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tHotelling's one sample T2-test\n\ndata:  Y\nT.2 = 34.375, df1 = 3, df2 = 16, p-value = 3.318e-07\nalternative hypothesis: true location is not equal to c(0,0,0)\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n**Conclusion**: We clearly reject the null at significance level $\\alpha=0.05$, so the question now becomes whether there is a difference between CO2 pressure, between halothane levels or perhaps there is no main effect of treatment but there is still an interaction. **This question can be answered by looking at confidence intervals**.\n\n::: \n\n::: {.callout-note title=\"Paired $t$-Test\" collapse=\"true\"}\n\nA **paired t-test** is a special, simplified case of the Hotelling's TÂ² test on contrasts. The powerful, general framework of the F-test on contrasts simplifies to become the familiar paired t-test when you are making only one comparison between two measurements.\n\n**The Paired t-Test: A Simple Contrast**\n\nA paired t-test is designed to answer one question: \"Is there a significant difference between two matched measurements (e.g., before vs. after)?\"\n\n* **The Data:** You have two measurements for each subject, $X_1$ and $X_2$.\n* **The Method:** For each subject, you calculate a single **difference score**, $d = X_1 - X_2$. You then perform a one-sample t-test on these difference scores to see if their mean is significantly different from 0.\n* **The null hypothesis:** $H_0: \\mu_d = 0$, which is the same as $H_0: \\mu_1 - \\mu_2 = 0$.\n\n:::\n\n\n\n\n### Confidence Regions for Linear Combinations of Means\n\nA $100(1-\\alpha)\\%$ confidence region for any linear combination of population means, say $C \\boldsymbol{\\mu}$, is the set of all vectors $\\boldsymbol \\mu$ satisfying:\n$$ \nn(C\\bar{\\mathbf{x}} - C\\boldsymbol{\\mu})^\\top (CSC^\\top)^{-1} (C\\bar{\\mathbf{x}} - C\\boldsymbol{\\mu}) \\le \\frac{(p-1)(n-1)}{n-p+1} F_{p-1, n-p+1, 1-\\alpha} \n$$\n\n\n\n### Simultaneous CIs for Linear Combinations of Means\n\nAfter performing a multivariate test, we are often interested in more complex comparisons than just single means. We might want to test a **linear combination** of the population means, which takes the general form:\n\n$$ \\mathbf{c}'\\boldsymbol{\\mu} = c_1\\mu_1 + c_2\\mu_2 + \\dots + c_p\\mu_p $$\n\nWe can construct simultaneous confidence intervals for a set of `m` such combinations, where the `k`-th combination is defined by the coefficient vector $\\mathbf{c}_k = [c_{k1}, c_{k2}, \\dots, c_{kp}]'$.\n\nThe point estimate for any linear combination $\\mathbf{c}_k'\\boldsymbol{\\mu}$ is given by $\\mathbf{c}_k'\\bar{\\mathbf{x}} = \\sum_{j=1}^p c_{jk} \\mu_j$, and the estimated variance is $\\frac{\\mathbf{c}_k'{S}\\mathbf{c}_k}{n}$. The margin of error is then determined by one of the following methods.\n\n:::{.callout-note title=\"$T^2$ CI\"}\n\nThis method derives its critical value from the F-distribution associated with Hotelling's TÂ² test. The confidence intervals are valid simultaneously for *any and all* possible linear combinations.\n\nA $100(1-\\alpha)\\%$ simultaneous confidence intervals are given by:\n\n$$ \n\\mathbf{c}_k'\\bar{\\mathbf{x}} \\pm \\sqrt{\\frac{p(n-1)}{n-p} F_{p, n-p, 1-\\alpha}} \\sqrt{\\frac{\\mathbf{c}_k'{S}\\mathbf{c}_k}{n}} \\quad \\text{for } k=1, 2, \\dots, m \n$$\n\nIn the dog anesthetics study, the $T^2$ CIs can be computed as follows. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"R Code: T$^2$ CIs\"}\nybar = colMeans(Y)\nSy = cov(Y)  # this is the same as C%*%cov(X)%*%t(C) \nF_crit = qf(1-0.05, p, n-p)\n\ncval_T2 = sqrt(p*(n-1)/(n-p) * F_crit) \nS_kk = diag(Sy)\n\nci_T2 = tibble(\n  Component = names(ybar),\n  Estimate = ybar,\n  Lower = Estimate - cval_T2 * sqrt(S_kk/n), \n  Upper = Estimate + cval_T2 * sqrt(S_kk/n)\n)\nprint(ci_T2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 Ã— 4\n  Component          Estimate  Lower  Upper\n  <chr>                 <dbl>  <dbl>  <dbl>\n1 Halothane_Effect      -60.1 -123.    2.46\n2 CO2_Effect            209.   125.  294.  \n3 Interaction_Effect     12.8  -62.6  88.2 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n:::\n\n:::{.callout-note title=\"Bonferroni CI\"}\n\nThis is a general approach that adjusts the confidence level for each individual interval to control the overall family-wise confidence level. It is often more precise (i.e., yields narrower intervals) than the TÂ² method, especially if the number of combinations, `m`, is small.\n\nThe $100(1-\\alpha)\\%$ simultaneous confidence intervals are given by:\n\n$$ \\mathbf{c}_k'\\bar{\\mathbf{x}} \\pm t_{n-1, 1-\\alpha/(2m)} \\sqrt{\\frac{\\mathbf{c}_k'{S}\\mathbf{c}_k}{n}} \\quad \\text{for } k=1, 2, \\dots, m $$\nNote the critical value is a t-statistic adjusted by `m`, the number of intervals being constructed.\n\nIn the dog anesthetics study, the Bonferroni CIs can be computed as follows. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"R Code: Bonferroni CIs\"}\nm = 3 \n\ncval_Bon = qt(1-0.05/(2*m), n-1)\n\nci_Bon = tibble(\n  Component = names(ybar),\n  Estimate = ybar,\n  Lower = Estimate - cval_Bon * sqrt(S_kk/n), \n  Upper = Estimate + cval_Bon * sqrt(S_kk/n)\n)\nprint(ci_Bon)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 Ã— 4\n  Component          Estimate  Lower  Upper\n  <chr>                 <dbl>  <dbl>  <dbl>\n1 Halothane_Effect      -60.1 -119.   -1.37\n2 CO2_Effect            209.   130.  288.  \n3 Interaction_Effect     12.8  -58.0  83.6 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n:::\n\n\n\n## Exercises\n\n### Exercise 1: `iris` Data, Cont'd\n\nUsing the `iris` data, answer the following questions.\n\na. For the `versicolor` species, test if the mean vector for `Sepal.Length` and `Petal.Length` is equal to $\\boldsymbol{\\mu}_0 = [6.0, 4.0]'$. Use $\\alpha=0.05$. If you reject the null hypothesis, construct 95% Bonferroni simultaneous CIs to determine which variable(s) differ from the hypothesized values.\n\n::: {.callout-tip title=\"View Solution\" collapse=\"true\"}\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- iris[iris$Species == \"versicolor\", c(\"Sepal.Length\", \"Petal.Length\")]\nmu0 <- c(6.0, 4.0)\n\nht <- HotellingsT2Test(dat, mu = mu0)\nprint(ht)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tHotelling's one sample T2-test\n\ndata:  dat\nT.2 = 24.124, df1 = 2, df2 = 48, p-value = 5.602e-08\nalternative hypothesis: true location is not equal to c(6,4)\n```\n\n\n:::\n:::\n\n\n\n\n\n**Conclusion:** The p-value is much less than 0.05. We reject $H_0$ at significance level $\\alpha=0.05$ and conclude that the mean sepal length and the mean petal length is different from the the hypothesized values. To understand which variable would contribute to such difference, we look at the individual CIs below.  \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 3. Construct Bonferroni intervals since we rejected H0\nn <- nrow(dat)\np <- ncol(dat)\nxbar <- colMeans(dat)\nS <- cov(dat)\ns_ii <- diag(S)\n\ncval_bon <- qt(1 - 0.05 / (2 * p), df = n - 1)\nbonf_margins <- cval_bon * sqrt(s_ii / n)\nbonf_intervals <- data.frame(\n  Variable = names(dat),\n  Hypothesized_Mean = mu0,\n  Lower = xbar - bonf_margins,\n  Upper = xbar + bonf_margins\n)\n\nprint(\"95% Bonferroni Simultaneous CIs:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"95% Bonferroni Simultaneous CIs:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(bonf_intervals)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 Variable Hypothesized_Mean    Lower    Upper\nSepal.Length Sepal.Length                 6 5.767202 6.104798\nPetal.Length Petal.Length                 4 4.106330 4.413670\n```\n\n\n:::\n:::\n\n\n\n\n\n\n**Interpretation:** The CI for Sepal.Length (5.77 to 6.10) contains the hypothesized mean of 6.0. However, the CI for Petal.Length (4.11 to 4.41) does NOT contain its hypothesized mean of 4.0. Therefore, we conclude the overall mean vector is different because the mean Petal Length is significantly greater than 4.0.\n\n:::\n\n\nb. For the `setosa` species, construct simultaneous 95% confidence intervals for two linear combinations ($m=2$):\n\n*  **Difference:** $\\mu_{Sepal.Length} - \\mu_{Sepal.Width}$\n*  **Sum:** $\\mu_{Sepal.Length} + \\mu_{Sepal.Width}$\n\n::: {.callout-tip title=\"View Solution\" collapse=\"true\"}\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"R Code: CI Calculation\"}\nlibrary(dplyr)\n\ndata <- iris %>%\n  filter(Species == \"setosa\") %>%\n  select(Sepal.Length, Sepal.Width)\n\nn <- nrow(data)\np <- ncol(data)\nalpha <- 0.05\nsample_mean <- colMeans(data)\nS <- cov(data)\n\n# Define the linear combinations (m=2)\n# c1 will be for the Difference, c2 for the Sum\nc1 <- c(1, -1)\nc2 <- c(1, 1)\n# Combine into a list for easy iteration\ncombinations <- list(Difference = c1, Sum = c2)\nm <- length(combinations)\n\n# Calculate intervals for each method\nresults <- list()\nfor (i in 1:m) {\n  c_k <- combinations[[i]]\n  combo_name <- names(combinations)[i]\n  \n  point_estimate <- t(c_k) %*% sample_mean\n  std_error <- sqrt((t(c_k) %*% S %*% c_k) / n)\n  \n  # T-squared Method\n  T2_crit <- sqrt((p * (n - 1) / (n - p)) * qf(1 - alpha, p, n - p))\n  T2_margin <- T2_crit * std_error\n  \n  # Bonferroni Method (adjust alpha by m)\n  Bonf_crit <- qt(1 - alpha / (2 * m), df = n - 1)\n  Bonf_margin <- Bonf_crit * std_error\n\n  results[[i]] <- data.frame(\n    Combination = combo_name,\n    T2_Lower = point_estimate - T2_margin,\n    T2_Upper = point_estimate + T2_margin,\n    Bonf_Lower = point_estimate - Bonf_margin,\n    Bonf_Upper = point_estimate + Bonf_margin\n  )\n}\n\n# Combine and print the results\nfinal_table <- do.call(rbind, results)\nprint(final_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Combination T2_Lower T2_Upper Bonf_Lower Bonf_Upper\n1  Difference 1.482838 1.673162   1.491785   1.664215\n2         Sum 8.187499 8.680501   8.210674   8.657326\n```\n\n\n:::\n:::\n\n\n\n\n\n**Interpretation:** As shown in the table, the Bonferroni confidence intervals for both the \"Difference\" and the \"Sum\" are narrower than their TÂ² counterparts, offering a more precise estimate for these specific comparisons.\n\n:::\n\n### Exercise 2: Baseball Player Data\n\n**Background**\n\nA sports science journal from the 1950s established a \"classic\" physical standard for professional baseball players, claiming the ideal physique had a mean height of **72.5 inches** and a mean weight of **209 pounds**. The data is from the `Lahman::People` dataset. \n\nYour task is to determine if the average physique of modern players has significantly changed from this historical benchmark using a Hotelling's TÂ² test with a significance level of $\\alpha = 0.05$. In the analysis, we assume that the random vector follows a multivariate distribution and carry out the analysis, although it actually fails the multivariate Shapiro-Wilk test.  \n\n\n\n**1. State the Hypotheses**\n\nWrite the **null hypothesis ($H_0$)** and the **alternative hypothesis ($H_1$)** for this test using proper mathematical notation. Let $\\boldsymbol{\\mu}$ represent the true mean vector of `[height, weight]` for modern players.\n\n::: {.callout-tip title=\"View Solution\" collapse=\"true\"}\n\nThe hypotheses are formulated as:\n\n* **Null Hypothesis ($H_0$)**: The true mean vector of modern players is equal to the historical standard.\n    $$H_0: \\boldsymbol{\\mu} = \\begin{bmatrix} 72.5 \\\\ 209 \\end{bmatrix}$$\n\n* **Alternative Hypothesis ($H_1$)**: The true mean vector of modern players is not equal to the historical standard.\n    $$H_1: \\boldsymbol{\\mu} \\neq \\begin{bmatrix} 72.5 \\\\ 209 \\end{bmatrix}$$\n    \n::: \n\n\n\n**2. Prepare the Data**\n\nLoad the necessary R packages. From the `Lahman::People` dataset, create a final data frame that contains only the `height` and `weight` columns for players who debuted in the year 2010 or later, with any missing values removed.\n\n::: {.callout-tip title=\"View Solution\" collapse=\"true\"}\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Lahman)\nlibrary(dplyr)\nlibrary(ggplot2)\n# Create the final data frame for analysis\nplayer_data <- Lahman::People %>%\n  filter(debut>2010) %>%\n  filter(!is.na(height) & !is.na(weight)) %>%\n  dplyr::select(height, weight)%>%\n  mutate(across(c(height, weight), as.numeric))\n\ndf = player_data\nn = nrow(df)\np = ncol(df)\nhead(df)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"height\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"weight\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"74\",\"2\":\"235\",\"_rn_\":\"1\"},{\"1\":\"74\",\"2\":\"220\",\"_rn_\":\"2\"},{\"1\":\"74\",\"2\":\"185\",\"_rn_\":\"3\"},{\"1\":\"74\",\"2\":\"190\",\"_rn_\":\"4\"},{\"1\":\"73\",\"2\":\"225\",\"_rn_\":\"5\"},{\"1\":\"75\",\"2\":\"235\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\nGGally::ggpairs(df)\n```\n\n::: {.cell-output-display}\n![](04-Inference_for_Means_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n:::\n\n\n\n**3. Check Key Assumptions**\n\nCreate and examine histograms for both the `height` and `weight` distributions in your sample. Do they appear approximately normal?\n\n::: {.callout-tip title=\"View Solution\" collapse=\"true\"}\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"R Code: Normality Check\"}\n## Q-Q plot\npar(mfrow = c(1, 3))\n\n# Loop through the column names and create a plot for each\nfor (col_name in colnames(df)) {\n  qqnorm(df[[col_name]], main = col_name)\n  qqline(df[[col_name]], col = \"red\", lwd = 2)\n}\n\n## Compute Shapiro-Wilk statistic to test normality for each variable\nsapply(colnames(df), function(x) {\n               shapiro.test(df[[x]]) } )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          height                        weight                       \nstatistic 0.9834675                     0.9929459                    \np.value   1.309451e-19                  6.188722e-12                 \nmethod    \"Shapiro-Wilk normality test\" \"Shapiro-Wilk normality test\"\ndata.name \"df[[x]]\"                     \"df[[x]]\"                    \n```\n\n\n:::\n\n```{.r .cell-code  code-summary=\"R Code: Normality Check\"}\n## Perform multivariate normality test\nmvShapiroTest::mvShapiro.Test(as.matrix(df))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tGeneralized Shapiro-Wilk test for Multivariate Normality by\n\tVillasenor-Alva and Gonzalez-Estrada\n\ndata:  as.matrix(df)\nMVW = 0.99687, p-value = 1.045e-10\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](04-Inference_for_Means_files/figure-html/check-assumptions-callout-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n**Interpretation:** Both distributions are mound-shaped and reasonably symmetric, supporting the assumption of approximate normality.\n\n:::\n\n\n\n**4. Perform the Statistical Test**\n\nRun a one-sample Hotelling's $T^2$ test on your prepared data using the historical standard as your hypothesized mean vector.\n\n::: {.callout-tip title=\"View Solution\" collapse=\"true\"}\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"R Code: Hotelling's T$^2$ Test\"}\n# Define the hypothesized mean vector from the 1950s standard\nmu0 <- c(72.5, 209.0)\n\n# Perform the test\ntest_result <- DescTools::HotellingsT2Test(x = df, mu = mu0)\n\n# Print the results\nprint(test_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tHotelling's one sample T2-test\n\ndata:  df\nT.2 = 589.64, df1 = 2, df2 = 3432, p-value < 2.2e-16\nalternative hypothesis: true location is not equal to c(72.5,209)\n```\n\n\n:::\n:::\n\n\n\n\n\n\n:::\n\n\n\n**5. Interpret the Results**\n\nWhat is the p-value from your test? Based on this and $\\alpha = 0.05$, do you reject or fail to reject the null hypothesis? State your conclusion in the context of the problem.\n\n::: {.callout-tip title=\"View Solution\" collapse=\"true\"}\n\nThe p-value is exceptionally small and far below the significance level of $\\alpha = 0.05$.\n\n**Conclusion:** We **strongly reject the null hypothesis**. There is overwhelming statistical evidence to conclude that the average physique (the mean vector of height and weight) of modern baseball players is significantly different from the historical standard of 72 inches and 190 pounds.\n\n:::\n\n\n\n**6. Draw Conclusions Based on Confidence Region**\n\nCheck if the hypothesized mean is in the 95% confidence ellipse. Explain how it supports your conclusion.\n\n::: {.callout-tip title=\"View Solution\" collapse=\"true\"}\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"R Code: Visualization\"}\n# Calculate the mean of our modern player sample\nxbar <- colMeans(df)\nS = cov(df)\nT2 = n*t(xbar - mu0) %*%solve(S)%*%(xbar - mu0)\nscaledT2 = (n-p)/(p*(n-1)) * T2 \n# Calculate points for the 95% confidence ellipse\nalpha=0.05\nFvalue <- qf(1 - alpha, p, n - p)\n\ncat(\"scaled T2 statistic is\",scaledT2, \", F critical value is\", Fvalue)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nscaled T2 statistic is 589.6427 , F critical value is 2.998349\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# this is the covariance of xbar\nSigma_ell <- S / n\neig <- eigen(Sigma_ell)\nA <- eig$vectors %*% diag(sqrt(eig$values)) *\nsqrt((n - 1) * p / (n - p) * Fvalue)\n\ntheta <- seq(0, 2 * pi, length.out = 400)\npts <- t(matrix(xbar, nrow = 2, ncol = length(theta)) +\nA %*% rbind(cos(theta), sin(theta)))\n\ndf_ell <- as.data.frame(pts)\ncolnames(df_ell) <- names(df)\ncenter <- data.frame(height = xbar[1], weight = xbar[2])\n\ngg <- ggplot() +\ngeom_path(data = df_ell, aes(height, weight)) +\ngeom_point(\ndata = center,\naes(height, weight),\ncolor = \"red\",\nsize = 3\n) +\ntheme_minimal() +\nlabs(x = \"Height Difference\", y = \"Weight Difference\")\n\nprint(gg)\n```\n\n::: {.cell-output-display}\n![](04-Inference_for_Means_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n**Interpretation:** This result confirms the test result. The historical standard is far outside the confidence ellipse (the 95% confidence region for the true mean of modern players),  confirming that modern players are, on average, both taller and heavier. At this point we do not know which one contributes to the difference, which can be answered using simultaneous confidence intervals. \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Simultaneous T2 CIs\"}\ncval_T2 = sqrt((n-1)*p/(n-p) * Fvalue) \nse_ii = sqrt(diag(S)/n)\n\nci_T2 = tibble(\nComponent = names(df),\nEstimate = xbar,\nLower = Estimate - cval_T2 * se_ii,\nUpper = Estimate + cval_T2 * se_ii\n)\nprint(ci_T2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 4\n  Component Estimate Lower Upper\n  <chr>        <dbl> <dbl> <dbl>\n1 height        73.7  73.6  73.8\n2 weight       209.  208.  210. \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Simultaneous Bonferroni CIs\"}\nm = 2\ncval_Bon = qt(1-0.05/(2*m), n-1)\nci_Bon = tibble(\nComponent = names(df),\nEstimate = xbar,\nLower = Estimate - cval_Bon * se_ii,\nUpper = Estimate + cval_Bon * se_ii\n)\nprint(ci_Bon)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 4\n  Component Estimate Lower Upper\n  <chr>        <dbl> <dbl> <dbl>\n1 height        73.7  73.6  73.8\n2 weight       209.  208.  210. \n```\n\n\n:::\n:::\n\n\n\n\n\n\nBased on simultaneous CIs (T2 or Bonferroni), the results indicates that there is a significantly individual difference in height  and weight at $\\alpha=0.05$. \n\n:::\n\n\n\n",
    "supporting": [
      "04-Inference_for_Means_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}