[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Multivariate Data Analysis",
    "section": "",
    "text": "Preface\nThis book contains the course notes for STAT 4750/5750 (Introduction to Multivariate Data Analysis) at Iowa State University. This course is designed for undergraduate students in statistics and data science and graduate students from the applied sciences with majors outside statistics. The prerequisite for this course includes STAT 3010 or STAT 3260 (for undergraduates) or STAT 5101 for graduate students. Knowledge of matrix algebra is recommended but not required to understand the topics covered in this book.\nThe course STAT 3010 (Intermediate Statistical Concepts and Methods) covers statistical concepts and methods used in the analysis of observational data. Topics include analysis of single sample, two sample and paired sample data; simple and multiple linear regression; model building and analysis of residuals; one-way ANOVA, tests of independence for contingency tables, and logistic regression.\nThe course STAT 3260 (Introduction to Business Statistics II) covers multiple regression, regression diagnostics, model building, applications in analysis of variance and time series, random variables, conditional probability, and data visualization.\nThe course STAT 5101 (Statistical Methods for Research Workers) was renamed from the previous course STAT 5870 starting Fall 2025. STAT 5101 is a first course in statistics for graduate students from the applied sciences, and covers topics including basic experimental designs and analysis of variance, analysis of categorical data, logistic and log-linear regression, likelihood-based inference, and the use of simulation.\nStudents who have the needed backgrounds shall find the statistical concepts and methods in this book easy to follow and understand. All the methods are illustrated with various examples with step-by-step solutions and extensive R code. Exercises are also given at each chapter to help students better understand the statistical methods and apply these methods for real data analysis by adapting the corresponding R code in the book.\nThe materials in this book are largely influenced by previous course notes taught by several instructors including Yumou Qiu and Kenneth Koehler in the Department of Statistics at Iowa State University.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "ch1/01-intro.html",
    "href": "ch1/01-intro.html",
    "title": "1  Introduction to Multivariate Data",
    "section": "",
    "text": "1.1 Course Outline",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Multivariate Data</span>"
    ]
  },
  {
    "objectID": "ch1/01-intro.html#course-outline",
    "href": "ch1/01-intro.html#course-outline",
    "title": "1  Introduction to Multivariate Data",
    "section": "",
    "text": "Introduction to Multivariate Data\nNumerical Summaries and Visualization of Multivariate Data\nMultivariate Normal Distribution\nComparing Centers of Distributions\n(Hotelling T^2, MANOVA, Repeated Measures)\nPrincipal Component Analysis\n(Summarizing data in lower dimensions)\nExploratory Factor Analysis\n(What to do when the variables of interest cannot be directly observed.)\nMulti-Dimensional Scaling\nCluster Analysis\nDiscriminant Analysis and Classification\n(Including linear discriminants, logistic regression, classification trees, and random forests)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Multivariate Data</span>"
    ]
  },
  {
    "objectID": "ch1/01-intro.html#objectives-of-multivariate-analysis",
    "href": "ch1/01-intro.html#objectives-of-multivariate-analysis",
    "title": "1  Introduction to Multivariate Data",
    "section": "1.2 Objectives of Multivariate Analysis",
    "text": "1.2 Objectives of Multivariate Analysis\n\nUnderstand dependencies among variables: What is the nature of associations among variables?\nPrediction: If variables are associated, then we might be able to predict the value of some of them given information on the others. (Statistical inference)\nHypothesis testing: Are differences in sets of response means for two or more groups large enough to be distinguished from sampling variation? (Statistical inference)\nDimensionality reduction: Can we reduce the dimensionality of the problem by considering a small number of (linear) combinations of a large number of measurements without losing important information?\n\nPrincipal Components\nFactor Analysis\nMultidimensional Scaling\n\nGrouping (Cluster Analysis): Identify groups of “similar” units using a common set of measured traits.\nClassification: Classify units into previously defined groups using a common set of measured traits.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Multivariate Data</span>"
    ]
  },
  {
    "objectID": "ch1/01-intro.html#introduction-to-r-and-rstudio",
    "href": "ch1/01-intro.html#introduction-to-r-and-rstudio",
    "title": "1  Introduction to Multivariate Data",
    "section": "1.3 Introduction to R and RStudio",
    "text": "1.3 Introduction to R and RStudio\n\n1.3.1 Getting Started\nTo download R, please choose your preferred CRAN mirror. Here I recommend the mirror 0-Cloud available at https://cloud.r-project.org/.\nInstall R on macOS\nFor macOS users, below are the steps you need to install R.\n\nInstall Xcode (e.g., via the App Store). Skip this step if your Mac already has Xcode installed.\nInstall XQuartz.\nGo to https://cloud.r-project.org/ and click “Download R for macOS.”\nClick on the download link for the latest R version for macOS and follow the instruction.\n\nInstall R on Windows:\nFor Windows users, the following instructions guide you to install R.\n\nGo to https://cloud.r-project.org/ and click “Download R for Windows.”\nClick “install R for the first time.”\nChoose a download mirror (any CRAN mirror will work).\nClick on the download link for the latest R version for Windows and follow the instruction\n\nInstall RStudio\nRStudio has multiple panes in the window, open by default: one for writing and editing code, another for executing it, another for plots produced or help, and another that lists the R data objects available. RStudio can be download for free at https://posit.co/download/rstudio-desktop/.\n\n\n1.3.2 Introduction to R Programming\n\n\n\n\n\n\nSome Tips: R Working Environment\n\n\n\n\n\nWhenever you work with R for data analysis, it is recommended to load all the packages used in the data analysis pipeline before the function sessionInfo(). In addition, if random numbers are generated, it is also recommended to set random seed to ensure reproducibility using set.seed().\n\n\nR Working Environment\n# load packages \nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(lubridate)\n\nsessionInfo()\n\n\nR version 4.4.3 (2025-02-28)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.3.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] lubridate_1.9.4 readr_2.1.5     tidyr_1.3.1     dplyr_1.1.4    \n[5] ggplot2_3.5.2  \n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.5        cli_3.6.5          knitr_1.50         rlang_1.1.6       \n [5] xfun_0.52          purrr_1.0.4        generics_0.1.4     jsonlite_2.0.0    \n [9] glue_1.8.0         htmltools_0.5.8.1  hms_1.1.3          scales_1.4.0      \n[13] rmarkdown_2.29     grid_4.4.3         evaluate_1.0.3     tibble_3.2.1      \n[17] tzdb_0.5.0         fastmap_1.2.0      yaml_2.3.10        lifecycle_1.0.4   \n[21] compiler_4.4.3     RColorBrewer_1.1-3 timechange_0.3.0   htmlwidgets_1.6.4 \n[25] pkgconfig_2.0.3    rstudioapi_0.17.1  farver_2.1.2       digest_0.6.37     \n[29] R6_2.6.1           tidyselect_1.2.1   pillar_1.10.2      magrittr_2.0.3    \n[33] withr_3.0.2        tools_4.4.3        gtable_0.3.6      \n\n\n\n\n\n\n\n\n\n\n\nR is a Calculator\n\n\n\n\n\n\nCode\n# Numeric, logical, character types\n3.14159             # numeric\n\n[1] 3.14159\n\nCode\nT                   # logical, can also be TRUE\n\n[1] TRUE\n\nCode\nF                   # logical, can also be FALSE \n\n[1] FALSE\n\nCode\n\"Stat 4750/5750\"    # character\n\n[1] “Stat 4750/5750”\n\nCode\n# Basic arithmetic operators\n1 + 2\n\n[1] 3\n\nCode\n20 - 3\n\n[1] 17\n\nCode\n2 * 6\n\n[1] 12\n\nCode\n4 / 3\n\n[1] 1.333333\n\nCode\n2 ^ 4\n\n[1] 16\n\nCode\n-3.14\n\n[1] -3.14\n\nCode\n(1 + 2) * (3 / 4)  # use parenthesis\n\n[1] 2.25\n\nCode\n2 == 1\n\n[1] FALSE\n\nCode\n# Vector\nc()  # empty vector\n\nNULL\n\nCode\nc(1, 2, 3, 4)\n\n[1] 1 2 3 4\n\nCode\n1:4\n\n[1] 1 2 3 4\n\nCode\n# Look up the function help, 3 ways:\n# Following 3 ways work for most functions\n          # 1. search in help pane\n?sum      # 2. use ?\nsum(1:4)  # 3. move your cursor to the function, then press F1, the easiest way\n\n[1] 10\n\nCode\n# The 3rd way doesn't work for functions having some symbols, like +, ==, %*%\n          # 1. search in help pane\n?`+`      # 2. use ? but wrap the symbol with ``\n\n\n\n\n\n\n\n\n\n\nR Objects\n\n\n\n\n\n\n\nCode\n# Assignment operator &lt;-\n\n# Numeric class\nmynumbers &lt;- 5:12\nmynumbers\n\n\n[1]  5  6  7  8  9 10 11 12\n\n\nCode\nmynumbers + 2\n\n\n[1]  7  8  9 10 11 12 13 14\n\n\nCode\nmynumbers * 10\n\n\n[1]  50  60  70  80  90 100 110 120\n\n\nCode\ntypeof(mynumbers)      # type of an object\n\n\n[1] \"integer\"\n\n\nCode\n# check if is numeric (both integer and double type are numeric)\nis.numeric(mynumbers)  \n\n\n[1] TRUE\n\n\nCode\nis.vector(mynumbers)   # check if is vector\n\n\n[1] TRUE\n\n\nCode\nlength(mynumbers)      # length of an object\n\n\n[1] 8\n\n\nCode\n# Character class\nmytext &lt;- c(\"hello\", \"class\", \"four\") \nmytext\n\n\n[1] \"hello\" \"class\" \"four\" \n\n\nCode\ntypeof(mytext)\n\n\n[1] \"character\"\n\n\nCode\nis.numeric(mytext)\n\n\n[1] FALSE\n\n\nCode\nis.character(mytext)\n\n\n[1] TRUE\n\n\nCode\nis.character(mynumbers)\n\n\n[1] FALSE\n\n\nCode\nis.vector(mytext)\n\n\n[1] TRUE\n\n\nCode\nlength(mytext)\n\n\n[1] 3\n\n\nCode\n# Logical class\nmylogic &lt;- c(TRUE, FALSE, TRUE, TRUE)\ntypeof(mylogic)\n\n\n[1] \"logical\"\n\n\nCode\n# Factor class\ngender &lt;- c(\"male\", \"female\", \"female\", \"female\", \"male\") \ngender\n\n\n[1] \"male\"   \"female\" \"female\" \"female\" \"male\"  \n\n\nCode\nis.vector(gender)\n\n\n[1] TRUE\n\n\nCode\nis.character(gender)\n\n\n[1] TRUE\n\n\nCode\nis.vector(gender)\n\n\n[1] TRUE\n\n\nCode\nis.factor(gender)\n\n\n[1] FALSE\n\n\nCode\ngenderf &lt;- factor(gender)\ngenderf\n\n\n[1] male   female female female male  \nLevels: female male\n\n\nCode\nis.character(genderf)\n\n\n[1] FALSE\n\n\nCode\nis.factor(genderf)\n\n\n[1] TRUE\n\n\nCode\nsummary(mynumbers)  # numeric: 5-number summary\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   5.00    6.75    8.50    8.50   10.25   12.00 \n\n\nCode\nsummary(mylogic)    # logical: count how many T and F\n\n\n   Mode   FALSE    TRUE \nlogical       1       3 \n\n\nCode\nsummary(mytext)     # character:\n\n\n   Length     Class      Mode \n        3 character character \n\n\nCode\nsummary(genderf)    # factor: count the frequency\n\n\nfemale   male \n     3      2 \n\n\nCode\n# List class\nmylist &lt;- list(1, \"ABC\", FALSE)\nmylist\n\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] \"ABC\"\n\n[[3]]\n[1] FALSE\n\n\nCode\nis.vector(mylist)\n\n\n[1] TRUE\n\n\nCode\nis.list(mylist)\n\n\n[1] TRUE\n\n\nCode\nlength(mylist)\n\n\n[1] 3\n\n\n\n\n\n\n\n\n\n\n\nWorking with Matrices\n\n\n\n\n\n\n\nCode\n# All elements should be the same type\nmydata &lt;- matrix(c(140, 120, 160, 145, 125, 65, 60, 63, 66, 61), ncol = 2, byrow = FALSE) \nmydata\n\n\n     [,1] [,2]\n[1,]  140   65\n[2,]  120   60\n[3,]  160   63\n[4,]  145   66\n[5,]  125   61\n\n\nCode\ndim(mydata)  # dimensions\n\n\n[1] 5 2\n\n\nCode\ncolnames(mydata) &lt;- c(\"Weight.lbs\", \"Height.in\")\nrownames(mydata) &lt;- c(\"a\", \"b\", \"c\", \"d\", \"e\")\nmydata\n\n\n  Weight.lbs Height.in\na        140        65\nb        120        60\nc        160        63\nd        145        66\ne        125        61\n\n\nCode\nsummary(mydata)  # summary of each column\n\n\n   Weight.lbs    Height.in \n Min.   :120   Min.   :60  \n 1st Qu.:125   1st Qu.:61  \n Median :140   Median :63  \n Mean   :138   Mean   :63  \n 3rd Qu.:145   3rd Qu.:65  \n Max.   :160   Max.   :66  \n\n\n\n\n\n\n\n\n\n\n\nWorking with Data Frames\n\n\n\n\n\n\n\nData Frames\n# Different columns can have different types\ndf &lt;- data.frame(Weight.lbs = c(140, 120, 160, 145, 125, 180, 165),\n                 Height.in = c(65, 60, 63, 66, 61, 70, 68),\n                 Gender = c(rep(\"Female\", 5), rep(\"Male\", 2)), stringsAsFactors = T)\ndf\n\n\n  Weight.lbs Height.in Gender\n1        140        65 Female\n2        120        60 Female\n3        160        63 Female\n4        145        66 Female\n5        125        61 Female\n6        180        70   Male\n7        165        68   Male\n\n\nData Frames\ndim(df)\n\n\n[1] 7 3\n\n\nData Frames\nhead(df)     # the first 6 rows\n\n\n  Weight.lbs Height.in Gender\n1        140        65 Female\n2        120        60 Female\n3        160        63 Female\n4        145        66 Female\n5        125        61 Female\n6        180        70   Male\n\n\nData Frames\nsummary(df)  # summary of each column\n\n\n   Weight.lbs      Height.in        Gender \n Min.   :120.0   Min.   :60.00   Female:5  \n 1st Qu.:132.5   1st Qu.:62.00   Male  :2  \n Median :145.0   Median :65.00             \n Mean   :147.9   Mean   :64.71             \n 3rd Qu.:162.5   3rd Qu.:67.00             \n Max.   :180.0   Max.   :70.00             \n\n\nData Frames\nstr(df)      # structure of the data frame\n\n\n'data.frame':   7 obs. of  3 variables:\n $ Weight.lbs: num  140 120 160 145 125 180 165\n $ Height.in : num  65 60 63 66 61 70 68\n $ Gender    : Factor w/ 2 levels \"Female\",\"Male\": 1 1 1 1 1 2 2\n\n\na tibble is a modern implementation of a data frame, designed to be more user-friendly and efficient, especially when working with large datasets.\n\n\ntibble\n# install.packages(\"tibble\")\nlibrary(tibble)\n# create a tibble from an existing object\nas_tibble(df) \n\n\n# A tibble: 7 × 3\n  Weight.lbs Height.in Gender\n       &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt; \n1        140        65 Female\n2        120        60 Female\n3        160        63 Female\n4        145        66 Female\n5        125        61 Female\n6        180        70 Male  \n7        165        68 Male  \n\n\ntibble\n# create a new tibble \ntibble(x=1:5, \n       y=1.0,\n       z=x^2+y)\n\n\n# A tibble: 5 × 3\n      x     y     z\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     1     2\n2     2     1     5\n3     3     1    10\n4     4     1    17\n5     5     1    26\n\n\ntibble\n# define a row-by-row tibble\ntribble(\n  ~x, ~y, ~z,\n  \"a\", 2, 1,\n  \"b\", 3, 4\n)\n\n\n# A tibble: 2 × 3\n  x         y     z\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 a         2     1\n2 b         3     4\n\n\n\n\n\n\n\n\n\n\n\nOperations on Matrix and Data Frame\n\n\n\n\n\n\n\nCode\nmydata[, 1]                     # extract the first column\n\n\n  a   b   c   d   e \n140 120 160 145 125 \n\n\nCode\nmydata[1, ]                     # extract the first row\n\n\nWeight.lbs  Height.in \n       140         65 \n\n\nCode\nmydata[1:2, ]                   # extract the first two rows\n\n\n  Weight.lbs Height.in\na        140        65\nb        120        60\n\n\nCode\nmydata[3, 2]                    # extract the element in the third row and the second column\n\n\n[1] 63\n\n\nCode\ndf[seq(1, 7, 2), ]              # extract every second row\n\n\n  Weight.lbs Height.in Gender\n1        140        65 Female\n3        160        63 Female\n5        125        61 Female\n7        165        68   Male\n\n\nCode\nseq(1, 7, 2)\n\n\n[1] 1 3 5 7\n\n\nCode\nsubset(df, Gender == \"Male\")    # select based on value\n\n\n  Weight.lbs Height.in Gender\n6        180        70   Male\n7        165        68   Male\n\n\nCode\nt(mydata)                       # transpose of matrix\n\n\n             a   b   c   d   e\nWeight.lbs 140 120 160 145 125\nHeight.in   65  60  63  66  61\n\n\nCode\nmydata[, 1] * mydata[, 2]       # element-wise multiplication\n\n\n    a     b     c     d     e \n 9100  7200 10080  9570  7625 \n\n\nCode\nmydata %*% t(mydata)            # matrix multiplication\n\n\n      a     b     c     d     e\na 23825 20700 26495 24590 21465\nb 20700 18000 22980 21360 18660\nc 26495 22980 29569 27358 23843\nd 24590 21360 27358 25381 22151\ne 21465 18660 23843 22151 19346\n\n\n\n\n\n\n\n\n\n\n\nPlotting with the ggplot2 Package\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\ndata(\"USArrests\")\ndat = USArrests %&gt;% as_tibble()\ndat %&gt;% \n  ggplot(aes(x=UrbanPop, y=Murder)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndat_long = dat %&gt;%\n  tidyr::pivot_longer(1:4,names_to = \"Variable\", values_to = \"Value\") \nhead(dat_long)\n\n\n# A tibble: 6 × 2\n  Variable Value\n  &lt;chr&gt;    &lt;dbl&gt;\n1 Murder    13.2\n2 Assault  236  \n3 UrbanPop  58  \n4 Rape      21.2\n5 Murder    10  \n6 Assault  263  \n\n\nCode\n# faceted histogram\nggplot(dat_long, aes(Value)) + \n  geom_histogram(bins=20, fill=\"grey80\", color=\"white\") + \n  facet_wrap(~Variable, scales=\"free\") + \n  labs(title = \"Distributions by Variable\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Multivariate Data</span>"
    ]
  },
  {
    "objectID": "ch1/01-intro.html#organization-of-data-and-notation",
    "href": "ch1/01-intro.html#organization-of-data-and-notation",
    "title": "1  Introduction to Multivariate Data",
    "section": "1.4 Organization of Data and Notation",
    "text": "1.4 Organization of Data and Notation\n\nn = the number of observations or units\n\np = the number of variables measured on each unit\n\nIf p = 1, then we are back in the usual univariate setting\n\nx_{ik} = the i-th observation of the k-th variable\n\n\n\\text{Observations} \\quad \\overset{\\text{Variables}}{\n\\begin{bmatrix}\nx_{11} & x_{12} & \\cdots & x_{1p} \\\\\nx_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{bmatrix}\n}\n\n\nData matrix:  X_{n \\times p}=\\left[ \\begin{array}{cccc} x_{11} & x_{12} & \\cdots & x_{1p} \\\\ x_{21} & x_{22} & \\cdots & x_{2p} \\\\ \\vdots & \\vdots & & \\vdots  \\\\ x_{n1} & x_{n2} & \\cdots & x_{np}\\end{array} \\right ] \nThis can be written as n rows or as p columns \nX_{n \\times p} =\n\\begin{bmatrix}\n\\mathbf{x}_1^{\\prime} \\\\\n\\mathbf{x}_2^{\\prime} \\\\\n\\vdots \\\\\n\\mathbf{x}_n^{\\prime}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf{x}_1^{\\top} \\\\\n\\mathbf{x}_2^{\\top} \\\\\n\\vdots \\\\\n\\mathbf{x}_n^{\\top}\n\\end{bmatrix}\n=\n\\left[ \\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_p \\right]\n where both \\prime and \\top represent matrix transpose.\n\n\n\nR Code: Data Organization\nX = as.matrix(USArrests)\ndim(X)\n\n\n[1] 50  4\n\n\n\n\nR Code: Matrix Transpose\nt(X[1:5,])\n\n\n         Alabama Alaska Arizona Arkansas California\nMurder      13.2   10.0     8.1      8.8        9.0\nAssault    236.0  263.0   294.0    190.0      276.0\nUrbanPop    58.0   48.0    80.0     50.0       91.0\nRape        21.2   44.5    31.0     19.5       40.6",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Multivariate Data</span>"
    ]
  },
  {
    "objectID": "ch1/01-intro.html#descriptive-statistics",
    "href": "ch1/01-intro.html#descriptive-statistics",
    "title": "1  Introduction to Multivariate Data",
    "section": "1.5 Descriptive Statistics",
    "text": "1.5 Descriptive Statistics\n\n1.5.1 Sample Mean\n\nThe sample mean of the kth variable (k = 1,...,p) is \n\\bar{x}_k = \\frac{1}{n} \\sum_{i = 1}^n x_{ik}\n\n\n\n\n1.5.2 Sample Variance and Sample Standard Deviation\n\nThe sample variance of the kth variable is \ns^2_k = \\frac{1}{n-1} \\sum_{i=1}^n (x_{ik} - \\bar{x}_k)^2  \n\nThe sample standard deviation is given by \ns_k = \\sqrt{s^2_k}\n\nWe often use s_{kk} to denote the sample variance for the k-th variable. Thus, \ns^2_k = s_{kk}\n\n\n\n\n1.5.3 Sample Covariance and Sample Correlation\n\nThe sample covariance between variable k and variable j is computed as \ns_{jk} = \\frac{1}{n-1} \\sum_{i=1}^n (x_{ij} - \\bar{x}_j) (x_{ik} - \\bar{x}_k)\n\nIf variables k and j are independent, the population covariance will be exactly zero, but the sample covariance will vary about zero.\nThe sample correlation between variables k and j is defined as \nr_{jk} = \\frac{s_{jk}}{\\sqrt{s_{jj}} \\sqrt{s_{kk}}}\n\nr_{jk} is between -1 and 1.\nr_{jk} = r_{kj}\nThe sample correlation is equal to the sample covariance if measurements are standardized.\nThe sample correlation (r_{ij}) will vary about the value of the population correlation (\\rho_{ij})\n\n\n\n\n\n\n\nR Code: Descriptive Statistics\n\n\n\n\n\n\n# load built-in US crime rates data\ndata(\"USArrests\") \ndat = USArrests \nhead(dat)\n\n           Murder Assault UrbanPop Rape\nAlabama      13.2     236       58 21.2\nAlaska       10.0     263       48 44.5\nArizona       8.1     294       80 31.0\nArkansas      8.8     190       50 19.5\nCalifornia    9.0     276       91 40.6\nColorado      7.9     204       78 38.7\n\n\n\n\nSample mean\nmu = colMeans(dat)\nmu \n\n\n  Murder  Assault UrbanPop     Rape \n   7.788  170.760   65.540   21.232 \n\n\nSample mean\nlibrary(dplyr)\ndat %&gt;% \n  summarise(across(where(is.numeric), mean))\n\n\n  Murder Assault UrbanPop   Rape\n1  7.788  170.76    65.54 21.232\n\n\n\n\nSample variance\ns2 = apply(dat, 2, var)\ns2 \n\n\n    Murder    Assault   UrbanPop       Rape \n  18.97047 6945.16571  209.51878   87.72916 \n\n\nSample variance\n# using dplyr\ndat %&gt;% \n  summarise(across(where(is.numeric), var))\n\n\n    Murder  Assault UrbanPop     Rape\n1 18.97047 6945.166 209.5188 87.72916\n\n\n\n\nSample standard deviation\n# compute standard deviation from data \ns = apply(dat, 2, sd)  \n# or from sample variance \ns = sqrt(s2)\ns \n\n\n   Murder   Assault  UrbanPop      Rape \n 4.355510 83.337661 14.474763  9.366385 \n\n\n\n\nSample covariance\nn = nrow(dat)\ns_jk = 1/(n-1) * sum((dat[,1] - mu[1]) * (dat[,2] - mu[2]))\ns_jk \n\n\n[1] 291.0624\n\n\n\n\nSample correlation\nr_jk = s_jk / sqrt(s2[1] * s2[2])\n\n\n\n\n\n\n\n1.5.4 Covariance and Correlation Measures\n\nCovariance and correlation measure linear association.\nOther non-linear (or curved) relationships may exist among variables even if r_{jk} = 0.\nA population correlation of zero means no linear association,\nbut it does not necessarily imply independence.\n\n\n\n\n\n\nCorrelation Measures Linear Association\n\n\n\n\n\n\n\n\n\nNonlinear Dependence with (Near) Zero Correlation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Multivariate Data</span>"
    ]
  },
  {
    "objectID": "ch1/01-intro.html#matrix-organization-of-descriptive-statistics",
    "href": "ch1/01-intro.html#matrix-organization-of-descriptive-statistics",
    "title": "1  Introduction to Multivariate Data",
    "section": "1.6 Matrix Organization of Descriptive Statistics",
    "text": "1.6 Matrix Organization of Descriptive Statistics\n\n1.6.1 Sample Mean\n\nSample mean: \\bar{\\mathbf{x}} is the p \\times 1 vector of sample means:\n\n\n\\bar{\\mathbf{x}} =\n\\begin{bmatrix}\n\\bar{x}_1 \\\\\n\\bar{x}_2 \\\\\n\\vdots \\\\\n\\bar{x}_p\n\\end{bmatrix}\n\n\n\\bar{\\mathbf{x}} is an estimate of the vector of population means:\n\n\n\\boldsymbol{\\mu} =\n\\begin{bmatrix}\n\\mu_1 \\\\\n\\mu_2 \\\\\n\\vdots \\\\\n\\mu_p\n\\end{bmatrix}\n\n\n\nR Code: Sample Mean\nX = as.matrix(USArrests)\nxbar = colMeans(X)\nxbar \n\n\n  Murder  Assault UrbanPop     Rape \n   7.788  170.760   65.540   21.232 \n\n\n\nCentered Data: X_c = X - \\mathbf{1}_n \\bar{\\mathbf{x}}^\\top\n\n\n\nR Code: Centered Data\nn = nrow(X) \nones_n = matrix(1, n, ncol=1)\nXc = X - ones_n %*% t(xbar)\n\n\n\n\n1.6.2 Sample Covariance\n\nS is the p \\times p symmetric matrix of sample variances (on the diagonal) and sample covariances (the off-diagonal elements):\n\n\nS =\n\\begin{bmatrix}\ns_{11} & s_{12} & s_{13} & \\cdots & s_{1p} \\\\\ns_{21} & s_{22} & s_{23} & \\cdots & s_{2p} \\\\\n\\vdots & \\vdots & \\vdots &        & \\vdots \\\\\ns_{p1} & s_{p2} & s_{p3} & \\cdots & s_{pp}\n\\end{bmatrix}\n= \\frac{1}{n-1} \\sum_{i=1}^n (\\mathbf{x}_i - \\bar{\\mathbf{x}})(\\mathbf{x}_i - \\bar{\\mathbf{x}})^{\\top} = \\frac{1}{n-1} X_c^\\top X_c\n\n\nS is an estimate of the population covariance matrix:\n\n\n\\Sigma =\n\\begin{bmatrix}\n\\sigma_{11} & \\sigma_{12} & \\sigma_{13} & \\cdots & \\sigma_{1p} \\\\\n\\sigma_{21} & \\sigma_{22} & \\sigma_{23} & \\cdots & \\sigma_{2p} \\\\\n\\vdots      & \\vdots      & \\vdots      &        & \\vdots      \\\\\n\\sigma_{p1} & \\sigma_{p2} & \\sigma_{p3} & \\cdots & \\sigma_{pp}\n\\end{bmatrix}\n\n\n\nR Code: Sample Covariance\nS = cov(X) \n# or using matrix algebra\nS_mat = t(Xc)%*%Xc/(n-1)\nsignif(cbind(S, S_mat),4)\n\n\n          Murder Assault UrbanPop   Rape  Murder Assault UrbanPop   Rape\nMurder    18.970   291.1    4.386  22.99  18.970   291.1    4.386  22.99\nAssault  291.100  6945.0  312.300 519.30 291.100  6945.0  312.300 519.30\nUrbanPop   4.386   312.3  209.500  55.77   4.386   312.3  209.500  55.77\nRape      22.990   519.3   55.770  87.73  22.990   519.3   55.770  87.73\n\n\n\n\n1.6.3 Sample Correlation\n\nThe p \\times p matrix of sample correlations is also symmetric:\n\n\nR =\n\\begin{bmatrix}\n1       & r_{12} & r_{13} & \\cdots & r_{1p} \\\\\nr_{21}  & 1      & r_{23} & \\cdots & r_{2p} \\\\\n\\vdots  & \\vdots & \\vdots &        & \\vdots \\\\\nr_{p1}  & r_{p2} & r_{p3} & \\cdots & 1\n\\end{bmatrix}\n= D^{-1/2} \\, S \\, D^{-1/2}\n\n\nD^{-1/2} is a diagonal matrix with (j,j) entry 1/\\sqrt{s_{jj}} = 1/s_j, i.e.,\n\n\nD^{-1/2} =\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{s_{11}}} & 0                       & 0                       & \\cdots & 0 \\\\\n0                       & \\frac{1}{\\sqrt{s_{22}}} & 0                       & \\cdots & 0 \\\\\n\\vdots                  & \\vdots                  & \\vdots                  &        & \\vdots \\\\\n0                       & 0                       & 0                       & \\cdots & \\frac{1}{\\sqrt{s_{pp}}}\n\\end{bmatrix}\n\n\nR is an estimate of the population correlation matrix \nP =\n\\begin{bmatrix}\n1          & \\rho_{12} & \\rho_{13} & \\cdots & \\rho_{1p} \\\\\n\\rho_{21}  & 1         & \\rho_{23} & \\cdots & \\rho_{2p} \\\\\n\\vdots     & \\vdots    & \\vdots    &        & \\vdots   \\\\\n\\rho_{p1}  & \\rho_{p2} & \\rho_{p3} & \\cdots & 1\n\\end{bmatrix}\n\n\n\n\nR Code: Sample Correlation\nR = cor(X)\nR\n\n\n             Murder   Assault   UrbanPop      Rape\nMurder   1.00000000 0.8018733 0.06957262 0.5635788\nAssault  0.80187331 1.0000000 0.25887170 0.6652412\nUrbanPop 0.06957262 0.2588717 1.00000000 0.4113412\nRape     0.56357883 0.6652412 0.41134124 1.0000000\n\n\n\n\nR Code: Pairwise Scatterplots and Correlations\nGGally::ggpairs(dat)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Multivariate Data</span>"
    ]
  },
  {
    "objectID": "ch1/01-intro.html#standardization",
    "href": "ch1/01-intro.html#standardization",
    "title": "1  Introduction to Multivariate Data",
    "section": "1.7 Standardization",
    "text": "1.7 Standardization\n\n1.7.1 Standardized Data (or z-Scores)\n\nSuppose x_{ij} is the measurement on the j-th outcome variable for the i-th subject in the data set.\nThe standardized value is\n\nz_{ij} = \\frac{x_{ij} - \\bar{x}_{j}}{s_j}\n\nThe entire set of p standardized responses for the i-th subject can be computed as\n\n\\mathbf{z}_i =\n\\begin{bmatrix}\nz_{i1} \\\\\nz_{i2} \\\\\n\\vdots \\\\\nz_{ip}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\dfrac{x_{i1} - \\bar{x}_{1}}{s_1} \\\\\n\\dfrac{x_{i2} - \\bar{x}_{2}}{s_2} \\\\\n\\vdots \\\\\n\\dfrac{x_{ip} - \\bar{x}_{p}}{s_p}\n\\end{bmatrix}\n= D^{-1/2} (\\mathbf{x}_i - \\bar{\\mathbf{x}})\n\n\n\n\nR Code: Standardized Data\ns_j = sqrt(diag(S))\n\nDsqrt = diag(1/s_j)\n\n# using matrix algebra\nZ =  (X -  matrix(1, nrow(X), 1) %*% matrix(xbar, 1)) %*% Dsqrt\n\n# using scale function \nX_scaled = as.matrix(scale(X, center=TRUE, scale=TRUE))\n\n\n# using mutate function\ndat_scaled &lt;- dat %&gt;%\n  mutate(across(everything(), ~ (.x - mean(.x)) / sd(.x)))\n\n\n\n\nR Code: Plotting the Data\np1 = ggplot(dat, aes(Murder, Assault)) + \n  geom_point(alpha=0.8) + \n  labs(title=\"Unscaled\")\n\np2 = ggplot(dat_scaled, aes(Murder, Assault)) + \n  geom_point(alpha=.8) + \n  labs(title=\"Standardized (z-scores)\")\n\npatchwork::wrap_plots(p1, p2, ncol=1)\n\n\n\n\n\n\n\n\n\n\n\n1.7.2 Standardized Population Mean\n\nThe vector of true means for a set of standardized responses is\na vector of zeros:\n\n\nE(\\mathbf{z}_i) =\nE\\begin{bmatrix}\nz_{i1} \\\\\nz_{i2} \\\\\n\\vdots \\\\\nz_{ip}\n\\end{bmatrix}\n=\nE\\begin{bmatrix}\n\\dfrac{x_{i1} - \\bar{x}_{1}}{s_1} \\\\\n\\dfrac{x_{i2} - \\bar{x}_{2}}{s_2} \\\\\n\\vdots \\\\\n\\dfrac{x_{ip} - \\bar{x}_{p}}{s_p}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{bmatrix}\n\n\nThe vector of estimated means for a set of standardized responses\nis also a vector of zeros.\n\n\n\n1.7.3 Standardized Population Covariance\n\nThe true covariance matrix for a set of standardized responses is the population correlation matrix:\n\n\n\\text{Var}(\\mathbf{z}_i) = P =\n\\begin{bmatrix}\n1 & \\rho_{12} & \\cdots & \\rho_{1p} \\\\\n\\rho_{21} & 1 & \\cdots & \\rho_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\rho_{p1} & \\rho_{p2} & \\cdots & 1\n\\end{bmatrix}\n\n\nAn estimate of the true covariance matrix for a set of standardized responses is the sample correlation matrix:\n\n\n\\widehat{\\text{Var}(\\mathbf{z}_i)} = R =\n\\begin{bmatrix}\n1 & r_{12} & \\cdots & r_{1p} \\\\\nr_{21} & 1 & \\cdots & r_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nr_{p1} & r_{p2} & \\cdots & 1\n\\end{bmatrix}\n= D^{-1/2} S D^{-1/2}",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Multivariate Data</span>"
    ]
  },
  {
    "objectID": "ch1/01-intro.html#exercises",
    "href": "ch1/01-intro.html#exercises",
    "title": "1  Introduction to Multivariate Data",
    "section": "1.8 Exercises",
    "text": "1.8 Exercises\n\n1.8.1 Exercise 1: Data Types\nCommon types of objects for data analysis are numeric, character, logical, factors, and dates. Character data do not have numerical values, such as names of people or words in a book, Logical data takes values of true or false and can be used to make decisions.\n\nFor each of the following commands, either explain why they should be errors, or explain the non-erroneous result.\n\n    x &lt;- c(\"1\",\"2\",\"3\")\n    max(x)\n    sort(x)\n    sum(x)\n\nFor the next two commands, either explain their results, or why they should produce errors.\n\n    y &lt;- c(\"1\",3,4)\n    y[2] + y[3]\n\nFor the next two commands, either explain their results, or why they should produce errors.\n\n    z &lt;- data.frame(z1=\"1\", z2=3, z3=5)\n    z[1,2] + z[1,3]\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\nx is bound to a vector of characters instead of numerical values. The functions max, sort, sum should take numerical values as input, so the values in x are first converted from characters to numerical values implicitly and then the functions apply to these incorrect numerical values\ny is a vector of characters, which cannot be used for addition\nz is a data frame, but the variable z1 is a character while z2 and z3 are numerical. Adding z2 and z3 will produce the correct results.\n\n\n\n\n\n\n1.8.2 Exercise 2: Working with Matrix and Data Frames\n\nA matrix is a 2D array, with rows and columns, much like you would have seen in linear algebra. Primarily we think of these as numeric objects. Arrays can have more than two dimensions. In multivariate analysis we are typically thinking of data in the form of a matrix, with samples/cases in the rows and variables represented as columns. Data frames are 2D arrays that could have multiple types of data in different columns. Lists are collections of possibly different length and different types of objects.\n\n\nCreate a matrix and print it out.\n\n\n\nView Solution\ndat = matrix(c(140, 120, 160, 145, 125, \n               65, 60, 63, 66, 61), \n             ncol = 2, byrow = FALSE) \n\n\n\n\nView Solution\ndat # or print(dat) \n\n\n     [,1] [,2]\n[1,]  140   65\n[2,]  120   60\n[3,]  160   63\n[4,]  145   66\n[5,]  125   61\n\n\n\nRename the column names of a matrix.\n\n\n\nView Solution\ncolnames(dat) = c(\"weight.lbs\", \"Height.in\")\ndat \n\n\n     weight.lbs Height.in\n[1,]        140        65\n[2,]        120        60\n[3,]        160        63\n[4,]        145        66\n[5,]        125        61\n\n\n\nData frames can store both columns of numeric data and columns of character data, columns of integers, and factors.\n\n\nCreate a data frame with Male and Female observations, and print out the data frame.\n\n\n\nView Solution\ndf = data.frame(\n  Weight.lbs = c(140, 120, 160, 145, 125,\n       180, 165), \n  Height.in = c(65, 60, 63, 66, 61, 70, 68),\n  Gender = c(rep(\"Female\", 5), rep(\"Male\", 2))\n  ) \n\ndf\n\n\n  Weight.lbs Height.in Gender\n1        140        65 Female\n2        120        60 Female\n3        160        63 Female\n4        145        66 Female\n5        125        61 Female\n6        180        70   Male\n7        165        68   Male\n\n\n\nGet the rows and columns of a data frame.\n\n\n\nView Solution\ndim(df)\n\n\n[1] 7 3\n\n\nView Solution\nnrow(df)\n\n\n[1] 7\n\n\nView Solution\nncol(df)\n\n\n[1] 3\n\n\n\nGet summary statistics of a data frame.\n\n\n\nView Solution\nsummary(df)\n\n\n   Weight.lbs      Height.in        Gender         \n Min.   :120.0   Min.   :60.00   Length:7          \n 1st Qu.:132.5   1st Qu.:62.00   Class :character  \n Median :145.0   Median :65.00   Mode  :character  \n Mean   :147.9   Mean   :64.71                     \n 3rd Qu.:162.5   3rd Qu.:67.00                     \n Max.   :180.0   Max.   :70.00                     \n\n\n\nOperations with Matrix and Data Frame\n\n\nExtract the first row and second column of a matrix.\n\n\n\nView Solution\ndat[1,]\n\n\nweight.lbs  Height.in \n       140         65 \n\n\nView Solution\ndat[,2]\n\n\n[1] 65 60 63 66 61\n\n\n\nSubset the first two rows of a matrix\n\n\n\nView Solution\ndat[1:2,]\n\n\n     weight.lbs Height.in\n[1,]        140        65\n[2,]        120        60\n\n\n\nSubset rows 1,3,5 of a matrix.\n\n\n\nView Solution\ndat[c(1,3,5), ]\n\n\n     weight.lbs Height.in\n[1,]        140        65\n[2,]        160        63\n[3,]        125        61\n\n\n\nSelect all the Male observations from a data frame.\n\n\n\nView Solution\nsubset(df, Gender==\"Male\")\n\n\n  Weight.lbs Height.in Gender\n6        180        70   Male\n7        165        68   Male\n\n\n\nTranspose of a matrix\n\n\n\nView Solution\nA = matrix(c(3,1,2,4), ncol=2)\nA\n\n\n     [,1] [,2]\n[1,]    3    2\n[2,]    1    4\n\n\n\nMatrix multiplication\n\n\n\nView Solution\nB = matrix(c(1,2,3,4), ncol=2)\nA %*% B \n\n\n     [,1] [,2]\n[1,]    7   17\n[2,]    9   19\n\n\nView Solution\nB %*% A \n\n\n     [,1] [,2]\n[1,]    6   14\n[2,]   10   20\n\n\n\nMatrix inversion\n\n\n\nView Solution\nsolve(A)\n\n\n     [,1] [,2]\n[1,]  0.4 -0.2\n[2,] -0.1  0.3\n\n\n\n\n1.8.3 Exercise 3: Linear Algebra\nConsider the linear system A X = b, where A is an n\\times n positive definite matrix and b is a n-dimensional vector, the unique solution is X = A^{-1}b. Please answer the following questions:\n\nWrite an R function called my_solver() such that given inputs A and b, the function my_solver() returns the solution of the linear system, i.e., X &lt;- my_solver(A, b).\nRun the following code to get A and b.\n\nset.seed(123)\nA = matrix(c(5,1,1,6), ncol=2)\nn = nrow(A)\nb = rnorm(n,1)\nThen use your function my_solver() to produce the answer and verify your solution. (hint: AX should be equal to b)\n\n\nView Solution\nmy_solver &lt;- function(A, b){\n  x = solve(A, b)\n  return(x)\n}\n\nA = matrix(c(5,1,1,6), ncol=2)\nn = nrow(A)\nb = rnorm(n,1)\nx1 = my_solver(A, b)\nsum((A%*%x1-b)^2)\n\n\n[1] 1.232595e-32\n\n\n\n\n1.8.4 Exercise 4: Working with ggplot2 Package\nEPA monitors Air Quality data across the entire U.S. The file AQSdata.csv contains daily PM 2.5 concentrations and other information. Please answer the following questions using the ggplot() function for plotting. In addition make sure that all the x-axis and y-axis labels have 14 font size.\n\nRead the data file AQSdata.csv into R.\nGenerate density plots of PM2.5 concentrations grouped by County in one single panel, where each density should have its own color. What do you find from the figure?\nPlot histograms of PM2.5 concentrations across different counties with one panel for one histogram.\nGenerate boxplots of PM2.5 concentrations by County. What would you say about the distributions?\nReorder the boxplots above by the median value of PM2.5 concentrations.\nConverting the Site ID to a factor and plot the histogram grouped by Site ID.\nGenerate the time series plot for the monitoring Site ID 450190048.\nPlot time series of PM2.5 concentrations for all monitoring sites in one panel, where each site has its own color\nPlot time series of PM2.5 concentrations across all monitoring sites in multiple panels, where one panel only has one site, and each row only has two panels.\nIn the time series plot, there seems to be not enough space to hold the x-axis labels. One way to avoid this is to rotate the axis labels. Please rotate all the time labels 45 degree.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\nlibrary(readr)\nlibrary(lubridate)\n\ndf = read_csv(\"AQSdata.csv\")\nhead(df)\n\n\n# A tibble: 6 × 20\n  Date       Source `Site ID`   POC Daily Mean PM2.5 Con…¹ UNITS DAILY_AQI_VALUE\n  &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;                  &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n1 11/09/2021 AQS    450190020     1                   15.5 ug/m…              58\n2 11/10/2021 AQS    450190020     1                   13.6 ug/m…              54\n3 11/11/2021 AQS    450190020     1                    8.1 ug/m…              34\n4 11/12/2021 AQS    450190020     1                    7.1 ug/m…              30\n5 11/13/2021 AQS    450190020     1                   10.7 ug/m…              45\n6 11/14/2021 AQS    450190020     1                    7.5 ug/m…              31\n# ℹ abbreviated name: ¹​`Daily Mean PM2.5 Concentration`\n# ℹ 13 more variables: `Site Name` &lt;chr&gt;, DAILY_OBS_COUNT &lt;dbl&gt;,\n#   PERCENT_COMPLETE &lt;dbl&gt;, AQS_PARAMETER_CODE &lt;dbl&gt;, AQS_PARAMETER_DESC &lt;chr&gt;,\n#   CBSA_CODE &lt;dbl&gt;, CBSA_NAME &lt;chr&gt;, STATE_CODE &lt;dbl&gt;, STATE &lt;chr&gt;,\n#   COUNTY_CODE &lt;chr&gt;, COUNTY &lt;chr&gt;, SITE_LATITUDE &lt;dbl&gt;, SITE_LONGITUDE &lt;dbl&gt;\n\n\nCode\ntheme_mat = theme(axis.text = element_text(size = 14),\n                  axis.title = element_text(size = 14, face = \"bold\"))\ndf = rename(df, PM2.5 = `Daily Mean PM2.5 Concentration`)\nggplot(df) +\n  geom_freqpoly(aes(\n    x = PM2.5,\n    y = after_stat(density),\n    color = COUNTY\n  ), binwidth = 2) +\n  xlab(\"Daily PM2.5 concentration (ug/m3 LC)\") +\n  theme_mat\n\n\n\n\n\n\n\n\n\nCode\nggplot(df) +\n  geom_histogram(aes(x = PM2.5), binwidth = .8) +\n  facet_wrap(~ COUNTY, ncol = 3) +\n  xlab(\"Daily PM2.5 concentration (ug/m3 LC)\") +\n  theme_mat\n\n\n\n\n\n\n\n\n\nCode\nggplot(df) +\n  geom_boxplot(aes(x = COUNTY, y = PM2.5)) +\n  xlab(\"County\") +\n  ylab(\"Daily PM2.5 concentration (ug/m3 LC)\") +\n  theme_mat\n\n\n\n\n\n\n\n\n\nCode\nggplot(df) +\n  geom_boxplot(aes(x = reorder(COUNTY, PM2.5, FUN = median), \n                   y = PM2.5)) +\n  xlab(\"County\") +\n  ylab(\"Daily PM2.5 concentration (ug/m3 LC)\") +\n  theme_mat\n\n\n\n\n\n\n\n\n\nCode\ndf1 = df\ndf1$`Site ID` =  as.factor(df$`Site ID`)\nggplot(df1) +\n  geom_freqpoly(aes(x = PM2.5, color = `Site ID`), binwidth = 2) +\n  xlab(\"Daily PM2.5 concentration (ug/m3 LC)\") +\n  theme_mat\n\n\n\n\n\n\n\n\n\nCode\ndf1 %&gt;%\n  filter(`Site ID` == 450190048) %&gt;%\n  ggplot() +\n  geom_line(aes(x = mdy(Date), y = PM2.5)) +\n  labs(x = \"Time\", y = \"Daily PM2.5 concentration (ug/m3 LC)\") + \n  theme_mat\n\n\n\n\n\n\n\n\n\nCode\ndf1 %&gt;%\n  ggplot() +\n  geom_line(aes(x = mdy(Date), y = PM2.5, color = `Site ID`)) +\n  labs(x = \"Time\", y = \"Daily PM2.5 concentration (ug/m3 LC)\") + \n  theme_mat\n\n\n\n\n\n\n\n\n\nCode\ng &lt;- df1 %&gt;%\n  ggplot() +\n  geom_line(aes(x = mdy(Date), y = PM2.5)) +\n  facet_wrap(~ `Site ID`, ncol = 2) +\n  labs(x = \"Time\", y = \"Daily PM2.5 concentration (ug/m3 LC)\") + \n  theme_mat\nprint(g)\n\n\n\n\n\n\n\n\n\nCode\ng + theme(axis.text.x = element_text(angle = 45))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.8.5 Exercise 5: Working with dplyr Package\nContinuing working with the above PM 2.5 data.\n\nFilter all the observations in the county Greenville. How many observations are there?\nFilter all the observations in Greenville in August 2021\nFilter all the observations in Greenville in August 2021 and select the variables PM2.5 concentrations, Date, latitude and longitude of sites\nGenerate scatterplots of PM2.5 against latitude and longitude in two different panels\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\nlibrary(dplyr)\n\ndf %&gt;%\n  filter(COUNTY == \"Greenville\")\n\n\n# A tibble: 937 × 20\n   Date       Source `Site ID`   POC PM2.5 UNITS    DAILY_AQI_VALUE `Site Name` \n   &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       \n 1 01/01/2021 AQS    450450015     1   6.3 ug/m3 LC              26 Greenville …\n 2 01/02/2021 AQS    450450015     1   6.6 ug/m3 LC              28 Greenville …\n 3 01/03/2021 AQS    450450015     1   5.4 ug/m3 LC              23 Greenville …\n 4 01/05/2021 AQS    450450015     1   7.4 ug/m3 LC              31 Greenville …\n 5 01/06/2021 AQS    450450015     1   7.7 ug/m3 LC              32 Greenville …\n 6 01/07/2021 AQS    450450015     1   9.5 ug/m3 LC              40 Greenville …\n 7 01/08/2021 AQS    450450015     1   7.5 ug/m3 LC              31 Greenville …\n 8 01/09/2021 AQS    450450015     1   5.3 ug/m3 LC              22 Greenville …\n 9 01/10/2021 AQS    450450015     1  12.1 ug/m3 LC              51 Greenville …\n10 01/11/2021 AQS    450450015     1  16.7 ug/m3 LC              61 Greenville …\n# ℹ 927 more rows\n# ℹ 12 more variables: DAILY_OBS_COUNT &lt;dbl&gt;, PERCENT_COMPLETE &lt;dbl&gt;,\n#   AQS_PARAMETER_CODE &lt;dbl&gt;, AQS_PARAMETER_DESC &lt;chr&gt;, CBSA_CODE &lt;dbl&gt;,\n#   CBSA_NAME &lt;chr&gt;, STATE_CODE &lt;dbl&gt;, STATE &lt;chr&gt;, COUNTY_CODE &lt;chr&gt;,\n#   COUNTY &lt;chr&gt;, SITE_LATITUDE &lt;dbl&gt;, SITE_LONGITUDE &lt;dbl&gt;\n\n\nCode\ndf %&gt;%\n  mutate(Date = mdy(Date), YM = format_ISO8601(Date, precision = \"ym\")) %&gt;%\n  filter(COUNTY == \"Greenville\", YM == \"2021-08\")\n\n\n# A tibble: 82 × 21\n   Date       Source `Site ID`   POC PM2.5 UNITS    DAILY_AQI_VALUE `Site Name` \n   &lt;date&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       \n 1 2021-08-01 AQS    450450015     1  13.8 ug/m3 LC              55 Greenville …\n 2 2021-08-02 AQS    450450015     1  19   ug/m3 LC              66 Greenville …\n 3 2021-08-03 AQS    450450015     1  16.9 ug/m3 LC              61 Greenville …\n 4 2021-08-04 AQS    450450015     1  15.6 ug/m3 LC              58 Greenville …\n 5 2021-08-05 AQS    450450015     1  11   ug/m3 LC              46 Greenville …\n 6 2021-08-06 AQS    450450015     1  10.3 ug/m3 LC              43 Greenville …\n 7 2021-08-07 AQS    450450015     1   9.7 ug/m3 LC              40 Greenville …\n 8 2021-08-08 AQS    450450015     1  10   ug/m3 LC              42 Greenville …\n 9 2021-08-09 AQS    450450015     1  12   ug/m3 LC              50 Greenville …\n10 2021-08-10 AQS    450450015     1  12.5 ug/m3 LC              52 Greenville …\n# ℹ 72 more rows\n# ℹ 13 more variables: DAILY_OBS_COUNT &lt;dbl&gt;, PERCENT_COMPLETE &lt;dbl&gt;,\n#   AQS_PARAMETER_CODE &lt;dbl&gt;, AQS_PARAMETER_DESC &lt;chr&gt;, CBSA_CODE &lt;dbl&gt;,\n#   CBSA_NAME &lt;chr&gt;, STATE_CODE &lt;dbl&gt;, STATE &lt;chr&gt;, COUNTY_CODE &lt;chr&gt;,\n#   COUNTY &lt;chr&gt;, SITE_LATITUDE &lt;dbl&gt;, SITE_LONGITUDE &lt;dbl&gt;, YM &lt;chr&gt;\n\n\nCode\ndf %&gt;%\n  mutate(Date = mdy(Date), YM = format_ISO8601(Date, precision = \"ym\")) %&gt;%\n  filter(COUNTY == \"Greenville\", YM == \"2021-08\") %&gt;%\n  dplyr::select(PM2.5, Date, SITE_LATITUDE, SITE_LONGITUDE)\n\n\n# A tibble: 82 × 4\n   PM2.5 Date       SITE_LATITUDE SITE_LONGITUDE\n   &lt;dbl&gt; &lt;date&gt;             &lt;dbl&gt;          &lt;dbl&gt;\n 1  13.8 2021-08-01          34.8          -82.4\n 2  19   2021-08-02          34.8          -82.4\n 3  16.9 2021-08-03          34.8          -82.4\n 4  15.6 2021-08-04          34.8          -82.4\n 5  11   2021-08-05          34.8          -82.4\n 6  10.3 2021-08-06          34.8          -82.4\n 7   9.7 2021-08-07          34.8          -82.4\n 8  10   2021-08-08          34.8          -82.4\n 9  12   2021-08-09          34.8          -82.4\n10  12.5 2021-08-10          34.8          -82.4\n# ℹ 72 more rows\n\n\nCode\ndf %&gt;%\n  dplyr::select(PM2.5, SITE_LATITUDE, SITE_LONGITUDE) %&gt;%\n  pivot_longer(\n    cols = c(\"SITE_LATITUDE\", \"SITE_LONGITUDE\"),\n    names_to = \"variable\",\n    values_to = \"value\"\n  ) %&gt;%\n  ggplot(aes(x = value, y = PM2.5)) +\n  geom_point() +\n  facet_wrap( ~ variable, scale = \"free\") +\n  xlab(\"\") +\n  ylab(\"Daily PM2.5 concentration (ug/m3 LC)\") +\n  theme_mat",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Multivariate Data</span>"
    ]
  },
  {
    "objectID": "ch2/02-graphs.html",
    "href": "ch2/02-graphs.html",
    "title": "2  Graphs and Data Visualization",
    "section": "",
    "text": "2.1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphs and Data Visualization</span>"
    ]
  },
  {
    "objectID": "ch2/02-graphs.html#introduction",
    "href": "ch2/02-graphs.html#introduction",
    "title": "2  Graphs and Data Visualization",
    "section": "",
    "text": "Graphs reveal information about the center, shape, and spread of distributions.\nThey can also show typical and extreme outcomes, associations, and differences between groups.\nDifferent data types require different graph types.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphs and Data Visualization</span>"
    ]
  },
  {
    "objectID": "ch2/02-graphs.html#categorical-data",
    "href": "ch2/02-graphs.html#categorical-data",
    "title": "2  Graphs and Data Visualization",
    "section": "2.2 Categorical Data",
    "text": "2.2 Categorical Data\nCategorical data types:\n\nNominal: no natural order (e.g., eye color, blood type).\nOrdinal: ordered, but spacing not meaningful (e.g., therapy response).\nInterval: ordered and evenly spaced (e.g., fruit count).\n\n\n2.2.1 Bar Chart Example\nBar chart is used for displaying categorical variables.\n\n\nR Code: Bart Chart\ncolors &lt;- c('Red', 'Pink', 'White')\ncounts &lt;- c(80, 60, 40)\nbar_df &lt;- data.frame(Color = colors, Count = counts)\nggplot(bar_df, aes(x = Color, y = Count)) +\n  geom_bar(stat = 'identity', fill = 'steelblue') +\n  theme_minimal() +\n  labs(title = 'Poinsettia Colors')\n\n\n\n\n\n\n\n\n\n\nQuestion: What does the bar plot tell you about the color distribution for this poinsettia variety?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphs and Data Visualization</span>"
    ]
  },
  {
    "objectID": "ch2/02-graphs.html#quantitative-data",
    "href": "ch2/02-graphs.html#quantitative-data",
    "title": "2  Graphs and Data Visualization",
    "section": "2.3 Quantitative Data",
    "text": "2.3 Quantitative Data\n\nMeasured on a numeric scale: e.g., weights, heights, cholesterol levels.\nUse histograms, boxplots, dotplots, density plots.\n\n\n2.3.1 Histogram Example (Tip Amounts)\nHistograms classify values into bins of equal width\n\nHeights of bars represent relative frequencies\n\n\n\nR Code: Histogram\ndata(\"tips\", package=\"reshape2\")\nggplot(tips, aes(x = tip)) +\n  geom_histogram(binwidth = 1, fill = 'orange', color = 'black') +\n  theme_minimal() +\n  labs(title = 'Histogram of Tip Amounts', x = 'Tip ($)', y = 'Count')\n\n\n\n\n\n\n\n\n\n\nQuestion: What does this histogram tell you about tips at this restaurant?\n\n\n\n\n\n\n\nTip\n\n\n\n\nCenter\nShape\nSpread\n\n\n\n\n\n2.3.2 Try Different Bin Widths\n\n\nR Code: Histograms with Different Bin Widths\nbw_list &lt;- c(0.25, 0.5, 1.0)\nplots &lt;- lapply(bw_list, function(bw) {\n  ggplot(tips, aes(x = tip)) +\n    geom_histogram(binwidth = bw, fill = 'lightblue', color = 'black') +\n    ggtitle(paste('Bin Width =', bw)) +\n    theme_minimal()\n})\nlibrary(gridExtra)\ndo.call(grid.arrange, c(plots, ncol = 1))\n\n\n\n\n\n\n\n\n\n\n\n2.3.3 Boxplot\nBoxplot is graphical display of the five number data summary (minimum, Q1, median, Q3, maximum). It is good for comparing samples from different populations.\n\n\nR Code: Boxplot\nggplot(tips, aes(y = tip)) +\n  geom_boxplot(fill = 'lightgreen') +\n  theme_minimal() +\n  labs(title = 'Boxplot of Tip Amounts', y = 'Tip ($)')",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphs and Data Visualization</span>"
    ]
  },
  {
    "objectID": "ch2/02-graphs.html#one-quantitative-one-categorical",
    "href": "ch2/02-graphs.html#one-quantitative-one-categorical",
    "title": "2  Graphs and Data Visualization",
    "section": "2.4 One Quantitative & One Categorical",
    "text": "2.4 One Quantitative & One Categorical\nSide by side box plots and dot plots can be used to compare distributions of a quantitative response variable for different levels of a categorical variable.\n\n\nR Code: Boxplots with Grouping\nggplot(tips, aes(x = day, y = tip)) +\n  geom_boxplot(fill = 'skyblue') +\n  theme_minimal() +\n  labs(title = 'Tip Amount by Day of Week')",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphs and Data Visualization</span>"
    ]
  },
  {
    "objectID": "ch2/02-graphs.html#two-quantitative-variables",
    "href": "ch2/02-graphs.html#two-quantitative-variables",
    "title": "2  Graphs and Data Visualization",
    "section": "2.5 Two Quantitative Variables",
    "text": "2.5 Two Quantitative Variables\nScatterplots convey information about associations between quantitative variables and also about unusual observations.\n\n\nR Code: Scatterplot\ntips$bill &lt;- tips$total_bill\nggplot(tips, aes(x = bill, y = tip)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se = FALSE, col = 'red') +\n  theme_minimal() +\n  labs(title = 'Tip vs Bill Amount', x = 'Bill ($)', y = 'Tip ($)')\n\n\n\n\n\n\n\n\n\n\nHow is the tip related to the bill?\nWhich variable is the response? Which is the explanatory variable?\nWhat type of relationship would you expect?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphs and Data Visualization</span>"
    ]
  },
  {
    "objectID": "ch2/02-graphs.html#practice-problems",
    "href": "ch2/02-graphs.html#practice-problems",
    "title": "2  Graphs and Data Visualization",
    "section": "2.6 Practice Problems",
    "text": "2.6 Practice Problems\n\n2.6.1 Exercise 1: Bar Chart from Survey\nCreate a bar chart for the following survey results:\n\n\n\nPet Type\nCount\n\n\n\n\nDog\n15\n\n\nCat\n10\n\n\nFish\n3\n\n\nBird\n2\n\n\n\nUse ggplot2 package to visualize the data and describe what the chart tells you about pet preferences.\n\n\nView Solution\nlibrary(ggplot2)\npet_df &lt;- data.frame(\n  Pet = c(\"Dog\", \"Cat\", \"Fish\", \"Bird\"),\n  Count = c(15, 10, 3, 2)\n)\nggplot(pet_df, aes(x = Pet, y = Count, fill = Pet)) +\n  geom_bar(stat = \"identity\") +\n  theme_minimal() +\n  labs(title = \"Pet Types\", x = \"Pet Type\", y = \"Count\") +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\n\n\n\n2.6.2 Exercise 2: Simulated Histogram\nSimulate 300 observations from a normal distribution with mean 70 and standard deviation 10.\n\nPlot a histogram using binwidth = 5 and 10.\nOverlay a density curve using geom_density().\nDescribe the center, spread, and shape of the distribution.\n\n\n\nView Solution\nset.seed(4750)\nx &lt;- rnorm(300, mean = 70, sd = 10)\n# Histogram with binwidth 5\np1 &lt;- ggplot(data.frame(x), aes(x = x)) +\n  geom_histogram(\n    binwidth = 5,\n    fill = \"skyblue\",\n    color = \"white\",\n    alpha = 0.7\n  ) +\n  geom_density(aes(y = after_stat(count) * 5),\n               color = \"red\",\n               linewidth = 1.2) +\n  labs(title = \"Histogram (binwidth = 5) with Density\", x = \"Value\", y = \"Count\") +\n  theme_minimal()\n\n# Histogram with binwidth 10\np2 &lt;- ggplot(data.frame(x), aes(x = x)) +\n  geom_histogram(\n    binwidth = 10,\n    fill = \"orange\",\n    color = \"white\",\n    alpha = 0.7\n  ) +\n  geom_density(aes(y =  after_stat(count) * 10),\n               color = \"red\",\n               linewidth = 1.2) +\n  labs(title = \"Histogram (binwidth = 10) with Density\", x = \"Value\", y = \"Count\") +\n  theme_minimal()\n\nlibrary(patchwork)\np1 / p2\n\n\n\n\n\n\n\n\n\n\n\n2.6.3 Exercise 3: Boxplots by Group\nSimulate exam scores for two groups of students (Group A and Group B), each with 50 observations.\n\nGenerate scores from rnorm(50, mean=80, sd=5) for Group A and rnorm(50, mean=75, sd=7) for Group B.\nCreate a combined data.frame and make a boxplot comparing the two groups.\nInterpret differences in central tendency and variability.\n\n\n\nView Solution\nset.seed(4750)\ngroup_a &lt;- rnorm(50, mean = 80, sd = 5)\ngroup_b &lt;- rnorm(50, mean = 75, sd = 7)\n\nscores &lt;- data.frame(\n  Score = c(group_a, group_b),\n  Group = rep(c(\"A\", \"B\"), each = 50)\n)\n\nlibrary(ggplot2)\nggplot(scores, aes(x = Group, y = Score, fill = Group)) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(title = \"Exam Scores by Group\", y = \"Score\")\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nCentral tendency: Group A has a higher median and mean score than Group B.\nVariability: Group B shows a wider spread (greater interquartile range and more outliers) compared to Group A, which aligns with the larger standard deviation used in its simulation.\n\n\n\n2.6.4 Exercise 4: Scatterplot with Regression\nSimulate a dataset of 100 observations where x ~ runif(100, 0, 100) and y = 0.5 * x + rnorm(100, 0, 5).\n\nCreate a scatterplot of y vs x.\nAdd a regression line.\nDescribe the relationship and interpret the slope.\n\n\n\nView Solution\nset.seed(4750)\nx &lt;- runif(100, 0, 100)\ny &lt;- 0.5 * x + rnorm(100, 0, 5)\ndata &lt;- data.frame(x = x, y = y)\n\nggplot(data, aes(x = x, y = y)) +\n  geom_point(color = \"steelblue\", alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(title = \"Scatterplot of y vs x with Regression Line\",\n       x = \"x\",\n       y = \"y\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nThere is a strong positive linear relationship between x and y, as expected from the model.\nThe slope of the regression line is close to 0.5, indicating that, on average, each 1-unit increase in x results in a 0.5-unit increase in y.\nThe scatter around the regression line reflects the normal error term with standard deviation 5.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphs and Data Visualization</span>"
    ]
  },
  {
    "objectID": "ch2/02-graphs.html#two-quantitative-one-categorical",
    "href": "ch2/02-graphs.html#two-quantitative-one-categorical",
    "title": "2  Graphs and Data Visualization",
    "section": "2.7 Two Quantitative & One Categorical",
    "text": "2.7 Two Quantitative & One Categorical\n\nGrouping: a graph consisting of a single panel with multiple variables differentiated using different visual characteristics such as color, shape, and size.\nFaceting: a graph consisting of several separate panels, with one for each level of the faceted variable, or combination of two faceted variables.\n\n\n\nR Code: Grouping\nggplot(tips, aes(x = bill, y = tip, color = sex)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = 'lm', se = FALSE) +\n  labs(title = 'Tip vs Bill by Gender')\n\n\n\n\n\n\n\n\n\n\n\nR Code: Faceting\nggplot(tips, aes(x=bill, y=tip)) + \n  geom_point(alpha=0.6) + \n  geom_smooth(method = 'lm', se = FALSE) + \n  facet_wrap(~ sex, ncol=2) + \n  labs(title = 'Tip vs Bill by Gender')\n\n\n\n\n\n\n\n\n\n\nDoes the relationship differ for men and women?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphs and Data Visualization</span>"
    ]
  },
  {
    "objectID": "ch2/02-graphs.html#two-quantitative-two-categorical-variables",
    "href": "ch2/02-graphs.html#two-quantitative-two-categorical-variables",
    "title": "2  Graphs and Data Visualization",
    "section": "2.8 Two Quantitative & Two Categorical Variables",
    "text": "2.8 Two Quantitative & Two Categorical Variables\n\n\nR Code: Grouping + Faceting\nggplot(tips, aes(x = bill, y = tip, color = sex)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_grid(smoker ~ .) +\n  labs(title = \"Tip vs Bill by Sex and Smoking Status\",\n       x = \"Bill ($)\", y = \"Tip ($)\")\n\n\n\n\n\n\n\n\n\n\n\nR Code: Faceting Only\nggplot(tips, aes(x = bill, y = tip, color = sex)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_grid(smoker ~ sex) +\n  labs(title = \"Tip vs Bill by Sex and Smoking Status\",\n       x = \"Bill ($)\", y = \"Tip ($)\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphs and Data Visualization</span>"
    ]
  },
  {
    "objectID": "ch2/02-graphs.html#three-variables",
    "href": "ch2/02-graphs.html#three-variables",
    "title": "2  Graphs and Data Visualization",
    "section": "2.9 Three Variables",
    "text": "2.9 Three Variables\n\n2.9.1 3D Scatterplot\n\n\nCode\nlibrary(plotly)\ndata(mtcars)\n# Example with mtcars\nplot_ly(\n  data = mtcars,\n  x = ~mpg, y = ~hp, z = ~wt,\n  type = 'scatter3d',\n  mode = 'markers',\n  color = ~as.factor(cyl)\n)\n\n\n\n\n\n\n\n\n2.9.2 Bubble Chart\n\n\nCode\nggplot(mtcars, \n       aes(x=mpg, y=hp, size=wt)) + \n  geom_point(alpha=.5, \n             fill=\"red\", \n             color=\"black\",\n             shape=21) + \n  scale_size_continuous(range = c(1, 5))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphs and Data Visualization</span>"
    ]
  },
  {
    "objectID": "ch2/02-graphs.html#scatterplot-matrix",
    "href": "ch2/02-graphs.html#scatterplot-matrix",
    "title": "2  Graphs and Data Visualization",
    "section": "2.10 Scatterplot Matrix",
    "text": "2.10 Scatterplot Matrix\nA scatterplot matrix (sometimes called a “pairs plot”) is used to visualize the pairwise relationships between several quantitative variables in a dataset, all at once.\n\nEach row and column represents one variable.\nThe off-diagonal panels show scatterplots for each pair of variables (e.g., x vs y, x vs z, y vs z).\nThe diagonal often shows the distribution of each variable (as a histogram, density, or boxplot).\nYou can optionally color points by a group or class variable.\n\n\n\nR Code: Scatterplot Matrix\ndata(mtcars)\nlibrary(GGally)\nGGally::ggpairs(\n  mtcars,\n  columns = c(\"mpg\", \"hp\", \"wt\")\n)\n\n\n\n\n\n\n\n\n\n\n\nScatterplot Matrix with Base R\npairs(mtcars[,c(\"mpg\", \"hp\", \"wt\")])\n\n\n\n\n\n\n\n\n\n\n\nR Code: Sample Correlation\ncor(mtcars[,c(\"mpg\", \"hp\", \"wt\")])\n\n\n           mpg         hp         wt\nmpg  1.0000000 -0.7761684 -0.8676594\nhp  -0.7761684  1.0000000  0.6587479\nwt  -0.8676594  0.6587479  1.0000000",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphs and Data Visualization</span>"
    ]
  },
  {
    "objectID": "ch2/02-graphs.html#parallel-coordinates",
    "href": "ch2/02-graphs.html#parallel-coordinates",
    "title": "2  Graphs and Data Visualization",
    "section": "2.11 Parallel Coordinates",
    "text": "2.11 Parallel Coordinates\nA parallel coordinates plot is a powerful visualization tool used for exploring and comparing multivariate data (data with several quantitative variables). It’s especially valuable when you want to see how patterns, clusters, or outliers appear across many variables at once.\n\n\nR Code: Parallel Coordinates Plot\n# Convert cyl to factor for coloring\nmtcars$cyl &lt;- as.factor(mtcars$cyl)\n\nGGally::ggparcoord(\n  mtcars,\n  columns = c(1,4,6), # mpg, hp, wt\n  groupColumn = \"cyl\",\n  scale = \"uniminmax\",    # Normalize to [0,1] for fair comparison\n  showPoints = TRUE,\n  alphaLines = 0.6\n) +\n  scale_color_brewer(palette = \"Dark2\", name = \"Cylinders\") +\n  labs(title = \"Parallel Coordinates Plot: mtcars (by cylinders)\")\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nEach line is a car.\nThe color indicates the number of cylinders.\nCars with more cylinders generally have lower mpg and higher hp and weight.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphs and Data Visualization</span>"
    ]
  },
  {
    "objectID": "ch2/02-graphs.html#exercises",
    "href": "ch2/02-graphs.html#exercises",
    "title": "2  Graphs and Data Visualization",
    "section": "2.12 Exercises",
    "text": "2.12 Exercises\n\n2.12.1 Exercise 1: Ames Housing Data\nLoad the Ames Housing data into R and answer the following questions:\n\nAmes = read.csv(\"Ameshousing.csv\")\nhead(Ames)\n\n  SalePrice Bedrooms LotArea LivingArea GarageArea Neighborhood\n1    270000        4   11792       2283        632      Gilbert\n2    377500        3   14892       1746        758      Gilbert\n3    337500        3   12456       1718        786      NridgHt\n4    462000        4   14257       2772        754      NridgHt\n5    489900        2   14803       2084       1220      NridgHt\n6    555000        2   15431       2402        672      NridgHt\n\n\n\nSummary Statistics\n\n\nHow many homes were included in this study?\n\nWhat are the names of the variables for which information was collected?\nCompute the median living area, the mean living area and the standard error for the mean living area for all of the houses in the data set.\nCompute the median living area, the mean living area and the standard error for the mean living area for the houses in each neighborhood.\nCompute the number of houses, mean sale price and the standard error of the mean sale price for houses with living areas less than 1800 square feet.\nSummarize in a paragraph the information in the correlation matrix about associations between sales price, lot size, living area, garage area, and number of rooms from the sample correlation matrix.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\ndim(Ames)[1]\n\n\n[1] 168\n\n\n\n\nCode\nnames(Ames)\n\n\n[1] \"SalePrice\"    \"Bedrooms\"     \"LotArea\"      \"LivingArea\"   \"GarageArea\"  \n[6] \"Neighborhood\"\n\n\n\n\nCode\nmean_LivingArea &lt;- mean(Ames$LivingArea)\nmean_LivingArea\n\n\n[1] 1700.107\n\n\nCode\nmedian_LivingArea &lt;- median(Ames$LivingArea)\nsderr_LivingArea &lt;- sd(Ames$LivingArea)/ sqrt(length(Ames$LivingArea))\nsderr_LivingArea\n\n\n[1] 32.32775\n\n\n\n\nCode\nmean_LivingArea &lt;- tapply(Ames$LivingArea, Ames$Neighborhood, mean)\nmean_LivingArea\n\n\n CollgCr  Gilbert  NridgHt \n1565.921 1658.966 1880.921 \n\n\nCode\nmedian_LivingArea &lt;- tapply(Ames$LivingArea, Ames$Neighborhood, median)\nmedian_LivingArea\n\n\nCollgCr Gilbert NridgHt \n 1591.5  1560.0  1743.0 \n\n\nCode\nsd_LivingArea &lt;- tapply(Ames$LivingArea, Ames$Neighborhood, sd)\nsderr_LivingArea &lt;- sd_LivingArea / sqrt(tapply(Ames$LivingArea, Ames$Neighborhood, length))\nsderr_LivingArea\n\n\n CollgCr  Gilbert  NridgHt \n42.72812 56.21652 57.40376 \n\n\n\n\nCode\nAmes2 &lt;- Ames[ Ames$LivingArea&lt;1800, ]\nn &lt;- dim(Ames2)[1]\ncat(\"number of houses with living area &lt; 1800: \", n)\n\n\nnumber of houses with living area &lt; 1800:  116\n\n\n\n\nCode\nmean_Saleprice &lt;- mean(Ames2$SalePrice)\ncat(\"Mean Sale Price for Houses with Living Area &lt; 1800: \", mean_Saleprice)\n\n\nMean Sale Price for Houses with Living Area &lt; 1800:  215756.7\n\n\n\n\nCode\nsderr_SalePrice &lt;- sd(Ames2$SalePrice)/ sqrt(length(Ames2$SalePrice))\ncat(\"Stderror for Mean Sale Price: \", sderr_SalePrice)\n\n\nStderror for Mean Sale Price:  4693.031\n\n\n\n\n\n\nGraphical Summaries\n\n\nCreate a scatterplot with a smooth curve passed through the points.\nFit a least squares regression line to the data in the plot.\nWhat does the regression line indicate about sales prices of houses increase with living area?\nWhat information is provided by the plot with smooth curve?\nAdd a new variable to the data frame that contains information on sales price divided by the living area in the house, i.e., the sales price per square foot of living area.\nCreate a histogram for the price per square foot of living space.\nWhat does this histogram reveal about the distribution of costs per square foot of living space for houses sold in the Ames area? The description should be based on the shape, center and spread of the distribution.\nCreate separate plots of sales prices versus living space categorized for each neighborhood\nConstruct side-by-side box plots to compare prices per square foot of living space across neighborhoods.\nDescribe the relationship between sales price and total living area of house changes across the three neighborhoods.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nAmes %&gt;% \n  ggplot(aes(x=LivingArea, y=SalePrice)) + \n    geom_point(alpha=.7) + \n    geom_smooth() + \n    labs(\n      x=\"Living Area (sq ft)\",\n      y=\"Sale Price (dollars)\",\n      title=\"Sale Price vs Total Living Area\"\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\nlm(SalePrice ~ LivingArea, data = Ames)\n\n\n\nCall:\nlm(formula = SalePrice ~ LivingArea, data = Ames)\n\nCoefficients:\n(Intercept)   LivingArea  \n   -21769.1        159.5  \n\n\nCode\nplot( Ames$LivingArea, Ames$SalePrice, \n      xlab=\"Living Area (sq ft)\", \n      ylab=\"Sale Price (dollars)\",\n      main = \"Sale Price vs Total Living Area\")\nabline(lm(SalePrice~LivingArea, data=Ames), lty=1)\n\n\n\n\n\n\n\n\n\n\nThe slope of the regression suggests that the mean sale price of homes in the Ames area goes up by about 159.50 dollars for each additional square foot of living space.\nThe smooth curve indicates that there is an approximate straight line relationship between the mean home price and the living area of homes for homes with between 800 and 2000 square feet of living area, but the relationship curves up for larger home sizes. Also the variation in sale prices tends to increase with the amount of living area in the house.\n\n\n\nCode\nAmes$pricesqft  &lt;-  Ames$SalePrice/Ames$LivingArea \nhead(Ames)\n\n\n  SalePrice Bedrooms LotArea LivingArea GarageArea Neighborhood pricesqft\n1    270000        4   11792       2283        632      Gilbert  118.2654\n2    377500        3   14892       1746        758      Gilbert  216.2085\n3    337500        3   12456       1718        786      NridgHt  196.4494\n4    462000        4   14257       2772        754      NridgHt  166.6667\n5    489900        2   14803       2084       1220      NridgHt  235.0768\n6    555000        2   15431       2402        672      NridgHt  231.0575\n\n\n\n\nCode\nAmes %&gt;% ggplot() + \n  geom_histogram(aes(pricesqft), binwidth =20) \n\n\n\n\n\n\n\n\n\n\nThis histogram indicates that the distribution of costs per square foot of living space for houses sold in the Ames area is centered around $145 per square foot. The distribution is skewed to the right and it appears to be bimodal with one mode around $125 per square foot and another near $165 pre sqaure foot. Most houses cost between $100 and $200 per square foot of living space.\n\n\n\nCode\nAmes %&gt;% \n  ggplot(aes(x=LivingArea, y=SalePrice)) + \n  geom_point() + \n  geom_smooth() + \n  facet_grid( . ~ Neighborhood) + \n  labs(\n    x=\"Living Area (sq ft)\",\n    y=\"Sale Price (dollars)\",\n    title=\"Sales Price vs Total Living Area\"\n  )\n\n\n\n\n\n\n\n\n\nCode\nAmes %&gt;% \n  ggplot(aes(x=LivingArea, y=SalePrice)) + \n  geom_point() + \n  geom_smooth() + \n  facet_grid(Neighborhood ~ .) + \n  labs(\n    x=\"Living Area (sq ft)\",\n    y=\"Sale Price (dollars)\",\n    title=\"Sales Price vs Total Living Area\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nAmes %&gt;% \n  ggplot(aes(x=Neighborhood, y=pricesqft)) + \n  geom_boxplot() + \n  labs(y=\"Sales Price per Square Foot\")\n\n\n\n\n\n\n\n\n\n\nIn all three neighborhoods sales prices tend to be larger for house with more living area. In the College Circle (CollgCr) and Northridge Heights (NridgHt) neighborhoods, there are strong linear relationships between these two variables. Because the slope is larger for houses in the Northridge Heights neighborhood, the price per square foot of living space is higher. There are more expensive houses in the Northridge Heights neighborhood. In Gilbert, the trend in sales prices is not so close to a straight line. There is little trend in sales prices for houses with less than 1,750 square feet of living space. There is also little trend in the sales prices for houses with living space above 1,900 square feet, but those houses are more expensive than house with less then 17,50 square feet. There is one relatively expensive house with about 1,750 square feet of living space that appears to be an outlier. Perhaps this house has an extremely large lot size or some additional buildings on the property. This should be investigated.\n\n\n\n\n\n\n2.12.2 Exercise 2: Music Clips Data\nThe music clips data is posted in music-plusnew-sub.csv. The data file has five quantitative variables containing audio information from 62 songs. The first two columns (Artist, Type) describe the artist and type of music. The raw data come from a time series for the sound produced by each music clip (track). For each time series the variance of amplitude, average amplitude, maximum amplitude, and two additional variables calculated from the spectral decomposition of the time series are calculated. The Type variable classifies the tracks as either Rock, Classical or New Wave, and there are 5 tracks that are not identified.\n\nRead the data into a data frame, indicating that the row names are in column 1 of the data file and that column is not a variable.\nObtain information on the dimensions of the data frame. Also list the column names. List the first six columns odf data.\nFirst select a subset of the data that contains only classical and rock music.\nFor classical and rock music make histograms for the avergae amplitude variable (LAve) faceted by Type. Set the binwidth to units of 10. How do the distributions of average amplitude values differ between classical and rock music?\nMake a scatterplot of LVar vs LAve, with points colored by the type of music. Describe differences between the patterns of the points on the plot corresponding to Rock and Classical music.\nSelect three music types. The other songs have missing values for the music type.\nMake a parallel coordinate plot\nReorder how the variables appear on the plot.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\ndat = read.csv(\"music-plusnew-sub.csv\", row.names=1)  \n\nhead(dat) \n\n\n              Artist Type     LVar      LAve  LMax   LFEner     LFreq\nDancing Queen   Abba Rock 17600756 -90.00687 29921 105.9210  59.57379\nKnowing Me      Abba Rock  9543021 -75.76672 27626 102.8362  58.48031\nTake a Chance   Abba Rock  9049482 -98.06292 26372 102.3249 124.59397\nMamma Mia       Abba Rock  7557437 -90.47106 28898 101.6165  48.76513\nLay All You     Abba Rock  6282286 -88.95263 27940 100.3008  74.02039\nSuper Trouper   Abba Rock  4665867 -69.02084 25531 100.2485  81.40140\n\n\nCode\nstr(dat)\n\n\n'data.frame':   62 obs. of  7 variables:\n $ Artist: chr  \"Abba\" \"Abba\" \"Abba\" \"Abba\" ...\n $ Type  : chr  \"Rock\" \"Rock\" \"Rock\" \"Rock\" ...\n $ LVar  : num  17600756 9543021 9049482 7557437 6282286 ...\n $ LAve  : num  -90 -75.8 -98.1 -90.5 -89 ...\n $ LMax  : int  29921 27626 26372 28898 27940 25531 14699 8928 22962 15517 ...\n $ LFEner: num  106 103 102 102 100 ...\n $ LFreq : num  59.6 58.5 124.6 48.8 74 ...\n\n\nCode\nsummary(dat)\n\n\n    Artist              Type                LVar                LAve        \n Length:62          Length:62          Min.   :   293608   Min.   :-98.063  \n Class :character   Class :character   1st Qu.:  2844213   1st Qu.: -6.253  \n Mode  :character   Mode  :character   Median :  8210359   Median : -5.662  \n                                       Mean   : 19951792   Mean   : -7.807  \n                                       3rd Qu.: 24547475   3rd Qu.:  1.962  \n                                       Max.   :129472199   Max.   :216.232  \n      LMax           LFEner           LFreq       \n Min.   : 2985   Min.   : 83.88   Min.   : 41.41  \n 1st Qu.:16200   1st Qu.:101.69   1st Qu.: 99.18  \n Median :24431   Median :104.35   Median :175.29  \n Mean   :22486   Mean   :104.03   Mean   :231.39  \n 3rd Qu.:29918   3rd Qu.:108.15   3rd Qu.:315.12  \n Max.   :32766   Max.   :114.00   Max.   :877.77  \n\n\nCode\ntable(dat$Type)\n\n\n\nClassical  New wave      Rock \n       24         3        30 \n\n\nCode\ndf.sub = dat %&gt;% \n  dplyr::filter(Type==\"Rock\" | Type==\"Classical\")\n\nggplot(df.sub, aes(LAve)) + \n  geom_histogram(binwidth=10) + \n  facet_wrap( ~ Type, ncol=1)\n\n\n\n\n\n\n\n\n\nCode\nggplot(df.sub, aes(x=LVar, y=LAve, color=Type)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\nCode\ndat2 = dat %&gt;% \n  filter(Type==\"Rock\" | Type==\"Classical\" | Type==\"New wave\")\nhead(dat2)\n\n\n              Artist Type     LVar      LAve  LMax   LFEner     LFreq\nDancing Queen   Abba Rock 17600756 -90.00687 29921 105.9210  59.57379\nKnowing Me      Abba Rock  9543021 -75.76672 27626 102.8362  58.48031\nTake a Chance   Abba Rock  9049482 -98.06292 26372 102.3249 124.59397\nMamma Mia       Abba Rock  7557437 -90.47106 28898 101.6165  48.76513\nLay All You     Abba Rock  6282286 -88.95263 27940 100.3008  74.02039\nSuper Trouper   Abba Rock  4665867 -69.02084 25531 100.2485  81.40140\n\n\nCode\nGGally::ggparcoord(dat2, columns=3:7,\n                   groupColumn = \"Type\",\n                   title=\"Parallel Coordinate Plot: Music Types\")\n\n\n\n\n\n\n\n\n\nCode\nGGally::ggparcoord(dat2, columns=c(4,3,5,6,7),\n                   groupColumn = \"Type\",\n                   title=\"Parallel Coordinate Plot: Music Types\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphs and Data Visualization</span>"
    ]
  },
  {
    "objectID": "ch3/03-MVN.html",
    "href": "ch3/03-MVN.html",
    "title": "3  Multivariate Normal Distribution",
    "section": "",
    "text": "3.1 Why Multivariate Normal?\nThe univariate normal distribution is fundamental in statistics. But real data usually involve multiple variables measured on the same subjects, and these are often correlated.\nRecall that if a random variable X has a normal distribution with mean \\mu and variance \\sigma^2, its density function has the form \nf(x) = \\frac{1}{ \\sqrt{2 \\pi } \\sigma} \\exp \\left\\{-\\frac{1}{2\\sigma^2} (x - \\mu)^2 \\right\\}, \\;\\;\\;\\; -\\infty &lt; x &lt; \\infty.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "ch3/03-MVN.html#why-multivariate-normal",
    "href": "ch3/03-MVN.html#why-multivariate-normal",
    "title": "3  Multivariate Normal Distribution",
    "section": "",
    "text": "Height and weight of students, or exam scores in different subjects.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "ch3/03-MVN.html#sec-matrix-background",
    "href": "ch3/03-MVN.html#sec-matrix-background",
    "title": "3  Multivariate Normal Distribution",
    "section": "3.2 Matrix Background",
    "text": "3.2 Matrix Background\n\nThe inverse of a p\\times p matrix A is denoted by A^{-1}.\nif A^{-1} exists, A^{-1} is the unique matrix such that      AA^{-1} = A^{-1}A = I  =  \\left[ \\begin{array}{ccccc}  1 & 0 & \\cdots & 0 & 0 \\\\\n                                                              0 & 1 & \\cdots & 0 & 0  \\\\   \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n                                                              0 & 0 & \\cdots & 1 & 0  \\\\ 0 & 0 & \\cdots & 0 & 1  \\end{array} \\right]\n\n\nA is said to be symmetric if A is the same as its transpose, i.e., A=A^\\top or A = A'.\nA is said to be positive definite if \n\\left[ \\begin{array}{cccc} x_1 & x_2 & \\cdots & x_p \\end{array} \\right]\nA\n\\left[ \\begin{array}{c} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_p \\end{array} \\right]\n&gt; 0 \\quad \\text{for any} \\quad\n\\left[ \\begin{array}{cccc} x_1 & x_2 & \\cdots & x_p \\end{array} \\right] \\neq \\mathbf{0}\n\nIf A is positive definite, then A^{-1} exists.\n\n\n\n\n\n\n\nExercise: Matrix Algebra\n\n\n\n\n\n\nA = matrix(c(2,1,1,4), ncol=2, byrow=TRUE)\nA\n\n     [,1] [,2]\n[1,]    2    1\n[2,]    1    4\n\n\n\n\nR Code: Scalar-matrix multiplication\na = 2 \na * A\n\n\n     [,1] [,2]\n[1,]    4    2\n[2,]    2    8\n\n\n\n\nR Code: Matrix-matrix addition\nB = matrix(c(1,2,2,9), ncol=2, byrow=TRUE)\nA + B \n\n\n     [,1] [,2]\n[1,]    3    3\n[2,]    3   13\n\n\n\n\nR Code: Matrix-vector multiplication\nx = c(1,2)\nA %*% x \n\n\n     [,1]\n[1,]    4\n[2,]    9\n\n\n\n\nR Code: Matrix-matrix multiplication\nA %*% B \n\n\n     [,1] [,2]\n[1,]    4   13\n[2,]    9   38\n\n\n\n\nR Code: Matrix inversion\n# matrix inversion\nsolve(A)\n\n\n           [,1]       [,2]\n[1,]  0.5714286 -0.1428571\n[2,] -0.1428571  0.2857143\n\n\nR Code: Matrix inversion\n# check result\nA %*% solve(A)\n\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n\n\n\n\n\n3.2.1 Eigenvalues and Eigenvectors\n\n\n\n\n\n\nDefinition\n\n\n\nGiven a p \\times p square matrix A, an eigenvector \\mathbf{v} and its corresponding eigenvalue \\lambda satisfy: A \\mathbf{v} = \\lambda \\mathbf{v}\n\n\nInterpretation: Multiplying A by its eigenvector \\mathbf{v} simply stretches or shrinks \\mathbf{v} by a factor \\lambda—the direction of \\mathbf{v} does not change.\n\nWhy Do They Matter?\n\nEigenvectors show the principal directions in which A stretches or compresses the space.\nIn statistics, the eigenvectors of a covariance matrix gives the directions of maximum variance.\nThe corresponding eigenvalues tell you how much variance there is in each direction as explained below.\n\nSome Properties\n\nIf (\\lambda_j, \\mathbf{e}_j) is an eigenvalue-eigenvector pair for a p \\times p matrix A, by definition A \\mathbf{e}_j = \\lambda_j \\mathbf{e}_j.\nEigenvectors are usually scaled to have length one, i.e., 1=\\mathbf{e}_j^{\\top} \\mathbf{e}_j = e_{j1}^2+e_{j2}^2 + \\cdots + e_{jp}^2.\nEigenvectors are mutually orthogonal, i.e., \\mathbf{e}_j^{\\top} \\mathbf{e}_k = 0 for any j \\ne k.\n(\\lambda_j^{-1}, \\mathbf{e}_j) is an eigenvalue-eigenvector pair of A^{-1} when A is positive definite and A^{-1} exists.\n\n\n\n\n3.2.2 Spectral Decomposition\n\n\n\n\n\n\nSpectral decomposition\n\n\n\nFor any p\\times p symmetric matrix A, its spectral decomposition is  A = \\lambda_1 \\mathbf{e}_1 \\mathbf{e}_1^{\\top} +  \\lambda_2 \\mathbf{e}_2 \\mathbf{e}_2^{\\top} +  \\cdots +  \\lambda_p \\mathbf{e}_p \\mathbf{e}_p^{\\top}\n\n\n\n\nIf A^{-1} exists, then A^{-1} = \\lambda_1^{-1} \\mathbf{e}_1 \\mathbf{e}_1^{\\top} +  \\lambda_2^{-1} \\mathbf{e}_2 \\mathbf{e}_2^{\\top} +  \\cdots +  \\lambda_p^{-1} \\mathbf{e}_p \\mathbf{e}_p^{\\top}.\nA^{1/2} = \\lambda_1^{1/2} \\mathbf{e}_1 \\mathbf{e}_1^{\\top} +  \\lambda_2^{1/2} \\mathbf{e}_2 \\mathbf{e}_2^{\\top} +  \\cdots +  \\lambda_p^{1/2} \\mathbf{e}_p \\mathbf{e}_p^{\\top}\nA^{-1/2} = \\lambda_1^{-1/2} \\mathbf{e}_1 \\mathbf{e}_1^{\\top} +  \\lambda_2^{-1/2} \\mathbf{e}_2 \\mathbf{e}_2^{\\top} +  \\cdots +  \\lambda_p^{-1/2} \\mathbf{e}_p \\mathbf{e}_p^{\\top}\nA^{1/2} A^{1/2} = A and A^{-1/2}A^{-1/2}=A^{-1}.\n\n\n\nR Code: Create a Matrix\nA = matrix(c(4, 2, 1, 3), nrow = 2)\n\n\n\n\nR Code: Spectral Decomposition\neig = eigen(A)\nprint(eig)\n\n\neigen() decomposition\n$values\n[1] 5 2\n\n$vectors\n          [,1]       [,2]\n[1,] 0.7071068 -0.4472136\n[2,] 0.7071068  0.8944272\n\n\n\n\n3.2.3 Determinant of a p\\times p matrix\n\nThe determinant of the matrix A is denoted by |A| or \\text{det}(A).\nThe determinant of a p \\times p matrix is the product of its eigenvalues, i.e., |A| = \\text{det}(A) = \\lambda_1 \\lambda_2 \\cdots \\lambda_p.\nAll of the eigenvalues of a positive definite matrix are positive.\nThe determinant of a positive definite matrix must be larger than zero.\nIf at least one eigenvalue is zero and the inverse of the matrix does not exist, then the determinant of the matrix is zero.\n\n\n\nMatrix Determinant: det for Small Matrices\ndet(A) \n\n\n[1] 10\n\n\n\n\nMatrix Determinant: Eigen Decomposition\nE = eigen(A)\nprod(E$values)\n\n\n[1] 10",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "ch3/03-MVN.html#probability-density-function-of-mvn",
    "href": "ch3/03-MVN.html#probability-density-function-of-mvn",
    "title": "3  Multivariate Normal Distribution",
    "section": "3.3 Probability Density Function of MVN",
    "text": "3.3 Probability Density Function of MVN\nThe multivariate normal distribution (MVN) generalizes the univariate normal distribution to multiple variables.\nA random vector \\mathbf{x} = (x_1, x_2, \\ldots, x_p)^\\top follows a multivariate normal distribution if every linear combination of its components is normal. We use the notation: \n\\mathbf{x} \\sim N_p(\\boldsymbol{\\mu}, {\\Sigma}) \\quad\n\\text{ or }\\quad \\mathbf{x} \\sim \\text{MVN}(\\boldsymbol\\mu, \\Sigma)\n where\n\n\\boldsymbol{\\mu}:=(\\mu_1, \\mu_2, \\ldots, \\mu_p)^\\top is the p-dimensional mean vector\nand \\Sigma = \\left[ \\begin{array}{cccc} \\sigma_{11} & \\sigma_{12} & \\cdots & \\sigma_{1p} \\\\  \\sigma_{21} & \\sigma_{22} & \\cdots \\  & \\sigma_{2p} \\\\  \\vdots & \\vdots & \\ddots & \\vdots \\\\  \\sigma_{p1} & \\sigma_{p2} & \\cdots & \\sigma_{pp} \\end{array} \\right]  is the p \\times p covariance matrix of \\mathbf{x}.\n\n\n\n\n\n\n\nProbability Density Function (PDF)\n\n\n\nThe probability density function for \\mathbf{x} \\sim N_p(\\boldsymbol \\mu, \\Sigma) is \nf(\\mathbf{x}) = (2\\pi)^{-p/2} |\\Sigma|^{-1/2} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol\\mu)^\\top \\Sigma^{-1}(\\mathbf{x}-\\boldsymbol\\mu)\\right)\n\nGeometric Interpretation:\n\n\\boldsymbol{\\mu}: The center of the point cloud.\n{\\Sigma}: Controls the spread and orientation.\n\nDiagonal = variances\nOff-diagonal = covariances (correlation/shape)\n\n\n\n\n\n\n\n\n\n\nMahalanobis Distance\n\n\n\nThe quadratic form (\\mathbf{x}-\\boldsymbol\\mu)^\\top \\Sigma^{-1}(\\mathbf{x}-\\boldsymbol\\mu) in the density formula is the squared statistical distance of \\mathbf{x} from \\boldsymbol\\mu. It is often referred to as the square of the Mahalanobis distance.\n\n\n\n3.3.1 Some Properties\nIf \\mathbf{x} \\sim N_p(\\boldsymbol\\mu, \\Sigma), then the following statements are true:\n\nMarginal distributions: Each variable is normally distributed, i.e., x_i \\sim N(\\mu_i, \\sigma_{ii}).\nAny linear combinations is normal: Let a_1, \\ldots, a_p \\in \\mathbb{R}. Then \\mathbf{a}^\\top \\mathbf{x} = \\sum_{i=1}^p a_i x_i \\sim N(\\mathbf{a}^\\top \\boldsymbol \\mu, \\mathbf{a}^\\top \\Sigma \\mathbf{a})\nIf \\mathbf{x} \\sim N_p(\\boldsymbol \\mu, \\Sigma), then (\\mathbf{x}-\\boldsymbol\\mu)^\\top \\Sigma^{-1}(\\mathbf{x}-\\boldsymbol\\mu) \\sim \\chi_p^2\nConditional distributions: Any subset of variables, conditional on others, is normal.\nElliptical contours: The joint density forms ellipses.\n\n\n\n3.3.2 R Exercise: MVN Properties\n\nmu = c(0,0)\nSigma = matrix(c(2,1,1,4),ncol=2, byrow=TRUE)\na = c(1,1)\n\n# simulate from MVN\nset.seed(4750)\nx = MASS::mvrnorm(1, mu=mu, Sigma=Sigma)\n\n\n\nR Code: MVN properties\n# linear combination  \nt(a) %*% x \n\n\n           [,1]\n[1,] -0.4976936\n\n\nR Code: MVN properties\n# quadratic form \nt(x-mu)%*%solve(Sigma)%*%(x-mu)\n\n\n           [,1]\n[1,] 0.07250492",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "ch3/03-MVN.html#visualizing-the-mvn-in-r",
    "href": "ch3/03-MVN.html#visualizing-the-mvn-in-r",
    "title": "3  Multivariate Normal Distribution",
    "section": "3.4 Visualizing the MVN in R",
    "text": "3.4 Visualizing the MVN in R\n\n3.4.1 Visualizing Marginal Distributions\n\n\nR Code: Visualize MVN\nlibrary(MASS) \nset.seed(4750) \nmu &lt;- c(0, 0) \nSigma_none &lt;- matrix(c(1, 0, 0, 1), nrow=2) \nSigma_pos &lt;- matrix(c(1, 0.8, 0.8, 1), nrow=2) \nSigma_neg &lt;- matrix(c(1, -0.8, -0.8, 1), nrow=2)\n\nX_none &lt;- mvrnorm(500, mu, Sigma_none) \nX_pos &lt;- mvrnorm(500, mu, Sigma_pos) \nX_neg &lt;- mvrnorm(500, mu, Sigma_neg)\n\npar(mfrow = c(1,3)) \nplot(X_none, main=\"No Correlation\", xlab=\"X1\", ylab=\"X2\", col=rgb(0,0,1,0.3), pch=16) \nplot(X_pos, main=\"Positive Correlation\", xlab=\"X1\", ylab=\"X2\", col=rgb(0,0.5,0,0.3), pch=16) \nplot(X_neg, main=\"Negative Correlation\", xlab=\"X1\", ylab=\"X2\", col=rgb(1,0,0,0.3), pch=16) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nTry changing the covariance matrix \\Sigma in the code above.\n\nWhat happens if you set the off-diagonal entries to 0?\nWhat about +0.9 or -0.9?\n\n\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\nLet’s visualize how changing the off-diagonal entries (correlation) in \\Sigma affects the bivariate normal:\n\n\nCode\nlibrary(MASS)\nset.seed(4750)\nmu &lt;- c(0, 0)\n\n# Case 1: No correlation (off-diagonal = 0)\nSigma_none &lt;- matrix(c(1, 0, 0, 1), nrow=2)\nX_none &lt;- mvrnorm(500, mu, Sigma_none)\n\n# Case 2: Strong positive correlation (off-diagonal = +0.9)\nSigma_pos &lt;- matrix(c(1, 0.9, 0.9, 1), nrow=2)\nX_pos &lt;- mvrnorm(500, mu, Sigma_pos)\n\n# Case 3: Strong negative correlation (off-diagonal = -0.9)\nSigma_neg &lt;- matrix(c(1, -0.9, -0.9, 1), nrow=2)\nX_neg &lt;- mvrnorm(500, mu, Sigma_neg)\n\npar(mfrow = c(1,3))\nplot(X_none, main = \"No correlation\", xlab = \"X1\", \n     ylab = \"X2\", col = rgb(0,0,1,0.4), pch = 16)\nplot(X_pos, main = \"Positive correlation\", xlab = \"X1\",\n     ylab = \"X2\", col = rgb(0,0.5,0,0.4), pch = 16)\nplot(X_neg, main = \"Negative correlation\", xlab = \"X1\",\n     ylab = \"X2\", col = rgb(1,0,0,0.4), pch = 16)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.4.2 Density Contour\n\nA set of all vectors \\mathbf{x} that correspond to a constant height of the density function forms an ellipsoid centered at \\boldsymbol\\mu.\nThe MVN density is constant for all \\mathbf{x}’s that are the same statistical distance from the population mean vector, i.e., all \\mathbf{x}’s that satisfy \n\\sqrt{(\\mathbf{x}-\\boldsymbol\\mu)^\\top \\Sigma^{-1}(\\mathbf{x}-\\boldsymbol\\mu)} = \\text{constant}.\n\nThe axes of the ellipses are in the directions of the eigenvectors of \\Sigma and the length of the j-th longest axis is proportional to \\sqrt{\\lambda_{j}}, where \\lambda_{j} is the eigenvalue associated with the j-th eigenvector of \\Sigma.\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nThe ellipse shows a constant-density contour for the MVN.\nThe red and purple arrows indicate eigenvectors of \\Sigma.\nThe lengths of these arrows are proportional to \\sqrt{\\lambda_j}.\n\n\n\nR Code: Bivariate Normal Density Contour\nlibrary(mvtnorm)\nlibrary(ggplot2)\n\nmu = c(0, 0)\nSigma = matrix(c(1, 0.7, 0.7, 1), 2)\n\n# Create grid\nx = seq(-3, 3, length=100)\ny = seq(-3, 3, length=100)\ngrid = expand.grid(x = x, y = y)\n\n# Compute density at each grid point\ngrid$z = mvtnorm::dmvnorm(as.matrix(grid), \n                           mean = mu, sigma = Sigma)\n\n# Plot contours\nggplot(grid, aes(x = x, y = y, z = z)) +\n  geom_contour_filled(bins = 15) +\n  coord_fixed() +\n  labs(title = \"Bivariate Normal Density (Contours)\",\n       x = \"X1\", y = \"X2\", fill = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n3.4.3 3D Surface Plot\n\n\nR Code: 3D Surface Plot\nlibrary(plotly)\n\nz_matrix &lt;- matrix(grid$z, nrow = length(x), byrow = FALSE)\nplot_ly(x = x, y = y, z = z_matrix) %&gt;%\n  add_surface(\n    contours = list(\n      z = list(show = TRUE, \n               usecolormap = TRUE, \n               highlightcolor = \"#ff0000\", \n               project = list(z = TRUE)))) %&gt;%\n  layout(title = \"Bivariate Normal Density (Surface)\",\n         scene = list(xaxis = list(title = \"X1\"),\n                      yaxis = list(title = \"X2\"),\n                      zaxis = list(title = \"Density\")))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "ch3/03-MVN.html#central-1-alphatimes-100-region-of-mvn",
    "href": "ch3/03-MVN.html#central-1-alphatimes-100-region-of-mvn",
    "title": "3  Multivariate Normal Distribution",
    "section": "3.5 Central (1-\\alpha)\\times 100\\% Region of MVN",
    "text": "3.5 Central (1-\\alpha)\\times 100\\% Region of MVN\n\n3.5.1 Properties of Density Contours\n\n\n\n\n\n\nDefinition\n\n\n\n\nThe probability is 1 - \\alpha that the value of a random vector will be inside the ellipsoid defined by \n(\\mathbf{x}-\\boldsymbol\\mu)^\\top \\Sigma^{-1}(\\mathbf{x}-\\boldsymbol\\mu) \\leq \\chi^2_{(p), 1-\\alpha}\n where \\chi^2_{(p),1-\\alpha} is the upper (100 \\times \\alpha) percentile of a chi-square distribution with p degrees of freedom.\nThis is smallest region that has probability (1-\\alpha) of containing a vector of observations randomly selected from the population.\nThe jth axis of this ellipsoid is determined by the eigenvector associated with the j-th largest eigenvalue of \\Sigma, for j = 1,...,p.\nThe distance along the j-th axis from the center to the boundary of the ellipsoid is \\sqrt{\\lambda_j}  \\left( \\frac{2}{p \\Gamma(p/2)} \\right)^{1/p} \\sqrt{ \\chi^2_{(p), 1-\\alpha}}\n\n\n\n\n\n3.5.2 Geometric Properties\n\nThe ratio of the lengths of the major and minor axes is  \\frac{\\text{Length of major axis}}{\\text{Length of minor axis}}=\\frac{\\sqrt{\\lambda_1}}{\\sqrt{\\lambda_2}} \nThe area of the ellipse containing the central (1-\\alpha) \\times 100 \\% of a bivariate normal population is \narea = \\pi \\chi^2_{(2), 1-\\alpha} \\sqrt{\\lambda_1} \\sqrt{\\lambda_2} =\\pi \\chi^2_{(2),1-\\alpha} |\\Sigma|^{1/2} \nFor p-dimensional normal distributions the hypervolume of the p-dimensional ellipsoid is \\frac{2\\pi^{p/2}}{p \\Gamma(p/2)} \\left[\\chi^2_{(p),1-\\alpha}\\right]^{p/2} |\\Sigma|^{1/2} \n\n\n\n\n\n\n\nGamma function\n\n\n\n\n\n\n\\Gamma(1) is defined to be 1.0,\n \\Gamma\\left(\\frac{p}{2}\\right)=\\left(\\frac{p}{2}-1\\right)\\left(\\frac{p}{2}-2\\right) \\cdots (2)(1) when p is an even integer,\nand  \\Gamma(\\frac{p}{2})=\\frac{(p-2)(p-4) \\cdots(3)(1)}{ 2^{(p-1)/2}} \\sqrt{\\pi}  when p is an odd integer\n\n\n\n\n\n\n3.5.3 50% and 90% Contours for Two Bivariate Normals\n\n\nR Code: Bivariate Normal Contours\nlibrary(MASS)\nlibrary(ggplot2)\nlibrary(mvtnorm)\n\n# Two bivariate normals: different means, same covariance\nmu1 &lt;- c(0, 0)\nmu2 &lt;- c(2, 2)\nSigma &lt;- matrix(c(2, 1.2, 1.2, 2), 2)\n\n# Grid for density evaluation\nx &lt;- seq(-3, 6, length=120)\ny &lt;- seq(-3, 6, length=120)\ngrid &lt;- expand.grid(x=x, y=y)\nxy = as.matrix(grid)\nz1 &lt;- dmvnorm(xy, mean=mu1, sigma=Sigma)\nz2 &lt;- dmvnorm(xy, mean=mu2, sigma=Sigma)\ngrid$z1 = z1 \ngrid$z2 = z2\n\n# Find contour levels for 50% and 90%\nget_density_level &lt;- function(mu, Sigma, p) {\n  d2 &lt;- qchisq(p, df=2)\n  detS &lt;- det(Sigma)\n  norm_const &lt;- 1/(2*pi*sqrt(detS))\n  exp_part &lt;- exp(-0.5 * d2)\n  norm_const * exp_part\n}\nlevel_50 &lt;- get_density_level(mu1, Sigma, 0.5)\nlevel_90 &lt;- get_density_level(mu1, Sigma, 0.9)\n\n# Plot\nggplot() +\n  geom_contour(data=grid, aes(x=x, y=y, z=z1), \n               breaks=c(level_50, level_90),\n               color=\"blue\", size=1.2, linetype=\"solid\") +\n  geom_contour(data=grid, aes(x=x, y=y, z=z2), \n               breaks=c(level_50, level_90),\n               color=\"red\", size=1.2, linetype=\"dashed\") +\n  geom_point(aes(x=mu1[1], y=mu1[2]), color=\"blue\", size=3) +\n  geom_point(aes(x=mu2[1], y=mu2[2]), color=\"red\", size=3) +\n  annotate(\"text\", x=mu1[1], y=mu1[2]-0.5, \n           label=\"mu1\", color=\"blue\") +\n  annotate(\"text\", x=mu2[1], y=mu2[2]+0.5, \n           label=\"mu2\", color=\"red\") +\n  coord_fixed() +\n  labs(\n    title=\"50% (inner) and 90% (outer) Contours of Two Bivariate Normals\",\n    x=\"X1\", y=\"X2\", \n    caption=\"Density peaks at the mean for each distribution.\") +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "ch3/03-MVN.html#overall-measures-of-variability",
    "href": "ch3/03-MVN.html#overall-measures-of-variability",
    "title": "3  Multivariate Normal Distribution",
    "section": "3.6 Overall Measures of Variability",
    "text": "3.6 Overall Measures of Variability\n\n\n\n\n\n\nMeasures of variability\n\n\n\n\nGeneralized variance and generalized standard deviation tell us about the overall “multivariate” spread or “joint variability”—not just variable by variable, but how “big” the whole data cloud is, including dependencies.\nTotal variance tells us the aggregate variance but is “blind” to how variables overlap (i.e., to correlations).\n\n\n\n\n3.6.1 Generalized Variance\n\nDefinition: |\\Sigma|=\\lambda_1\\lambda_2\\cdots\\lambda_p \nThe generalized variance measures the overall spread of the data in all directions at once.\nIf any variable has no variation, or if there’s perfect linear dependence (collinearity), the generalized variance drops to zero (the volume “flattens”).\n\n\n\n3.6.2 Generalized Standard Deviation\n\nDefinition: |\\Sigma|^{1/2}=\\sqrt{\\lambda_1\\lambda_2\\cdots\\lambda_p} \nThe generalized standard deviation is proportional to the volume of the ellipsoid.\n\n\n\n3.6.3 Total Variance\n\nDefinition \n\\begin{aligned}\ntrace(\\Sigma)\\equiv tr(\\Sigma)&:= \\sigma_{11} + \\sigma_{22} + \\cdots + \\sigma_{pp} \\\\\n               &= \\lambda_1+\\lambda_2+\\cdots+\\lambda_p\n\\end{aligned}\n\nThis is the total marginal variability in all directions, but does not account for correlation.\nIn the ellipsoid analogy, it’s the sum of the squared axis lengths (not the “volume”).\n\n\n\n3.6.4 Example: iris Dataset\nThe iris dataset is a classic and widely used dataset in R, commonly employed for statistical analysis, machine learning, and data visualization. It is built into R and can be loaded directly. Let’s compute the overall measures of the dataset.\n\n# Use iris measurements\nX &lt;- iris[, 1:4]\nhead(X)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1          5.1         3.5          1.4         0.2\n2          4.9         3.0          1.4         0.2\n3          4.7         3.2          1.3         0.2\n4          4.6         3.1          1.5         0.2\n5          5.0         3.6          1.4         0.2\n6          5.4         3.9          1.7         0.4\n\n\n\n\nR Code: Covariance\n# compute covariance \nS &lt;- cov(X)\nprint(S)\n\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    0.6856935  -0.0424340    1.2743154   0.5162707\nSepal.Width    -0.0424340   0.1899794   -0.3296564  -0.1216394\nPetal.Length    1.2743154  -0.3296564    3.1162779   1.2956094\nPetal.Width     0.5162707  -0.1216394    1.2956094   0.5810063\n\n\n\n\nR Code: Spectral Decomposition\neig &lt;- eigen(S)\nlambda &lt;- eig$values\n\n\n\n\nR Code: Overall Measures\n# Generalized variance: product of eigenvalues\ngen_var &lt;- prod(lambda)\n# Generalized standard deviation: sqrt of product\ngen_sd &lt;- sqrt(prod(lambda))\n# Total variance: sum of eigenvalues\ntotal_var &lt;- sum(lambda)\nlibrary(tibble)\nresult = tibble(\n  \"Generalized variance\" = gen_var,\n  \"Generalized standard deviation\" = gen_sd,\n  \"Total variance\" = total_var\n)\nprint(t(result))\n\n\n                                     [,1]\nGeneralized variance           0.00191273\nGeneralized standard deviation 0.04373476\nTotal variance                 4.57295705",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "ch3/03-MVN.html#assessing-normality",
    "href": "ch3/03-MVN.html#assessing-normality",
    "title": "3  Multivariate Normal Distribution",
    "section": "3.7 Assessing Normality",
    "text": "3.7 Assessing Normality\n\nAssessing multivariate normality is difficult.\nIf any single variable does not have a normal distribution, then the joint distribution of p random variables cannot have a normal distribution.\nWe can check normality for each variable individually. If we reject normality for any variable then the joint distribution is not multivariate normal.\nAlso look at scatter plots of pairs of variables.\nLook for outliers.\n\n\n3.7.1 Normal Q-Q plots\n\nA quantile-quantile (Q-Q) plot can also be constructed for each of the p variables.\nIn a Q-Q plot, we plot the ordered data (sample quantiles) against the quantiles that would be expected if the sample came from a standard normal distribution.\nIf the hypothesis of normality holds, the points in the plot will fall closely along a straight line.\n\n\n\n\n\n\n\nNormal Q-Q plots\n\n\n\n\nThe slope of the line passing through the points is an estimate of the population standard deviation.\nThe intercept of the estimated line is an estimate of the population mean.\nThe sample quantiles are just the sample order statistics. For a sample x_1, x_2,...,x_n, quantiles are obtained by ordering sample observations \nx_{(1)} \\leq x_{(2)} \\leq ... \\leq x_{(n)},\n where x_{(j)} is the jth smallest sample observation.\n\n\n\n\n\nR Code: Q-Q Plots\n# Normal Q-Q plots for all variables in iris[, 1:4]\npar(mfrow = c(2,2))\nfor (i in 1:4) {\n  qqnorm(iris[,i], main = colnames(iris)[i])\n  qqline(iris[,i], col = \"blue\", lwd = 2)\n}\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nIf the points follow the line closely, that variable is approximately normal.\nSystematic departures from the line (curves, s-shapes, outliers) suggest non-normality.\n\n\n\nR Code: Scatterplots\n# Also look at scatterplots for pairs of variables\npairs(iris[,1:4], main = \"Scatterplots: Iris Variables\")\n\n\n\n\n\n\n\n\n\nInterpretation: - Outliers, nonlinear patterns, or heavy tails in these scatterplots suggest deviations from the multivariate normal model.\n\n\n3.7.2 Shapiro-Wilk Test\n\nThe Shapiro-Wilk test is a statistical test for normality. If the p-value is small (typically &lt;0.05), we reject the null hypothesis of normality.\nTest statistic: A weighted correlation between the x_{(j)} and the q_{(j)}: \nW = \\left( \\frac{\\sum_{i = 1}^n a_j(x_{(i)} - \\bar{x})(q_{(i)} - \\bar{q}) }{\\sqrt{\\sum_{i = 1}^n a_j^2(x_{(i)} - \\bar{x})^2}\\sqrt{\\sum_{i = 1}^n (q_{(i)} - \\bar{q})^2}}  \\right)^2.\n\nWe expect values of W to be close to one if the sample arises from a normal population.\nReject the null hypothesis that data were sampled from a normal distribution if W is too small.\nThis test has been extended to p-dimensional normal distributions.\n\n\n\n\n\n\n\nShapiro-Wilk Test\n\n\n\nThe Shapiro-Wilk test only checks marginal normality, not joint/multivariate normality when applied to each variable individually. This univariate test is implemented in the R function shapiro.test. To perform multivariate normaltiy check, we need to use the multivariate version of the Shapiro-Wilk test using the function mvShapiro.Test from the package mvShapiroTest.\n\n\n\n\nR Code: Univariate Normality Test\n# Shapiro-Wilk test for each of the four numeric iris variables\napply(iris[, 1:4], 2, shapiro.test)\n\n\n$Sepal.Length\n\n    Shapiro-Wilk normality test\n\ndata:  newX[, i]\nW = 0.97609, p-value = 0.01018\n\n\n$Sepal.Width\n\n    Shapiro-Wilk normality test\n\ndata:  newX[, i]\nW = 0.98492, p-value = 0.1012\n\n\n$Petal.Length\n\n    Shapiro-Wilk normality test\n\ndata:  newX[, i]\nW = 0.87627, p-value = 7.412e-10\n\n\n$Petal.Width\n\n    Shapiro-Wilk normality test\n\ndata:  newX[, i]\nW = 0.90183, p-value = 1.68e-08\n\n\n\n\nR Code: Multivariate Normality Test\nlibrary(mvShapiroTest)\n# multivariate Shapiro-Wilk test \nmvShapiro.Test(as.matrix(iris[, 1:4]))\n\n\n\n    Generalized Shapiro-Wilk test for Multivariate Normality by\n    Villasenor-Alva and Gonzalez-Estrada\n\ndata:  as.matrix(iris[, 1:4])\nMVW = 0.97327, p-value = 1.655e-06\n\n\n\n\n\n\n\n\nExercise: Assessing Normality with Real Data (mtcars)\n\n\n\nUsing the mtcars dataset, explore the relationship between mpg (miles per gallon) and hp (horsepower):\n\nMake a scatterplot of mpg vs hp and describe the pattern.\nOverlay bivariate normal contours (fit the mean and covariance from the data).\nDraw normal Q-Q plots for mpg and hp.\nPerform the Shapiro-Wilk test for both variables.\nInterpret your results: Does either variable appear to be normally distributed? Does the joint distribution appear approximately elliptical (as would be expected under bivariate normality)?\n\n\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nR Code: View Solution\nlibrary(mvtnorm)\nlibrary(ggplot2)\n\ndata(mtcars)\nX &lt;- mtcars[, c(\"mpg\", \"hp\")]\nmu &lt;- colMeans(X)\nSigma &lt;- cov(X)\n\n\np &lt;- ggplot(X, aes(x = mpg, y = hp)) +\n  geom_point(size = 2, alpha = 0.7) +\n  theme_minimal() +\n  labs(title = \"mtcars: mpg vs hp\")\n\nx_seq &lt;- seq(min(X$mpg) - 2, max(X$mpg) + 2, length = 100)\ny_seq &lt;- seq(min(X$hp) - 10, max(X$hp) + 10, length = 100)\ngrid &lt;- expand.grid(mpg = x_seq, hp = y_seq)\ngrid$z &lt;- dmvnorm(as.matrix(grid), mean = mu, sigma = Sigma)\n\nget_density_level &lt;- function(Sigma, p) {\n  d2 &lt;- qchisq(p, df=2)\n  detS &lt;- det(Sigma)\n  norm_const &lt;- 1/(2*pi*sqrt(detS))\n  exp_part &lt;- exp(-0.5 * d2)\n  norm_const * exp_part\n}\nlevel_50 &lt;- get_density_level(Sigma, 0.5)\nlevel_90 &lt;- get_density_level(Sigma, 0.9)\n\np + \n  geom_contour(data = grid, aes(z = z), breaks = level_50, \n               color = \"blue\", linetype = \"solid\", size = 1.2) +\n  geom_contour(data = grid, aes(z = z), breaks = level_90, \n               color = \"red\", linetype = \"dashed\", size = 1.2) +\n  labs(subtitle = \"Blue: 50% contour | Red dashed: 90% contour\")\n\n\n\n\n\n\n\n\n\nR Code: View Solution\npar(mfrow = c(1,2))\nqqnorm(X$mpg, main = \"Normal Q-Q: mpg\"); qqline(X$mpg, col = \"blue\")\nqqnorm(X$hp, main = \"Normal Q-Q: hp\"); qqline(X$hp, col = \"blue\")\n\n\n\n\n\n\n\n\n\nR Code: View Solution\npar(mfrow = c(1,1))\n\nsw1 &lt;- shapiro.test(X$mpg)\nsw2 &lt;- shapiro.test(X$hp)\ncat(\"Shapiro-Wilk p-value for mpg:\", signif(sw1$p.value, 3), \"\\n\")\n\n\nShapiro-Wilk p-value for mpg: 0.123 \n\n\nR Code: View Solution\ncat(\"Shapiro-Wilk p-value for hp:\", signif(sw2$p.value, 3), \"\\n\")\n\n\nShapiro-Wilk p-value for hp: 0.0488 \n\n\nR Code: View Solution\n# 5. multivariate Shapiro-Wilk test\nmvShapiro.Test(as.matrix(X[, c(\"mpg\", \"hp\")]))\n\n\n\n    Generalized Shapiro-Wilk test for Multivariate Normality by\n    Villasenor-Alva and Gonzalez-Estrada\n\ndata:  as.matrix(X[, c(\"mpg\", \"hp\")])\nMVW = 0.92425, p-value = 0.00639\n\n\nInterpretation:\n\nThe scatterplot shows the relationship between mpg and hp (likely negative).\nThe bivariate normal contours (fit from the data) show the estimated “ellipse” of the joint distribution.\nQ-Q plots help check whether each variable is approximately normal (look for straightness).\nShapiro-Wilk p-values: p &lt; 0.05 indicates non-normality; p &gt; 0.05 suggests no evidence against normality.\nIf either variable is non-normal or the scatterplot is strongly non-elliptical (e.g., curved, outliers), the joint distribution is not truly bivariate normal.\n\n\n\n\n\n\n3.7.3 Transformations to Near Normality\nIf observations show gross departures from normality, it might be necessary to transform some of the variables to near normality. Some recommendations are given below.\n\n\n\n\n\n\n\nOriginal scale\nTransformed scale\n\n\n\n\nRight skewed data\n\\sqrt{x}   log(x)   1/\\sqrt{x}   1/x\n\n\nx are counts\n\\sqrt{x}\n\n\nx are proportions \\hat{p}\n\\mathrm{logit}(\\hat{p}) = \\frac{1}{2} \\log\\left(\\frac{\\hat{p}}{1 - \\hat{p}}\\right)\n\n\nx are correlations r\nFisher’s z(r) = \\frac{1}{2} \\log\\left(\\frac{1 + r}{1 - r}\\right)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "ch3/03-MVN.html#detecting-outliers",
    "href": "ch3/03-MVN.html#detecting-outliers",
    "title": "3  Multivariate Normal Distribution",
    "section": "3.8 Detecting Outliers",
    "text": "3.8 Detecting Outliers\n\nAn outlier is a measurement that appears to be much different from neighboring observations.\nIn the univariate case with adequate sample sizes, and assuming that normality holds, an outlier can be detected by:\n\nStandardizing the n measurements so that they are approximately N(0, \\ 1)\nFlagging observations with standardized values below or above 3.5 or thereabouts.\n\nIn p dimensions, detecting outliers is not so easy. A sample unit which may not appear to be an outlier in each of the marginal distributions can still be an outlier relative to the multivariate distribution.\n\n\n3.8.1 Steps for Detecting Outliers\n\nInvestigate all univariate marginal distributions by computing the standardized values z_{ji} = (x_{ji} - \\bar{x}_i) / \\sqrt{\\sigma_{ii}} for the j-th sample unit and the i-th variable.\nIf p is moderate, construct all bivariate scatter plots. There are p(p-1)/2 of them.\nFor each sample unit, calculate the squared distance d^2_j = (\\mathbf{x}_j - \\bar{\\mathbf{x}})'S^{-1}(\\mathbf{x}_j - \\bar{\\mathbf{x}}), where \\mathbf{x}_j is the p \\times 1 vector of measurements on the j-th sample unit.\nTo decide if d^2_j is extreme, recall that the d^2_j are approximately \\chi^2_p. For example, if n = 100, we would expect to observe about 5 squared distances larger than the 0.95 percentile of the \\chi^2_p distribution.\n\n\n\n3.8.2 Example: iris Data\n\n\nVisualizing Data Using Scatterplot and Marginal Dot Plots\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndf = iris\n# Bin along x\nx_proj &lt;- df %&gt;%\n  dplyr::mutate(y = 2)\n\n# Bin along y\ny_proj &lt;- df %&gt;%\n  mutate(x = 4)\n\nggplot(df, \n  aes(x = Sepal.Length, \n      y = Sepal.Width, color = Species)) +\n  geom_point(size = 1, alpha = 0.7) +\n  geom_dotplot(data = x_proj, \n               aes(x = Sepal.Length, y=2, fill = Species),\n               binaxis = \"x\", stackdir = \"down\", dotsize = 0.5,\n               position = position_nudge(y=1.5),\n               inherit.aes = FALSE) +\n  geom_dotplot(data = y_proj, \n               aes(x=3.5, y = Sepal.Width, fill = Species),\n               binaxis = \"y\", stackdir = \"down\", dotsize = 0.5,\n               position = position_nudge(x=0),\n               inherit.aes = FALSE) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nDetecting Outliers Using Mahalanobis Distances\n## Finding mahalanobis distance\nxbar = colMeans(df[, 1:4])\nbvar = cov(df[,1:4])\ndsq = mahalanobis(x=df[,1:4], center=xbar, cov=bvar)\n\n## Cutoff value for distances from a chisquare distribution\ncutoff = qchisq(p=0.95, df=nrow(bvar)) # 95th percentile \n\n## plot observations whose distance is greater than cutoff value\nflag = dsq&gt;cutoff\ncbind(df[flag,1:4], d_sqaure=dsq[flag])\n\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width  d_sqaure\n16           5.7         4.4          1.5         0.4  9.712790\n42           4.5         2.3          1.3         0.3 11.424029\n107          4.9         2.5          4.5         1.7 10.137804\n115          5.8         2.8          5.1         2.4 11.410573\n118          7.7         3.8          6.7         2.2 12.813073\n132          7.9         3.8          6.4         2.0 13.101093\n135          6.1         2.6          5.6         1.4 12.880331\n136          7.7         3.0          6.1         2.3  9.656936\n142          6.9         3.1          5.1         2.3 12.441384\n\n\n\n\nScatterplot and Dot Plot\n# Add flag for outliers\ndf = df %&gt;%\n  mutate(flag = dsq &gt; cutoff)\n\n# Projections\nx_proj = df %&gt;% mutate(y = 2)\ny_proj = df %&gt;% mutate(x = 4)\n\nggplot(df, aes(x = Sepal.Length, y = Sepal.Width)) +\n  # Scatter plot with flagged points highlighted\n  geom_point(aes(color = Species,\n                 shape = flag,    # Different shape for flagged\n                 size = ifelse(flag, 3, 1.5)), # Larger for flagged\n             alpha = 0.7) +\n  # X-axis projection\n  geom_dotplot(\n    data = x_proj,\n    aes(x = Sepal.Length, y = 2, fill = Species),\n    binaxis = \"x\", stackdir = \"down\", binwidth = 0.15, \n    dotsize = 0.5,\n    position = position_nudge(y = 1.5),\n    inherit.aes = FALSE\n  ) +\n  # Y-axis projection\n  geom_dotplot(\n    data = y_proj,\n    aes(x = 3.5, y = Sepal.Width, fill = Species),\n    binaxis = \"y\", stackdir = \"down\", binwidth = 0.15, \n    dotsize = 0.5,\n    position = position_nudge(x = 0),\n    inherit.aes = FALSE\n  ) +\n  scale_shape_manual(\n    values = c(\n      `FALSE` = 16, \n      `TRUE` = 21)) + # open circle for outliers\n  scale_size_identity() +\n  coord_equal() +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "ch3/03-MVN.html#exercises",
    "href": "ch3/03-MVN.html#exercises",
    "title": "3  Multivariate Normal Distribution",
    "section": "3.9 Exercises",
    "text": "3.9 Exercises\n\n3.9.1 Exercise 1: Perspiration Data\nThese data were taken from an example in the Johnson and Wichern book, Applied Multivariate Analysis in the file sweat.dat. Three measurements on perspiration were taken from each woman in a random sample of 20 healthy women: X_1= sweat rate, X_2= sodium concentration, X_3=potassium concentration.\n\nRead the data into R.\nCompute the sample mean and sample covariance\nTreat sample mean and sample covariance as the estimate of population parameters and evaluate the MVN pdf for a range of possible values of variables X_1, X_2.\nCompute the mean and variance of 0.5X_1+ 0.5 X_2 and X_1-X_2.\nPlot the bivariate normal density contour for X_1, X_2 using sample mean and sample covariance.\nPlot a scatterplot with points outside the 95% highlighted.\nCompute Ellipse axes via spectral decomposition.\nCompute generalized variance and total variance based on the whole dataset.\n\nPerform normality check for all the variables.\nDetect any possible outliers at level \\alpha =0.05 for all the variables.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\ndat = read.table(\"sweat.dat\", header=F, \n                 col.names=c(\"subject\", \"x1\", \"x2\", \"x3\") )\nhead(dat)\n\n\n  subject  x1   x2   x3\n1       1 3.7 48.5  9.3\n2       2 5.7 65.1  8.0\n3       3 3.8 47.2 10.9\n4       4 3.2 53.2 12.0\n5       5 3.1 55.5  9.7\n6       6 4.6 36.1  7.9\n\n\nCode\nX = as.matrix(dat[,c(2,3)])\n\n\n\n\nCode\nxbar = colMeans(X)\nS = cov(X)\nS\n\n\n          x1       x2\nx1  2.879368  10.0100\nx2 10.010000 199.7884\n\n\n\n\nCode\ngrid &lt;- expand.grid(\n  x = seq(min(X[,1]) - 0.5, max(X[,1]) + 0.5, length = 50),\n  y = seq(min(X[,2]) - 0.5, max(X[,2]) + 0.5, length = 50)\n)\ngridmat = as.matrix(grid)\ndens &lt;- mvtnorm::dmvnorm(x=gridmat, \n                         mean = xbar, \n                         sigma = S)\nhead(data.frame(grid, f = dens))\n\n\n         x  y            f\n1 1.000000 13 0.0002244281\n2 1.163265 13 0.0002563175\n3 1.326531 13 0.0002894749\n4 1.489796 13 0.0003232772\n5 1.653061 13 0.0003570021\n6 1.816327 13 0.0003898505\n\n\n\n\nCode\n# mean, var\na1 = c(0.5, 0.5)\na2 = c(1, -1)\n\n# mean and variance of 0.5*X1 + 0.5*X2\ntibble(\n  mean = sum(a1 * xbar),\n  variance = drop(t(a1) %*% S %*% a1)\n)\n\n\n# A tibble: 1 × 2\n   mean variance\n  &lt;dbl&gt;    &lt;dbl&gt;\n1  25.0     55.7\n\n\nCode\n# mean and variance of X1 - X2\ntibble(\n  mean = sum(a2 * xbar),\n  variance = drop(t(a2) %*% S %*% a2)\n)\n\n\n# A tibble: 1 × 2\n   mean variance\n  &lt;dbl&gt;    &lt;dbl&gt;\n1 -40.8     183.\n\n\n\n\nCode\ngrid$z = dens \nggplot(grid, aes(x = x, y = y, z = z)) +\n  geom_contour_filled(bins = 10) +\n  #coord_equal() +\n  labs(title = \"Bivariate Normal Density (Contours)\",\n       x = \"Sweat rate\", y = \"Sodium concentration\", \n       fill = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nXc = X - matrix(1, nrow=nrow(X), ncol=1) %*% t(xbar)\nQ = diag( Xc %*% solve(S) %*% t(Xc) )\n# or use T2 = rowSums( (Xc %*% solve(S)) * Xc)\ncut = qchisq(.95, df=2)\nmean(Q&lt;=cut) # empirical coverage \n\n\n[1] 0.95\n\n\nCode\ndat1 = as.data.frame(X) %&gt;% \n  mutate(inside = Q&lt;=cut)\nggplot(dat1, aes(x=x1, y=x2, color=inside)) + \n  geom_point(size=2, alpha=.8) + \n  scale_color_manual(values = c(\"FALSE\"=\"grey60\",\n                                \"TRUE\"=\"tomato\")) + \n  labs(color = \"Inside 95% ellipse?\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nE = eigen(S)\n\n# Ellipse directions\nE$vectors\n\n\n          [,1]       [,2]\n[1,] 0.0506399 -0.9987170\n[2,] 0.9987170  0.0506399\n\n\nCode\n# lengths\nsqrt(E$values) * sqrt(cut)\n\n\n[1] 34.641972  3.769698\n\n\n\n\nCode\ntibble(\n  \"Generalized variance\" = prod(E$values),\n  \"Total variance\" = sum(E$values)\n)\n\n\n# A tibble: 1 × 2\n  `Generalized variance` `Total variance`\n                   &lt;dbl&gt;            &lt;dbl&gt;\n1                   475.             203.\n\n\n\n\nCode\npar(mfrow = c(1,3))\nfor (nm in colnames(dat[,2:4])) {\n  qqnorm(dat[, nm], main = paste(\"QQ:\", nm))\n  qqline(dat[, nm])\n}\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1,1))\n\napply(dat[, 2:4], 2, function(v) shapiro.test(v)$p.value)\n\n\n       x1        x2        x3 \n0.8689242 0.9861883 0.6232620 \n\n\nCode\nmvShapiroTest::mvShapiro.Test(as.matrix(dat[,2:4]))\n\n\n\n    Generalized Shapiro-Wilk test for Multivariate Normality by\n    Villasenor-Alva and Gonzalez-Estrada\n\ndata:  as.matrix(dat[, 2:4])\nMVW = 0.94446, p-value = 0.2567\n\n\n\n\nCode\ndat1 = as.matrix(dat[,2:4])\nxbar1 = colMeans(dat1)\nS1 = cov(dat1)\nXc1 = dat1 - matrix(1,nrow(dat1), 1) %*% t(xbar1)\nd2 = diag( (Xc1) %*% solve(S1) %*% t(Xc1) )\n\ncut2 = qchisq(.975, df=ncol(dat1))\nsum(d2&gt;cut2)\n\n\n[1] 0",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "ch4/04-Inference_for_Means.html",
    "href": "ch4/04-Inference_for_Means.html",
    "title": "4  Inference for Single Mean Vector",
    "section": "",
    "text": "4.1 Motivation\nLet’s look at the famous iris dataset. We’ll focus on the setosa species and two variables: Sepal Length and Sepal Width.\nA biologist might ask: “Is the mean vector of sepal measurements for setosa flowers equal to a specific standard, say \\boldsymbol{\\mu}_0 = [5.1, 3.6]'?”\nR Code: Iris Data\nlibrary(ggplot2)\nlibrary(DescTools) # For one-sample T2 test\nlibrary(ellipse)   # For plotting confidence ellipses\n\n# Isolate the setosa data for the first two variables\nsetosa_data = iris[iris$Species == \"setosa\", 1:2]\nnames(setosa_data) = c(\"Sepal.Length\", \"Sepal.Width\")\nhead(setosa_data)\nData Visualization: iris Data\n# The hypothesized mean vector\nmu_0 = c(5.1, 3.6)\n\n# Plot the data\nggplot(setosa_data, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point(color = \"purple\", alpha = 0.7) +\n  geom_vline(xintercept = mu_0[1],\n             linetype = \"dashed\",\n             color = \"red\") +\n  geom_hline(yintercept = mu_0[2],\n             linetype = \"dashed\",\n             color = \"red\") +\n  annotate(\n    \"point\",\n    x = mu_0[1],\n    y = mu_0[2],\n    color = \"red\",\n    size = 5,\n    shape = 4,\n    stroke = 1.5\n  ) +\n  labs(\n    title = \"Sepal Measurements for Iris Setosa\",\n    subtitle = \"Red crosshairs show the hypothesized mean vector [5.1, 3.6]\",\n    x = \"Sepal Length (cm)\",\n    y = \"Sepal Width (cm)\"\n  ) +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inference for Single Mean Vector</span>"
    ]
  },
  {
    "objectID": "ch4/04-Inference_for_Means.html#motivation",
    "href": "ch4/04-Inference_for_Means.html#motivation",
    "title": "4  Inference for Single Mean Vector",
    "section": "",
    "text": "4.1.1 Why Not Just Run Two t-tests?\nThe “naive” approach is to test each variable separately.\n\nTest H_0: \\mu_{\\text{Length}} = 5.1 \\quad \\text{v.s.} \\quad H_1: \\mu_{\\text{Length}} \\neq 5.1\nTest H_0: \\mu_{\\text{Width}} = 3.6 \\quad \\text{v.s.} \\quad H_1: \\mu_{\\text{Width}} \\neq 3.6\n\n\n\n\n\n\n\nR Code: One-Sample t-Test\n\n\n\n\n\n\n\nR Code: One-Sample t-Test\n# Test 1: Sepal Length\nt.test(setosa_data$Sepal.Length, mu = mu_0[1])\n\n\n\n    One Sample t-test\n\ndata:  setosa_data$Sepal.Length\nt = -1.8857, df = 49, p-value = 0.06527\nalternative hypothesis: true mean is not equal to 5.1\n95 percent confidence interval:\n 4.905824 5.106176\nsample estimates:\nmean of x \n    5.006 \n\n\nR Code: One-Sample t-Test\n# Test 2: Sepal Width\nt.test(setosa_data$Sepal.Width, mu = mu_0[2])\n\n\n\n    One Sample t-test\n\ndata:  setosa_data$Sepal.Width\nt = -3.2085, df = 49, p-value = 0.002354\nalternative hypothesis: true mean is not equal to 3.6\n95 percent confidence interval:\n 3.320271 3.535729\nsample estimates:\nmean of x \n    3.428 \n\n\n\n\n\nIndividually, we failed to reject the first null hypothesis and would reject the second null hypothes at \\alpha = 0.05. But this is misleading!\n\n\n\n\n\n\nWhy do we need a multivariate test?\n\n\n\n\n\n\nThe univariate approach doesn’t answer the right question. The research question is about the mean vector \\boldsymbol{\\mu}, not the individual means. The two t-tests do not provide a single measure of evidence (one p-value) against the single hypothesis H_0: \\boldsymbol{\\mu} = \\boldsymbol{\\mu}_0.\nIt ignores the data’s structure. Sepal length and petal length are correlated. Taller flowers tend to have longer petals. The separate t-tests treat the variables as independent, which they are not. This ignores crucial information about the joint variability (or covariance) of the data.\nIt cannot quantify the joint uncertainty. The true “distance” of the sample mean vector from the hypothesized mean vector can only be properly measured by accounting for the covariance between variables. We need a method that can create a single confidence region (an ellipse) for the mean vector, which is impossible with two separate confidence intervals.\n\nThe multivariate approach provides a single, unified test for a single, unified hypothesis, while properly accounting for the relationships between the variables.\n\n\n\n\n\n4.1.2 The Family-Wise Error Rate Problem\nIf we conduct p tests, each at a significance level \\alpha, the probability of making at least one Type I error (a false positive) skyrockets.\n\n\\text{Pr}(\\text{rejecting at least one} \\, H_0: \\mu_j = \\mu_{0j} \\, | \\, \\text{all}\\, H_0\\text{'s are true})= 1 - (1 - \\alpha)^p \n\nFor our p=2 case with \\alpha=0.05, the true error rate is 1 - (1-0.05)^2 = 0.0975, nearly double our intended \\alpha!\nThis approach is very conservative (i.e., tends to reject more null hypotheses than we should).\nMore importantly, this univariate approach completely ignores the correlation between variables. Multivariate methods account for this correlation structure.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inference for Single Mean Vector</span>"
    ]
  },
  {
    "objectID": "ch4/04-Inference_for_Means.html#review-of-univariate-inference",
    "href": "ch4/04-Inference_for_Means.html#review-of-univariate-inference",
    "title": "4  Inference for Single Mean Vector",
    "section": "4.2 Review of Univariate Inference",
    "text": "4.2 Review of Univariate Inference\nThe one-sample t-test answers a very common question: “Is the average of my sample significantly different from a known or claimed value?”\nBefore we go multivariate, let’s review the one-sample t-test for a mean \\mu.\n\n\n\n\n\n\nt-Test\n\n\n\n\nHypotheses: H_0: \\mu = \\mu_0 H_1: \\mu \\neq \\mu_0\nTest Statistic:  t = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} \\sim t_{n-1} \nConfidence Interval: A 100(1-\\alpha)\\% CI for \\mu is:  \\bar{x} \\pm t_{n-1, 1-\\alpha/2} \\frac{s}{\\sqrt{n}} \n\n\n\n\n\n\n\n\n\nDecision approach\n\n\n\n\n\nTo make the decision, we could either use the rejection region approach or the p-value approach.\n\nRejection Region\n\nIf |t|&gt; |t_{n-1,1-\\alpha/2}|, we should reject H_0 at significance level \\alpha; otherwise, we fail to reject H_0 at significance level \\alpha and conclude there is no sufficient evidence to detect the difference.\n\nP-Value\n\nThe p-value is the probability of seeing a result as extreme as yours (or more extreme) if the claim (in null hypothesis) were actually true.\nSmall p-value (typically &lt; 0.05): This result is very unlikely to happen by random chance alone. We reject the null hypothesis (H_0) and conclude there’s a significant difference at level \\alpha (e.g., 0.05).\nLarge p-value (typically ≥ 0.05): This result is reasonably likely to happen by random chance. The difference you saw could just be noise. We fail to reject the null hypothesis at level \\alpha (e.g., 0.05). There is no sufficient evidence from the data to indicate the null hypothesis is wrong.\n\n\n\n\n\n\n\n\n\n\n\nExample: Coffee Roaster\n\n\n\n\n\nLet’s test the coffee roaster’s claim. We buy 10 bags and weigh them with data given by . We want to test if the true mean weight is different from the claimed 12 ounces.\n\nNull hypothesis: H_0: ______\nAlternative hypothesis H_1: ______\n\n\n\nR Code: One-Sample t Test\n# Our sample data: weights of 10 coffee bags in ounces\ncoffee_weights &lt;- c(11.8, 12.1, 11.4, 11.7, \n                    12.0, 11.6, 11.8, 12.2, \n                    11.5, 11.8)\n\n# The hypothesized mean from the null hypothesis\nmu0 &lt;- 12\n\n# Perform the one-sample t-test\ntest_result &lt;- t.test(x = coffee_weights, mu = mu0)\n\n# Print the results\nprint(test_result)\n\n\n\n    One Sample t-test\n\ndata:  coffee_weights\nt = -2.5959, df = 9, p-value = 0.02893\nalternative hypothesis: true mean is not equal to 12\n95 percent confidence interval:\n 11.607 11.973\nsample estimates:\nmean of x \n    11.79 \n\n\nInterpreting the output:\n\nt=-2.596: The sample mean is 2.596 standard errors below the hypothesized mean. That’s a moderately strong signal.\nConclusion: Since the p-value = 0.029 is less than the significant level \\alpha=0.05, we reject the null hypothesis. We conclude that the true mean weight of the coffee bags is significantly different from 12 ounces at level \\alpha=0.05.\n95 percent confidence interval: [11.61, 11.97]: We are 95% confident that the true mean weight of all bags from this roaster is between 11.61 and 11.97 ounces. Notice that this interval does not contain 12, which confirms our decision to reject the null hypothesis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inference for Single Mean Vector</span>"
    ]
  },
  {
    "objectID": "ch4/04-Inference_for_Means.html#generalizing-to-the-multivariate-case",
    "href": "ch4/04-Inference_for_Means.html#generalizing-to-the-multivariate-case",
    "title": "4  Inference for Single Mean Vector",
    "section": "4.3 Generalizing to the Multivariate Case",
    "text": "4.3 Generalizing to the Multivariate Case\nWe upgrade our tools from scalars to vectors and matrices.\n\nThe sample mean vector \\bar{\\mathbf{x}} is a vector of the individual sample means.  \\bar{\\mathbf{x}} = \\begin{bmatrix} \\bar{x}_1 \\\\ \\vdots \\\\ \\bar{x}_p \\end{bmatrix} \nThe sample covariance matrix \\mathbf{S} contains the sample variances on its diagonal and the sample covariances on its off-diagonals.  \\mathbf{S} = \\frac{1}{n-1} \\sum_{j=1}^{n} (\\mathbf{x}_j - \\bar{\\mathbf{x}})(\\mathbf{x}_j - \\bar{\\mathbf{x}})^\\top\n  \n\n\n\nR Code: Sample Mean and Sample Covariance\nn = nrow(setosa_data)\np = ncol(setosa_data)\n\nx_bar = colMeans(setosa_data)\ncat(\"Sample Mean Vector (x_bar):\", x_bar)\n\n\nSample Mean Vector (x_bar): 5.006 3.428\n\n\nR Code: Sample Mean and Sample Covariance\nS &lt;- cov(setosa_data)\ncat(\"Sample Covariance Matrix (S):\\n\")\n\n\nSample Covariance Matrix (S):\n\n\nR Code: Sample Mean and Sample Covariance\nS\n\n\n             Sepal.Length Sepal.Width\nSepal.Length   0.12424898  0.09921633\nSepal.Width    0.09921633  0.14368980",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inference for Single Mean Vector</span>"
    ]
  },
  {
    "objectID": "ch4/04-Inference_for_Means.html#hotellings-t2-test",
    "href": "ch4/04-Inference_for_Means.html#hotellings-t2-test",
    "title": "4  Inference for Single Mean Vector",
    "section": "4.4 Hotelling’s T^2 Test",
    "text": "4.4 Hotelling’s T^2 Test\nThis is the multivariate workhorse, analogous to the squared t-statistic.\n\n4.4.1 Hotelling’s T^2 Statistic\nHotelling’s T^2 statistic measures the “distance” between the sample mean vector \\bar{\\mathbf{x}} and the hypothesized mean vector \\boldsymbol{\\mu}_0, accounting for sample size and covariance.\n T^2 = n(\\bar{\\mathbf{x}} - \\boldsymbol{\\mu}_0)^\\top \\mathbf{S}^{-1} (\\bar{\\mathbf{x}} - \\boldsymbol{\\mu}_0) \n\n\n4.4.2 The F-Distribution Connection\nThe T^2 statistic is not directly used. Instead, it is rescaled to a familiar F-statistic. Under H_0: \\boldsymbol{\\mu} = \\boldsymbol{\\mu}_0,\n \\frac{n-p}{p(n-1)} T^2 \\, \\overset{H_0}\\sim\\, F_{p, n-p} \nWe reject H_0 if our calculated F-statistic (or scaled T^2 statistic) is greater than the critical value F_{p, n-p, 1-\\alpha} at significance level \\alpha.\n\n\n4.4.3 Assumptions\n\nThe observations \\mathbf{x}_1, \\dots, \\mathbf{x}_n are independently drawn from the same population distribution.\nThe population distribution is multivariate normal (MVN): \\mathbf{x}_i \\overset{iid}{\\sim} N_p(\\boldsymbol{\\mu}, {\\Sigma}), i=1,\\ldots, n.\n\n\n\n4.4.4 Example: Sweat Data\nA study measured sweat rate, sodium content, and potassium content for 20 healthy females. Let’s test if the mean vector is \\boldsymbol{\\mu}_0 = [4, 50, 10]^\\top.\n\n# sweat data\nsweat = data.frame(\n    Rate = c(3.7, 5.7, 3.8, 3.2, 3.1, 4.6, 6.2, 3.4, 4.1, 5.5, 6.5,  \n             4.5, 3.9, 4.5, 6.2, 4.1, 5.5, 6.0, 5.2, 4.8),\n    Sodium = c(48.5, 65.1, 47.2, 31.1, 59.8, 37.8, 52.8, 43.2, 45.1,\n               50.3, 58.3, 40.2, 38.9, 48.8, 60.1, 44.5, 55.5, 59.9, \n               57.7, 51.0),\n    Potassium = c(9.3, 11.1, 9.6, 9.8, 8.0, 10.9, 12.2, 8.5, 9.2, \n                  10.4, 11.2, 9.7, 8.8, 10.1, 12.3, 9.5, 11.3, \n                  12.0, 10.8, 10.5)\n)\nmu0_sweat = c(4, 50, 10)\nnames(sweat) = c(\"x1\", \"x2\", \"x3\")\n\n\n\n\n\n\n\nStep 1: Normality Check\n\n\n\n\n\n\n\nCode\n## Q-Q plot\npar(mfrow = c(1, 3))\n\n# Loop through the first 3 column names \nfor (col_name in colnames(sweat)[1:3]) {\n  qqnorm(sweat[[col_name]], main = col_name)\n  qqline(sweat[[col_name]], col = \"red\", lwd = 2)\n}\n\n\n\n\n\n\n\n\n\nCode\n## Test normality for each variable\nsapply(colnames(sweat[ ,1:3]), function(x) {\n               shapiro.test(sweat[[x]]) } )\n\n\n          x1                            x2                           \nstatistic 0.9483909                     0.9757974                    \np.value   0.3433242                     0.8692117                    \nmethod    \"Shapiro-Wilk normality test\" \"Shapiro-Wilk normality test\"\ndata.name \"sweat[[x]]\"                  \"sweat[[x]]\"                 \n          x3                           \nstatistic 0.9759253                    \np.value   0.8714673                    \nmethod    \"Shapiro-Wilk normality test\"\ndata.name \"sweat[[x]]\"                 \n\n\nCode\n## Perform multivariate normality test\nlibrary(mvShapiroTest)\nmvShapiro.Test(as.matrix(sweat[ , 1:3]))\n\n\n\n    Generalized Shapiro-Wilk test for Multivariate Normality by\n    Villasenor-Alva and Gonzalez-Estrada\n\ndata:  as.matrix(sweat[, 1:3])\nMVW = 0.96404, p-value = 0.7893\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Calculate Sample Statistics\n\n\n\n\n\n\n\nCode\nn = nrow(sweat)\np = ncol(sweat)\nxbar = colMeans(sweat)\nS = cov(sweat)\nS_inv = solve(S)\n\n\n\n\n\n\n\n\n\n\n\nStep 3: Compute Hotelling’s T^2 Statistic\n\n\n\n\n\n\n\nCode\n# Explicit calculation \ndiff_vec &lt;- xbar - mu0_sweat\nT2_manual &lt;- n * t(diff_vec) %*% S_inv %*% diff_vec\ncat(\"Explicitly calculated T-squared:\", as.numeric(T2_manual), \"\\n\")\n\n\nExplicitly calculated T-squared: 43.50518 \n\n\nCode\n# Convert to F-statistic or scaled T-square statistic \nF_stat &lt;- as.numeric(T2_manual) * (n - p) / \n  (p * (n - 1))\ncat(\"Calculated F-statistic:\", F_stat, \"\\n\")\n\n\nCalculated F-statistic: 12.97523 \n\n\nCode\n# Find critical value\nalpha &lt;- 0.05\nF_crit &lt;- qf(1 - alpha, p, n - p)\ncat(\"Critical F-value:\", F_crit, \"\\n\")\n\n\nCritical F-value: 3.196777 \n\n\nCode\ncat(\"Decision:\", \n    ifelse(F_stat &gt; F_crit, \"Reject H0\", \"Do not reject H0\"), \"\\n\")\n\n\nDecision: Reject H0 \n\n\nConclusion: Since our F-statistic (12.98) is much larger than the critical F-value (3.197), we reject the null hypothesis at level \\alpha=0.05. The mean sweat composition is significantly different from [4, 50, 10]^\\top.\n\n\n\n\n\n\n\n\n\nHotelling’s T^2 Test\n\n\n\nThe explicit calculations in Steps 2-3 above can be completed using R package DescTools after normality check. If the data follows a multivariate normal distribution, then we can perform the Hotelling’s T^2 test; if the normality assumption is violated, the conclusion is misleading and this limitation should be acknowledged.\n\n\nR Code: Hotelling’s T2 test\n# install.packages(\"DescTools\")\nlibrary(DescTools)\nht_sweat &lt;- HotellingsT2Test(x=sweat, mu = mu0_sweat)\nprint(ht_sweat)\n\n\n\n    Hotelling's one sample T2-test\n\ndata:  sweat\nT.2 = 12.975, df1 = 3, df2 = 17, p-value = 0.0001177\nalternative hypothesis: true location is not equal to c(4,50,10)\n\n\nNote: At this point we do not know which of the two hypothesized mean values is not supported by the data.\n\n\n\n\n\n\n\n\nR Function HotellingsT2Test\n\n\n\nWhen the function HotellingsT2Test in the R package DescTools is used to perform the test, its output reports the scaled T^2 statistic (or F statistic) named T.2 (which is 12.975 in this example). This scaled T^2 statistic should not be confused with the T^2 statistic as the scaled T^2 statistic is \\frac{n-p}{p(n-1)}T^2.\n\n\n\n\n4.4.5 Exercise: Iris data\nBackground\nRonald Fisher’s iris dataset is a cornerstone of statistical analysis. Imagine a historical botanical guide from the 1930s describes the “type specimen” for the Iris setosa species as having a mean Sepal Length of 5.1 cm and a mean Sepal Width of 3.6 cm.\nYour task is to determine if the sample of 50 Iris setosa flowers from Fisher’s dataset is consistent with this historical description. You will use a Hotelling’s T² test with a significance level of \\alpha = 0.05.\n1. State the Hypotheses\nWrite the null hypothesis (H_0) and the alternative hypothesis (H_1) for this test. Let \\boldsymbol{\\mu} represent the true mean vector of [Sepal.Length, Sepal.Width].\n\n\n\n\n\n\nView Solution\n\n\n\n\n\nThe hypotheses are formulated as:\n\nNull Hypothesis (H_0): The true mean sepal measurements are equal to the historical description. H_0: \\boldsymbol{\\mu} = \\begin{bmatrix} 5.1 \\\\ 3.6 \\end{bmatrix}\nAlternative Hypothesis (H_1): The true mean sepal measurements are not equal to the historical description. H_1: \\boldsymbol{\\mu} \\neq \\begin{bmatrix} 5.1 \\\\ 3.6 \\end{bmatrix}\n\n\n\n\n2. Prepare and Visualize the Data\nLoad the necessary R packages. From the built-in iris dataset, create a final data frame containing only the Sepal.Length and Sepal.Width for the setosa species.\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nR Code: Data Preparation\n# Load the packages needed for the entire analysis\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Create the final data frame for analysis\nsetosa_data &lt;- iris %&gt;%\n  filter(Species == \"setosa\") %&gt;%\n  dplyr::select(Sepal.Length, Sepal.Width)\n\ndf = setosa_data\nn = nrow(df)\np = ncol(df)\n# Display the first few rows of the prepared data\nhead(df)\n\n\n\n  \n\n\n\nR Code: Data Preparation\npairs(df)\n\n\n\n\n\n\n\n\n\n\n\n\n3. Perform the Statistical Test\nRun a one-sample Hotelling’s T^2 test on your prepared data using the historical description as your hypothesized mean vector.\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nR Code: Hotelling’s T-squared Test\n# Define the hypothesized mean vector\nmu0 = c(5.1, 3.6)\n\n# Perform the test\ntest_result = DescTools::HotellingsT2Test(\n  x = df, \n  mu = mu0)\n\n# Print the results\nprint(test_result)\n\n\n\n    Hotelling's one sample T2-test\n\ndata:  df\nT.2 = 5.3116, df1 = 2, df2 = 48, p-value = 0.008244\nalternative hypothesis: true location is not equal to c(5.1,3.6)\n\n\n\n\n\n4. Interpret the Results\nWhat is the p-value from your test? Based on this and \\alpha = 0.05, do you reject or fail to reject the null hypothesis? State your conclusion in the context of the problem.\n\n\n\n\n\n\nView Solution\n\n\n\n\n\nThe p-value is 0.008244. Since this p-value is less than our significance level of \\alpha = 0.05, we should reject the null hypothesis at level 0.05.\nConclusion: There is enough statistical evidence to conclude that the mean sepal measurements of the setosa flowers in Fisher’s dataset are different from the historical description. The benchmark of 5.1 cm length and 3.6 cm width is not a plausible value for the true mean of this sample.\n\n\n\n5. Visualize the Conclusion\nCreate a scatter plot of Sepal.Width v.s. Sepal.Length. On the plot, mark the sample mean, the hypothesized mean, and add the 95% confidence ellipse. Explain how it supports your conclusion.\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nR Code: Visualization\n# Calculate the sample mean and sample covariance \nx_bar = colMeans(df)\nS = cov(df)\n\n# Calculate points for the 95% confidence ellipse\nalpha = 0.05\nFvalue = sqrt(p * (n - 1) / (n - p) * qf(1 - alpha, p, n - p))\n\nconfidence_ellipse &lt;- as.data.frame(ellipse::ellipse(\n  S,\n  centre = x_bar,\n  level = 1-alpha,\n  t = Fvalue / sqrt(n)\n))\n\n# Create the plot\nggplot(df, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point(color = \"purple\",\n             alpha = 0.6,\n             size = 2.5) +\n  geom_path(\n    data = confidence_ellipse,\n    aes(x = Sepal.Length, y = Sepal.Width),\n    color = \"blue\",\n    linewidth = 1,\n    inherit.aes = FALSE\n  ) +\n  annotate(\"point\", \n    x = x_bar[1], y = x_bar[2],\n    color = \"blue\", size = 5\n  ) +\n  annotate(\"text\",\n    x = x_bar[1], y = x_bar[2] + 0.03,\n    label = \"Sample Mean\", color = \"blue\"\n  ) +\n  annotate(\"point\",\n    x = mu0[1], y = mu0[2],\n    color = \"red\", size = 5,\n    shape = 4, stroke = 1.5\n  ) +\n  annotate(\"text\",\n    x = mu0[1], y = mu0[2] - 0.03,\n    label = \"Historical Description\",\n    color = \"red\"\n  ) +\n  labs(\n    title = \"Iris Setosa Sepal Measurements\",\n    subtitle = \"Hotelling's T² Test vs. Historical Description\",\n    x = \"Sepal Length (cm)\",\n    y = \"Sepal Width (cm)\"\n  ) +\n  theme_bw()\n\n\n\n\n\nComparison of the Iris setosa sample to a historical description.\n\n\n\n\nVisual Interpretation: The plot confirms the test result. The red cross (the historical description) lies outside the blue 95% confidence ellipse. Since the hypothesized value is outside the confidence region of plausible values for the true mean, it visually supports our conclusion to reject the null hypothesis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inference for Single Mean Vector</span>"
    ]
  },
  {
    "objectID": "ch4/04-Inference_for_Means.html#confidence-regions-for-boldsymbolmu",
    "href": "ch4/04-Inference_for_Means.html#confidence-regions-for-boldsymbolmu",
    "title": "4  Inference for Single Mean Vector",
    "section": "4.5 Confidence Regions for \\boldsymbol{\\mu}",
    "text": "4.5 Confidence Regions for \\boldsymbol{\\mu}\nA confidence interval for a single mean becomes a confidence region (an ellipse for p=2, an ellipsoid for p&gt;2) for a mean vector. This confidence region is also known as joint confidence.\nA 100(1-\\alpha)\\% confidence region for \\boldsymbol{\\mu} is the set of all vectors \\boldsymbol \\mu satisfying:  n(\\bar{\\mathbf{x}} - \\boldsymbol{\\mu})^\\top {S}^{-1} (\\bar{\\mathbf{x}} - \\boldsymbol{\\mu}) \\le \\frac{p(n-1)}{n-p} F_{p, n-p, 1-\\alpha} \n\n\n\n\n\n\nConfidence Region\n\n\n\nThe confidence region forms an ellipsoid. Its shape and orientation are determined by the eigenvalues and eigenvectors of the sample covariance matrix {S}.\n\nThe center of the ellipsoid is \\bar{\\mathbf{x}}.\nThe distance from the center of the ellipsoid to the edge of the ellipsoid along the i-th axis is \n\\pm \\sqrt{\\lambda_i} \\sqrt{\\frac{(n-1)p}{n(n-p)}F_{(p, n-p), 1-\\alpha}}.\n\nThe directions of the eigenvectors and sizes of the eigenvalues depend on\n\nthe relative sizes of the variances of the measured variables\nthe sizes of the correlations between pairs of variables\n\n\n\n\n\n4.5.1 Plot Confidence Ellipse (p=2) via the ellipse Package\nLet’s plot the 95% confidence ellipse for our setosa variable\n\n\nR Code: Plot an Ellipse\n# Plot the points\nplot(df,\n     xlab = \"Sepal Length\",\n     ylab = \"Sepal Width\",\n     main = \"95% Confidence Ellipse for Setosa Mean\",\n     pch = 19, col = alpha(\"purple\", 0.5),\n     xlim=c(4.3, 5.8), \n     ylim=c(2, 4.5)\n)\n\n# Add the sample mean\npoints(x_bar[1], x_bar[2], pch = 19, col = \"blue\", cex = 1.5)\n\n# Add the hypothesized mean\npoints(mu0[1], mu0[2], pch = 4, col = \"red\", cex = 1.5, lwd=2)\n\n# Add the ellipse\nalpha = 0.05\nFvalue = sqrt(p * (n - 1) / (n - p) * \n                qf(1 - alpha, p, n - p) \n              )\n\n\nlines(ellipse::ellipse(S, \n              centre = x_bar, \n              level = 1-alpha, \n              t=Fvalue/sqrt(n)\n              ), \n      col = \"blue\", lwd = 2)\n\n\nlegend(\"topleft\",\n       legend = c(\"Data\", \"Sample Mean\", \n                  \"Hypothesized Mean\", \n                  \"95% Confidence Ellipse\"), \n       cex=0.8,\n       col = c(\"purple\", \"blue\", \"red\", \"blue\"),\n       pch = c(19, 19, 4, NA),\n       lty = c(NA, NA, NA, 1),\n       lwd = c(NA, NA, 2, 2)\n       )\n\n\n\n\n\n95% confidence ellipse for the true mean vector of Setosa sepal measurements. The sample mean is the blue dot, and the hypothesized mean is the red cross.\n\n\n\n\nOur hypothesized mean (red cross) falls outside the 95% confidence ellipse, which is consistent with rejecting the null hypothesis in the Hotelling’s T^2 test.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inference for Single Mean Vector</span>"
    ]
  },
  {
    "objectID": "ch4/04-Inference_for_Means.html#simultaneous-confidence-intervals",
    "href": "ch4/04-Inference_for_Means.html#simultaneous-confidence-intervals",
    "title": "4  Inference for Single Mean Vector",
    "section": "4.6 Simultaneous Confidence Intervals",
    "text": "4.6 Simultaneous Confidence Intervals\nIf we reject H_0, we want to know which variables contributed to the rejection. We need CIs that hold simultaneously for all p variables with confidence level at least 1-\\alpha.\n\n4.6.1 T^2 CIs\nThe T^2 confidence intervals are derived from the Hotelling’s T^2 statistic. For each component \\mu_i:  \\bar{x}_i \\pm \\sqrt{\\frac{p(n-1)}{n-p} F_{p, n-p, 1-\\alpha}} \\sqrt{\\frac{s_{ii}}{n}}  where s_{ii} is the (i,i)-th entry of {S}.\n\n\n4.6.2 Bonferroni CIs\nAnother way to construct individual confidence intervals for each \\mu_i is to use the t-interval, but the combined set of individual t intervals result in a simulatenous confidence level that is less than than the normal level 1-\\alpha.\nTo avoid this limitation, one can use the so-called Bonferroni confidence intervals:  \\bar{x}_i \\pm t_{n-1, 1- \\alpha/(2p)} \\sqrt{\\frac{s_{ii}}{n}}  where p is the number of variables to be estimated.\n\n\n\n\n\n\nOne-At-a-Time t CI\n\n\n\nUsing the univariate approach, we can construct t-intervals for each of mean differences, and obtain the so-called one-at-a-time t intervals: \n\\bar{x}_i \\pm t_{n-1, 1- \\alpha/2} \\sqrt{\\frac{s_{ii}}{n}}\n The one-at-a-time CI does not control the family-wise error at \\alpha or the confidence level at least at the nominal level 1-\\alpha. Bonferroni CI corrects the one-at-a-time CI and produces a set of CIs that jointly have the confidence level at least 1-\\alpha.\n\n\n\n\n4.6.3 Comparison of Confidence Region, T^2 CI and Bonferroni CI\n\n\n\n\n\n95% joint confidence region (ellipse) with rectangles for T-squared intervals (red shaped area) and Bonferroni simultaneous intervals (purple shaped area).\n\n\n\n\n\n\n4.6.4 Example: Microwave Oven Data\nManufacturers of a microwave oven are concerned about radiation emission. They measure radiation with the door closed and open from n=12 ovens. Test if the mean radiation levels are \\boldsymbol{\\mu}_0 = [0.20, 0.37]'.\n\n# Generate some plausible data\nset.seed(4750)\noven_data &lt;- data.frame(\n  Closed = rnorm(12, mean = 0.25, sd = 0.08),\n  Open = rnorm(12, mean = 0.30, sd = 0.10)\n)\ndat = oven_data\nmu0 &lt;- c(0.20, 0.37)\n\n\n\n\n\n\n\nState The Hypotheses\n\n\n\n\n\nLet \\boldsymbol \\mu = (\\mu_1, \\mu_2)^\\top represents the mean radiation levels corresponding to door closed and door open conditions. The historical standard or hypothesized mean vector is \\boldsymbol \\mu_0. The hypotheses are \nH_0: \\boldsymbol \\mu  = \\boldsymbol \\mu_0 \\quad \\text{v.s.} \\quad H_1: \\boldsymbol \\mu \\neq \\boldsymbol \\mu_0\n\n\n\n\n\n\n\n\n\n\nCompute Test Statistic\n\n\n\n\n\nIf we do a manual calculation, we need to compute the Hotelling’s T^2 test\n\n\nR Code: T^2 Statistic\nxbar = colMeans(dat)\nS = cov(as.matrix(dat))\nn = nrow(dat)\np = ncol(dat)\nT2 = n*t(xbar-mu0)%*%solve(S)%*%(xbar-mu0)\nscaledT2 = (n-p)/(p*(n-1))*drop(T2)\nF_crit =  qf(0.95, p, n-p)\nif(scaledT2&gt;F_crit){\n  cat(paste0(\" scaled T2=\", signif(scaledT2,3)),\"&gt;\", \n      paste0(\"F critical value=\", signif(F_crit,3)), \n      \", thus we reject H0 at level 0.05\")\n}else{\n  cat(paste0(\" scaled T2=\", signif(scaledT2,3)),\"&lt;\", \n      paste0(\"F critical value=\", signif(F_crit,3)), \n      \", thus we fail to reject H0 at level 0.05\")\n}\n\n\n scaled T2=4.87 &gt; F critical value=4.1 , thus we reject H0 at level 0.05\n\n\n\n\nR Code: Use HotellingsT2Test\n# First, run the T2 test\nT2result &lt;- HotellingsT2Test(x=dat, mu = mu0)\nprint(T2result)\n\n\n\n    Hotelling's one sample T2-test\n\ndata:  dat\nT.2 = 4.8713, df1 = 2, df2 = 10, p-value = 0.03334\nalternative hypothesis: true location is not equal to c(0.2,0.37)\n\n\n\n\n\n\n\n\n\n\n\nSimultaneous T^2 CIs\n\n\n\n\n\n\n\nR Code: Simultaneous T2 CIs\n# The p-value is tiny, so we reject H0. Let's find out why.\n\ns_ii &lt;- diag(S)\n\n# T2-based intervals\nT2_multiplier &lt;- sqrt(p * (n - 1) / (n - p) * F_crit)\nT2_margins &lt;- T2_multiplier * sqrt(s_ii / n)\nT2_intervals &lt;- data.frame(\n  Variable = names(oven_data),\n  Lower = xbar - T2_margins,\n  Upper = xbar + T2_margins\n)\nprint(\"95% T-squared Simultaneous CIs:\")\n\n\n[1] \"95% T-squared Simultaneous CIs:\"\n\n\n\n\n       Variable     Lower     Upper\nClosed   Closed 0.1829773 0.2941655\nOpen       Open 0.1688177 0.3802530\n\n\n\n\n\n\n\n\n\n\n\nSimultaneous Bonferroni CIs\n\n\n\n\n\n\n\nR Code: Simultaneous Bonferroni CIs\n# Bonferroni-based intervals\nbonf_multiplier &lt;- qt(1 - 0.05 / (2 * p), df = n - 1)\nbonf_margins &lt;- bonf_multiplier * sqrt(s_ii / n)\nbonf_intervals &lt;- data.frame(\n  Variable = names(dat),\n  Lower = xbar - bonf_margins,\n  Upper = xbar + bonf_margins\n)\nprint(\"95% Bonferroni Simultaneous CIs:\")\n\n\n[1] \"95% Bonferroni Simultaneous CIs:\"\n\n\n\n\n       Variable     Lower     Upper\nClosed   Closed 0.1905876 0.2865551\nOpen       Open 0.1832896 0.3657812\n\n\nConclusion: Both methods produce similar intervals. Both the T2 CI and Bonferroni CI for “Closed” Oven contain its hypothesized mean 0.2. The T2 CI for “Open” oven contains its hypothesized mean 0.37 while the Bonferroni CI does not contain its hypothesized mean 0.37. Therefore, we conclude that\n\nthere is no sufficient evidence conclude that the mean radiation level for “Closed” oven is the significantly different from the specified standard using both methods at significance level 0.05;\nfor “Open” oven, there is no sufficient evidence to conclude that the mean radiation level for “Open” oven is significantly different from the specified standard using T2 CI at significance level 0.05; while the mean radiation level for “Open” Oven is significantly higher than the specified standard using Bonferroni CI at significance level 0.05.\nNote that the Bonferroni CIs are slightly narrower (more precise) in this example.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inference for Single Mean Vector</span>"
    ]
  },
  {
    "objectID": "ch4/04-Inference_for_Means.html#sec-LinearCombinations",
    "href": "ch4/04-Inference_for_Means.html#sec-LinearCombinations",
    "title": "4  Inference for Single Mean Vector",
    "section": "4.7 Inference for Linear Combinations of Means",
    "text": "4.7 Inference for Linear Combinations of Means\nIn many scientific applications, one is interested in making statistical inference for a linear combination of the mean components (also called mean contrast) based on multivariate data. In such applications, we can test hypotheses about any linear combination of the mean components, H_0: \\mathbf{c}'\\boldsymbol{\\mu} = c_0. One can still apply previous methods to perform the test. This is useful for testing things like “Is the average of all means equal to 5?” or “Is the difference between mean 1 and mean 2 equal to zero?”\n\n\n\n\n\n\nDogs Anesthetics Study (J. & W. 2007)\n\n\n\nBackground\nA study was conducted on a sample of 19 dogs to evaluate the effect of CO2 pressure and the anesthetic halothane on heart rate. Each dog was then administered carbon dioxide CO2 at each of two pressure levels. Next, halothane (H) was added and then administration of CO2 was repeated. There was a washout period (several weeks) between the use of one anesthetic and the use of another anesthetic. The response variable is the time in milliseconds between heartbeats. The data were obtained by measuring the response under four treatment combinations: (1) high CO2 pressure, (2) low CO2 pressure, (3) high pressure + halothane, and (4) low pressure + halothane.\n\n# load data\ndogdat &lt;- read.table(\"dogs.dat\",\n            col.names=c(\"dog\", \"HighCO2\", \"LowCO2\", \n                        \"HighCO2H\", \"LowCO2H\")) %&gt;%\n  mutate(across(c(\"HighCO2\", \"LowCO2\", \n         \"HighCO2H\", \"LowCO2H\"), as.numeric))\nhead(dogdat)\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nExperimental Setup\n\n\n\n\nDesign: p=4 treatments are assigned to the same experimental unit. In total, there are n= 19 experimental units.\nObjective: our interest is to formally assess whether there is significant difference between the two corresponding mean responses.\nMean responses: Let \\mu_j denote the mean response under treatment j, where j=1,\\ldots, 4. \\boldsymbol \\mu = (\\mu_1, \\ldots, \\mu_p)^\\top is a vector of mean responses over all p treatments (high CO2, low CO2, high CO2 + halothane, low CO2 + halothane).\nData: Let \\mathbf{x}_i=(x_{i1}, x_{i2}, \\ldots, x_{ip})^\\top denote the response variable for the i-th experimental unit under all the p treatments, where i=1, \\ldots, n.\n\n\n\n\n\n\n\n\n\nResearch Questions\n\n\n\n\n\nAs we want to assess whether there is significant difference between the two corresponding mean responses, we are interested in testing three specific effects simultaneously:\n\nIs there a main effect of halothane?\nIs there a main effect of CO2?\nIs there an interaction effect?\n\n\n\n\n\n\n\n\n\n\nStep 1: Prepare and Visualize the Data\n\n\n\n\n\n\n\nR Code: Scatterplot Matrix\nX = as.matrix(dogdat[ ,2:5])\nGGally::ggpairs(X)\n\n\n\n\n\n\n\n\n\nInterpretation: The smoothed histograms on the diagonal of the scatterplot matrix show unimodal distributions, but the responses to the high CO2 anesthetic appear to be skewed left and tails of the distributions appear to be truncated relative to normal distributions. The scatter plots show nearly elliptically shaped data clouds, however, with approximately straight line trends. Consequently, the pairwise correlations appear to provide a good description of the associations between the responses to the four anesthetics.\n\n\nR Code: Boxplot\nboxplot(X)\n\n\n\n\n\n\n\n\n\nInterpretation: The boxplots indicate that variation in times between heartbeats across the 19 dogs appear to be about the same for all for anesthetics. The distributions are roughly symmetric for all four anesthetics, but the distribution of times between heartbeats may be skewed left when the high level of CO2 is used without halothane. The boxplots also indicate the mean times between heartbeats tend to be higher when halothane is used, regardless of the level of CO2, so halothane may slow heart rates in dogs. There does not appear to be much difference in the distributions of times between heartbeats for low and high levels of CO2 when halothane is used. There is also not much difference between the distributions of times between heartbeats for the high and low levels of CO2 when halothane is not used, although times between heartbeats tend to be lower when the high level of CO2 is used. The higher level of CO2 may speed up heart rates in dogs, but the Halothane effect appears to have a much larger than the CO2 effect.\n\n\n\n\n4.7.1 Repeated Measures\nA repeated measures study is a research design where the same participants are measured multiple times under different conditions or over a period of time\nKey characteristics\n\nSame participants across conditions or time points\nMeasurements may be taken:\n\nAcross treatments (within-subjects design) – e.g., each participant tries all drug dosages.\nOver time (longitudinal design) – e.g., measuring blood pressure weekly for 8 weeks.\n\nCorrelation between measurements\n\nAdvantages\n\nControls for individual differences → reduces variability and increases statistical power.\nFewer participants needed compared to a between-subjects design for the same precision.\nAbility to track changes within individuals over time or across conditions.\n\nCommon Examples\n\nBefore-and-After Studies: This is the classic setup. You measure a group’s cholesterol levels (before), put them on a new diet for three months, and measure their levels again (after).\nComparing Multiple Treatments: Each participant is given several different drugs (with a washout period in between), and their reaction to each drug is measured. The dog anesthetic example from our previous conversation is a perfect case of this.\nLongitudinal Studies: A researcher tracks the same group of children and measures their reading ability at ages 6, 8, and 10 to see how it develops over time.\n\nKey Challenge: Dependent Data\nWhen the same subject is measured on p different occasions or under p different conditions, the measurements are not independent. A person’s score at time 1 is related to their score at time 2. Because of this dependency, we cannot directly apply previous methods on the data. However, we can still analyze this data by creating difference vectors and performing a one-sample Hotelling’s T^2 test on these differences.\n\n\n4.7.2 Hotelling’s T^2 Test for Contrasts\n\n\n\n\n\n\nStep 2: State Hypotheses\n\n\n\n\n\nThe research questions can be formulated as a hypothesis testing problem to test treatment effects simultaneously:\n\nHalothane Effect: The overall effect of halothane, averaging across CO2 pressure levels: (\\mu_3+\\mu_4)/2 - (\\mu_1+\\mu_2)/2.\nCO2 Pressure Effect: The overall effect of CO2 pressure, averaging across halothane levels: (\\mu_1+\\mu_3)/2 - (\\mu_2+\\mu_4)/2\nInteraction Effect: Whether the effect of CO2 pressure depends on the presence of halothane: (\\mu_1-\\mu_2)-(\\mu_3-\\mu_4)\n\n\nNull hypothesis: The no treatment effect can be written as testing H_0: C \\boldsymbol \\mu =\\mathbf{0} for a q\\times p contrast matrix C.\n\nq is the number of contrasts. In this example, q=3.\n\n\nAlternative hypothesis: H_1: C\\boldsymbol \\mu\\neq \\mathbf{0}\n\nQuestion: How to write down the contrast matrix for the null hypothesis?\n\nExercise: If one is interested in testing the hypothesis that all four treatments have the same mean, then \\begin{equation*}\nC=\n\\left[\\begin{array}{rrrr}\n-1 & 1 & 0 & 0 \\\\ 0 & -1 & 1 & 0 \\\\ 0 & 0 & -1 & 1\n\\end{array}\\right]\n\\end{equation*}\n\n\n\n\n\n\n\n\n\n\nStep 3: Define the Contrast Matrix\n\n\n\n\n\nThe contrast matrix defined by the null hypotheses above is given by\n\\begin{equation*}\nC=\n\\begin{bmatrix}\n-.5 & -.5 & .5 & .5 \\\\\n.5 & -.5 & .5 & -.5 \\\\\n1 & -1 & -1 & 1\n\\end{bmatrix}\n\\end{equation*} because\n\n\\begin{bmatrix}\n(\\mu_3 + \\mu_4)/2 - (\\mu_1 + \\mu_2)/2 \\\\\n(\\mu_1 + \\mu_3)/2 - (\\mu_2 + \\mu_4)/2 \\\\\n(\\mu_1 - \\mu_2) - (\\mu_3-\\mu_4)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-.5 & -.5 & .5 & .5 \\\\\n.5 & -.5 & .5 & -.5 \\\\\n1 & -1 & -1 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mu_1 \\\\\n\\mu_2 \\\\\n\\mu_3 \\\\\n\\mu_4 \\end{bmatrix}\n= {C} \\boldsymbol{\\mu}\n\n\nIt is important to note that C has dimension q\\times p, where q=p-1 is the number of contrasts.\n\n\n\nCode\nC = matrix( c(-.5, -.5, .5, .5, \n              .5, -.5, .5, -.5, \n              1, -1, -1, 1), \n    nrow=3, ncol=4, byrow=T)\nC\n\n\n     [,1] [,2] [,3] [,4]\n[1,] -0.5 -0.5  0.5  0.5\n[2,]  0.5 -0.5  0.5 -0.5\n[3,]  1.0 -1.0 -1.0  1.0\n\n\n\n\n\n\n\n\n\n\n\nStep 4: Transform the Data Using the Contrast Matrix\n\n\n\n\n\nFor each dog, apply the contrasts defined in matrix C to their four measurements. This converts the original 4 variables into 3 new contrast variables, which will be the subject of our test.\nWe use matrix multiplication to transform our 19\n\\times 4 data matrix into a 19\n\\times 3 matrix of contrast scores. The formula is Y = X  C^\\top.\n\n\nCode\n# Transform the original data into contrast data\nY &lt;-  X %*% t(C)\n\n# Rename columns for clarity\ncolnames(Y) &lt;- c(\"Halothane_Effect\", \"CO2_Effect\", \"Interaction_Effect\")\n\nhead(Y)\n\n\n     Halothane_Effect CO2_Effect Interaction_Effect\n[1,]             60.5     -113.5               -139\n[2,]            149.0        7.0                 20\n[3,]            -43.0      -41.0                -66\n[4,]            129.5      -38.5                 79\n[5,]             97.5      -10.5                -21\n[6,]            142.0      -73.0                -82\n\n\nCode\nGGally::ggpairs(Y)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 5: Check Assumptions\n\n\n\n\n\n\n\nCode\npar(mfrow=c(2,2),pch=1)     \nfor (i in 1:3){\n  qqnorm(Y[,i],  main=\"Normal Q-Q Plot\") \n  qqline(Y[,i], col=\"red\", lwd=2)\n}\n\napply(Y,2,shapiro.test)\n\n\n$Halothane_Effect\n\n    Shapiro-Wilk normality test\n\ndata:  newX[, i]\nW = 0.80247, p-value = 0.001242\n\n\n$CO2_Effect\n\n    Shapiro-Wilk normality test\n\ndata:  newX[, i]\nW = 0.97195, p-value = 0.8146\n\n\n$Interaction_Effect\n\n    Shapiro-Wilk normality test\n\ndata:  newX[, i]\nW = 0.96055, p-value = 0.5834\n\n\nCode\nmvShapiroTest::mvShapiro.Test(Y)\n\n\n\n    Generalized Shapiro-Wilk test for Multivariate Normality by\n    Villasenor-Alva and Gonzalez-Estrada\n\ndata:  Y\nMVW = 0.92376, p-value = 0.05625\n\n\n\n\n\n\n\n\n\nConclusion: The contrast data indeed approximately follow a multivariate normal distribution as indicated by the multivariate Shapiro-Wilk test even though the original data do not follow a multivariate normal distribution.\nNote: In practice, if the multivariate normality assumption is violated based on the Shapiro-Wilk test, one needs to address this issue before applying the Hotelling’s T^2 test. Alternatively, one can use a nonparametric method (e.g., Friedman test).\n\n\n\n\n\n\n\n\n\nStep 6: Hotelling’s T^2 Test\n\n\n\n\n\n\nTo test H_0: C\\boldsymbol \\mu = 0, we can again use T^2 statistic: \nT^2 = n(C\\bar{\\mathbf{x}} - \\mathbf{0})^\\top (CSC^\\top)^{-1} (C\\bar{\\mathbf{x}} - \\mathbf{0})\n\nThe null hypothesis is rejected at significance level \\alpha if \nT^2 \\geq \\frac{(n-1)(p-1)}{(n-p+1)} F_{p-1, n-p+1, 1-\\alpha}\n where the numerator degrees of freedom are p-1 instead of p because the null hypotheses puts only p-1 constraints on the mean responses.\n\n\n\nCode\n# Define the hypothesized mean vector (a zero vector)\nmu0 &lt;- c(0, 0, 0)\n\n# Perform the one-sample test on the contrast data\ntest_result = DescTools::HotellingsT2Test(Y, mu=mu0)\nprint(test_result)\n\n\n\n    Hotelling's one sample T2-test\n\ndata:  Y\nT.2 = 34.375, df1 = 3, df2 = 16, p-value = 3.318e-07\nalternative hypothesis: true location is not equal to c(0,0,0)\n\n\nConclusion: We clearly reject the null at significance level \\alpha=0.05, so the question now becomes whether there is a difference between CO2 pressure, between halothane levels or perhaps there is no main effect of treatment but there is still an interaction. This question can be answered by looking at confidence intervals.\n\n\n\n\n\n\n\n\n\nPaired t-Test\n\n\n\n\n\nA paired t-test is a special, simplified case of the Hotelling’s T² test on contrasts. The powerful, general framework of the F-test on contrasts simplifies to become the familiar paired t-test when you are making only one comparison between two measurements.\nThe Paired t-Test: A Simple Contrast\nA paired t-test is designed to answer one question: “Is there a significant difference between two matched measurements (e.g., before vs. after)?”\n\nThe Data: You have two measurements for each subject, X_1 and X_2.\nThe Method: For each subject, you calculate a single difference score, d = X_1 - X_2. You then perform a one-sample t-test on these difference scores to see if their mean is significantly different from 0.\nThe null hypothesis: H_0: \\mu_d = 0, which is the same as H_0: \\mu_1 - \\mu_2 = 0.\n\n\n\n\n\n\n4.7.3 Confidence Regions for Linear Combinations of Means\nA 100(1-\\alpha)\\% confidence region for any linear combination of population means, say C \\boldsymbol{\\mu}, is the set of all vectors \\boldsymbol \\mu satisfying: \nn(C\\bar{\\mathbf{x}} - C\\boldsymbol{\\mu})^\\top (CSC^\\top)^{-1} (C\\bar{\\mathbf{x}} - C\\boldsymbol{\\mu}) \\le \\frac{(p-1)(n-1)}{n-p+1} F_{p-1, n-p+1, 1-\\alpha}\n\n\n\n4.7.4 Simultaneous CIs for Linear Combinations of Means\nAfter performing a multivariate test, we are often interested in more complex comparisons than just single means. We might want to test a linear combination of the population means, which takes the general form:\n \\mathbf{c}'\\boldsymbol{\\mu} = c_1\\mu_1 + c_2\\mu_2 + \\dots + c_p\\mu_p \nWe can construct simultaneous confidence intervals for a set of m such combinations, where the k-th combination is defined by the coefficient vector \\mathbf{c}_k = [c_{k1}, c_{k2}, \\dots, c_{kp}]'.\nThe point estimate for any linear combination \\mathbf{c}_k'\\boldsymbol{\\mu} is given by \\mathbf{c}_k'\\bar{\\mathbf{x}} = \\sum_{j=1}^p c_{jk} \\mu_j, and the estimated variance is \\frac{\\mathbf{c}_k'{S}\\mathbf{c}_k}{n}. The margin of error is then determined by one of the following methods.\n\n\n\n\n\n\nT^2 CI\n\n\n\nThis method derives its critical value from the F-distribution associated with Hotelling’s T^2 test. The confidence intervals are valid simultaneously for any and all possible linear combinations.\nA 100(1-\\alpha)\\% simultaneous confidence intervals are given by:\n\n\\mathbf{c}_k'\\bar{\\mathbf{x}} \\pm \\sqrt{\\frac{(p-1)(n-1)}{n-p+1} F_{p-1, n-p+1, 1-\\alpha}} \\sqrt{\\frac{\\mathbf{c}_k'{S}\\mathbf{c}_k}{n}} \\quad \\text{for } k=1, 2, \\dots, m\n\nIn the dog anesthetics study, the T^2 CIs can be computed as follows.\n\n\nR Code: T^2 CIs\nybar = colMeans(Y)\nSy = cov(Y)  # this is the same as C%*%cov(X)%*%t(C) \nF_crit = qf(1-0.05, p-1, n-p)\n\ncval_T2 = sqrt((p-1)*(n-1)/(n-p+1) * F_crit) \nS_kk = diag(Sy)\n\nci_T2 = tibble(\n  Component = names(ybar),\n  Estimate = ybar,\n  Lower = Estimate - cval_T2 * sqrt(S_kk/n), \n  Upper = Estimate + cval_T2 * sqrt(S_kk/n)\n)\nprint(ci_T2)\n\n\n# A tibble: 3 × 4\n  Component          Estimate Lower  Upper\n  &lt;chr&gt;                 &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 Halothane_Effect      105.   73.4 136.  \n2 CO2_Effect            -30.0 -53.2  -6.84\n3 Interaction_Effect    -12.8 -68.7  43.1 \n\n\nInterpretation:\n\nThe T^2 CI for the Halothane effect is shifted far above zero, indicating that the mean time between heartbeats is likely to be between 73.4 and 136 milliseconds longer when Halothane is used than that when Halothane is not used, averaging across the CO2 pressure levels.\nThe T^2 CI for the CO2 effect is shifted below 0, indicating that the mean time between heartbeats is likely to be between 53.2 and 6.84 milliseconds shorter at high CO2 pressure than that at low CO2 pressure, averaging across the halothane effect.\n\nThe T^2 CI for the interaction effect contains 0, indicating\n\nthe effect of using halothane on the mean time between heartbeats is about the same for the two levels of CO2 pressure;\nthe effect of changing from low to high pressure of CO2 is about the same when halothane is used as when halothane is not used.\n\n\n\n\n\n\n\n\n\n\nBonferroni CI\n\n\n\nThe 100(1-\\alpha)\\% simultaneous Bonferroni confidence intervals are given by:\n \\mathbf{c}_k'\\bar{\\mathbf{x}} \\pm t_{n-1, 1-\\alpha/(2m)} \\sqrt{\\frac{\\mathbf{c}_k'{S}\\mathbf{c}_k}{n}} \\quad \\text{for } k=1, 2, \\dots, m \n\nNote the critical value is a t-statistic adjusted by m, the number of intervals being constructed.\nIt also controls the family-wise error rate at \\alpha.\n\nIn the dog anesthetics study, the Bonferroni CIs can be computed as follows.\n\n\nR Code: Bonferroni CIs\nm = 3 \n\ncval_Bon = qt(1-0.05/(2*m), n-1)\n\nci_Bon = tibble(\n  Component = names(ybar),\n  Estimate = ybar,\n  Lower = Estimate - cval_Bon * sqrt(S_kk/n), \n  Upper = Estimate + cval_Bon * sqrt(S_kk/n)\n)\nprint(ci_Bon)\n\n\n# A tibble: 3 × 4\n  Component          Estimate Lower   Upper\n  &lt;chr&gt;                 &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Halothane_Effect      105.   65.1 144.   \n2 CO2_Effect            -30.0 -59.4  -0.686\n3 Interaction_Effect    -12.8 -83.6  58.0  \n\n\nInterpretation: While specific Bonferroni CI limits are slightly different than the limits of T^2 CIs, we can notice that same conclusions will be drawn for testing the Halothane effect, the CO2 effect and the interaction effect.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inference for Single Mean Vector</span>"
    ]
  },
  {
    "objectID": "ch4/04-Inference_for_Means.html#exercises",
    "href": "ch4/04-Inference_for_Means.html#exercises",
    "title": "4  Inference for Single Mean Vector",
    "section": "4.8 Exercises",
    "text": "4.8 Exercises\n\n4.8.1 Exercise 1: iris Data, Cont’d\nUsing the iris data, answer the following questions.\n\nFor the versicolor species, test if the mean vector for Sepal.Length and Petal.Length is equal to \\boldsymbol{\\mu}_0 = [6.0, 4.0]'. Use \\alpha=0.05. If you reject the null hypothesis, construct 95% Bonferroni simultaneous CIs to determine which variable(s) differ from the hypothesized values.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\ndat &lt;- iris[iris$Species == \"versicolor\", c(\"Sepal.Length\", \"Petal.Length\")]\nmu0 &lt;- c(6.0, 4.0)\n\nht &lt;- HotellingsT2Test(dat, mu = mu0)\nprint(ht)\n\n\n\n    Hotelling's one sample T2-test\n\ndata:  dat\nT.2 = 24.124, df1 = 2, df2 = 48, p-value = 5.602e-08\nalternative hypothesis: true location is not equal to c(6,4)\n\n\nConclusion: The p-value is much less than 0.05. We reject H_0 at significance level \\alpha=0.05 and conclude that the mean sepal length and the mean petal length is different from the the hypothesized values. To understand which variable would contribute to such difference, we look at the individual CIs below.\n\n\nCode\n# 3. Construct Bonferroni intervals since we rejected H0\nn &lt;- nrow(dat)\np &lt;- ncol(dat)\nxbar &lt;- colMeans(dat)\nS &lt;- cov(dat)\ns_ii &lt;- diag(S)\n\ncval_bon &lt;- qt(1 - 0.05 / (2 * p), df = n - 1)\nbonf_margins &lt;- cval_bon * sqrt(s_ii / n)\nbonf_intervals &lt;- data.frame(\n  Variable = names(dat),\n  Hypothesized_Mean = mu0,\n  Lower = xbar - bonf_margins,\n  Upper = xbar + bonf_margins\n)\n\nprint(\"95% Bonferroni Simultaneous CIs:\")\n\n\n[1] \"95% Bonferroni Simultaneous CIs:\"\n\n\nCode\nprint(bonf_intervals)\n\n\n                 Variable Hypothesized_Mean    Lower    Upper\nSepal.Length Sepal.Length                 6 5.767202 6.104798\nPetal.Length Petal.Length                 4 4.106330 4.413670\n\n\nInterpretation: The CI for Sepal.Length (5.77 to 6.10) contains the hypothesized mean of 6.0. However, the CI for Petal.Length (4.11 to 4.41) does NOT contain its hypothesized mean of 4.0. Therefore, we conclude the overall mean vector is different because the mean Petal Length is significantly greater than 4.0.\n\n\n\n\nFor the setosa species, construct simultaneous 95% confidence intervals for two linear combinations (m=2):\n\n\nDifference: \\mu_{Sepal.Length} - \\mu_{Sepal.Width}\nSum: \\mu_{Sepal.Length} + \\mu_{Sepal.Width}\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nR Code: CI Calculation\nlibrary(dplyr)\n\ndata &lt;- iris %&gt;%\n  filter(Species == \"setosa\") %&gt;%\n  dplyr::select(Sepal.Length, Sepal.Width)\n\nn &lt;- nrow(data)\np &lt;- ncol(data)\nalpha &lt;- 0.05\nsample_mean &lt;- colMeans(data)\nS &lt;- cov(data)\n\n# Define the linear combinations (m=2)\n# c1 will be for the Difference, c2 for the Sum\nc1 &lt;- c(1, -1)\nc2 &lt;- c(1, 1)\n# Combine into a list for easy iteration\ncombinations &lt;- list(Difference = c1, Sum = c2)\nm &lt;- length(combinations)\n\n# Calculate intervals for each method\nresults &lt;- list()\nfor (i in 1:m) {\n  c_k &lt;- combinations[[i]]\n  combo_name &lt;- names(combinations)[i]\n  \n  point_estimate &lt;- t(c_k) %*% sample_mean\n  std_error &lt;- sqrt((t(c_k) %*% S %*% c_k) / n)\n  \n  # T-squared Method\n  T2_crit &lt;- sqrt(((p-1) * (n - 1) / (n - p+1)) * qf(1 - alpha, p-1, n - p+1))\n  T2_margin &lt;- T2_crit * std_error\n  \n  # Bonferroni Method (adjust alpha by m)\n  Bonf_crit &lt;- qt(1 - alpha / (2 * m), df = n - 1)\n  Bonf_margin &lt;- Bonf_crit * std_error\n\n  results[[i]] &lt;- data.frame(\n    Combination = combo_name,\n    T2_Lower = point_estimate - T2_margin,\n    T2_Upper = point_estimate + T2_margin,\n    Bonf_Lower = point_estimate - Bonf_margin,\n    Bonf_Upper = point_estimate + Bonf_margin\n  )\n}\n\n# Combine and print the results\nfinal_table &lt;- do.call(rbind, results)\nprint(final_table)\n\n\n  Combination T2_Lower T2_Upper Bonf_Lower Bonf_Upper\n1  Difference 1.503074 1.652926   1.491785   1.664215\n2         Sum 8.239918 8.628082   8.210674   8.657326\n\n\nInterpretation: As shown in the table, the Bonferroni confidence intervals for both the “Difference” and the “Sum” are narrower than their T² counterparts, offering a more precise estimate for these specific comparisons.\n\n\n\n\n\n4.8.2 Exercise 2: Dogs Anesthetics Study, Cont’d\nIn the Dogs Anesthetics study, we have tested the main effects and the interaction effect with the contrast matrix to test that all the three differences (the Halothane effect, the CO2 effect, and the interaction effect) in the mean responses have mean zero. This is equivalent to testing the null hypothesis that all four anesthetics induce the same mean times between heartbeats. In fact, this problem can be formulated by comparing any possible pairs of mean differences with zero.\n\n# load data\ndogdat &lt;- read.table(\"dogs.dat\",\n            col.names=c(\"dog\", \"HighCO2\", \"LowCO2\", \n                        \"HighCO2H\", \"LowCO2H\")) %&gt;%\n  mutate(across(c(\"HighCO2\", \"LowCO2\", \n         \"HighCO2H\", \"LowCO2H\"), as.numeric))\nhead(dogdat)\n\n\n  \n\n\n\n\nIn Section 4.7, we have used contrasts to test the hypothesis. Please justify why the hypothesis formulated via contrast is equivalent to test if all four anesthetics treatments induce the same mean times between heartbeats.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\nWe will use the same notation as defined in Section 4.7: \\boldsymbol{\\mu}=(\\mu_1, \\mu_2, \\mu_3, \\mu_4)^\\top represents the mean times between heartbeats under the four treatments: high CO2 pressure + no Halothane, low CO2 pressure + no Halothane, high CO2 pressure + Halothane, low CO2 pressure + Halothane.\nFrom the null hypothesis in Section 4.7, we have \n\\begin{aligned}\n\\mu_3 + \\mu_4 &= \\mu_1 + \\mu_2, \\\\\n\\mu_1 + \\mu_3 &= \\mu_2 + \\mu_4,\\\\\n\\mu_1 - \\mu_2 &=\\mu_3 - \\mu_4.\n\\end{aligned}\n Thus, after some algebra, we obtain that the above is true if and only if \\mu_1=\\mu_2=\\mu_3=\\mu_4.\nMoreover, this constraint gives three independent/unique differences as other differences are just linear combinations of these three independent differences.\nSo, the null hypothesis in Section 4.7 is also equivalent to test if all the tree independent/unique differences are zero.\n\n\n\n\nWrite down the null hypothesis (H_0) and the alternative hypothesis (H_1) using population means only.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\nNull Hypothesis (H_0): The true mean times between heartbeats for each anesthetic is the same: \nH_0: \\mu_1=\\mu_2=\\mu_3=\\mu_4\n\nAlternative Hypothesis (H_1): At least one mean is not the same as one of the other means.\n\n\n\n\n\nUse box plots to compare the distributions of the values for the differences in mean times between heartbeats.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\ndat = dogdat\ndat$diff1 &lt;- dat$HighCO2-dat$LowCO2 \ndat$diff2 &lt;- dat$HighCO2H-dat$LowCO2\ndat$diff3 &lt;- dat$LowCO2H-dat$LowCO2\nhead(dat)\n\n\n\n  \n\n\n\n\n\nCode\nboxplot(dat[, 6:8])\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Assumptions.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\npar(mfrow=c(2,2),pch=1)     \nfor (i in 6:8){\n  qqnorm(dat[,i],  main=\"Normal Q-Q Plot\") \n  qqline(dat[,i], col=\"red\", lwd=2)\n}\npar(c(1,1))\n\n\nNULL\n\n\nCode\napply(dat[,6:8],2,shapiro.test)\n\n\n$diff1\n\n    Shapiro-Wilk normality test\n\ndata:  newX[, i]\nW = 0.8493, p-value = 0.006534\n\n\n$diff2\n\n    Shapiro-Wilk normality test\n\ndata:  newX[, i]\nW = 0.90513, p-value = 0.06029\n\n\n$diff3\n\n    Shapiro-Wilk normality test\n\ndata:  newX[, i]\nW = 0.8558, p-value = 0.008349\n\n\nCode\nmvShapiroTest::mvShapiro.Test(as.matrix(dat[,6:8]))\n\n\n\n    Generalized Shapiro-Wilk test for Multivariate Normality by\n    Villasenor-Alva and Gonzalez-Estrada\n\ndata:  as.matrix(dat[, 6:8])\nMVW = 0.88078, p-value = 0.001033\n\n\n\n\n\n\n\n\n\n\nBecause the p-value is quite small (0.001033) the Shapiro-Wilk test for multivariate normality rejects the null hypothesis that the joint distribution of the three differences is multivariate normal distribution.\nThe Shapiro-Wilk tests for univariate normality indicate that the distributions of the first and third sets of differences are not normal distributions. The normal Q-Q plots indicate that those distributions are skewed to the left.\n\n\n\n\n\nPerform the Hotelling T^2 test for the null hypothesis.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\nDescTools::HotellingsT2Test(dat[,6:8], mu=c(0,0,0))\n\n\n\n    Hotelling's one sample T2-test\n\ndata:  dat[, 6:8]\nT.2 = 34.375, df1 = 3, df2 = 16, p-value = 3.318e-07\nalternative hypothesis: true location is not equal to c(0,0,0)\n\n\n\n\n\n\nTo examine which mean is significant, we can investigate the simultaneous 95% confidence intervals for all six possible pairs of population mean differences: \\mu_1-\\mu_2, \\mu_1-\\mu_3,\\mu_1-\\mu_4, \\mu_2-\\mu_3, \\mu_2-\\mu_4, \\mu_3-\\mu_4.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\nTB.conf.int &lt;- function(X, level = 0.95)\n{ \n  # Convert X to a matrix, if it is not a matrix already, from\n  # vectors or data frames.\n  X &lt;- as.matrix(X)\n  \n  # Set n to the number of observations, p to the number of variables.\n  n &lt;- nrow(X)\n  p &lt;- ncol(X)\n  \n  # Stop if arguments are invalid.\n  if (!is.numeric(X))\n  {\n    stop(\"Data must be numeric\")\n  }\n  \n  if (n &lt; p)\n  {\n    stop(\"Must have at least as many observations as variables\")\n  }\n  \n  if (!is.numeric(level) || length(level) != 1 || level &lt;= 0 || level &gt;= 1)\n  {\n    stop(\"Confidence level must be between 0 and 1\")\n  }\n  \n  # Create a matrix A in which each column represents\n  # a difference between two pairs of means\n  np &lt;- p * (p - 1) / 2\n  A &lt;- matrix(c(0), ncol = np, nrow = p)\n  nc &lt;- 0\n  for (i in 1:(p - 1)) {\n    for (j in 1:(p - i)) {\n      A[i, nc + j] &lt;- 1\n      A[i + j, nc + j] &lt;- -1\n    }\n    nc &lt;- nc + (p - i)\n  }\n  \n  # Create a matrix that will hold the confidence intervals.\n  CI &lt;- matrix(NA, 2, ncol(A))\n  rownames(CI) &lt;- c(\"lower\", \"upper\")\n  colnames(CI) &lt;- colnames(A)\n  \n  CIB &lt;- matrix(NA, 2, ncol(A))\n  rownames(CIB) &lt;- c(\"lower\", \"upper\")\n  colnames(CIB) &lt;- colnames(A)\n  \n  # Find F distribution quantile for T-squared confidence intervals.\n  F &lt;- qf(level, p, n - p)\n  \n  # Find t distribution percentile for Bonferroni confidence intervals\n  alpha &lt;- (1 - level) / 2 / ncol(A)\n  levelB &lt;- 1 - alpha\n  tB &lt;- qt(levelB, n - 1)\n  t &lt;- qt(1 - (1 - level) / 2, n - 1)\n  \n  # Compute the sample covariance matrix of the original variables.\n  C &lt;- cov(X)\n  \n  # Find the confidence intervals for the specified linear combinations.\n  for (i in 1:ncol(A))\n  { \n    # Find the sample mean and variance of this linear combination.\n    m &lt;- mean(X %*% A[, i])\n    v &lt;- t(A[, i]) %*% C %*% A[, i]\n    \n    # Find the confidence interval for this difference.\n    CI[1, i] &lt;- m - sqrt((p * (n - 1) / n / (n - p)) * F * v)\n    CI[2, i] &lt;- m + sqrt((p * (n - 1) / n / (n - p)) * F * v)\n    \n    CIB[1, i] &lt;- m - tB * sqrt(v / n)\n    CIB[2, i] &lt;- m + tB * sqrt(v / n)\n    \n  }\n  \n  # Print the confidence intervals.\n  cat(\" T-squared CIs: \\n\\n\")\n  print(CI)\n  \n  cat(\"\\n\\n Bonferroni CIs: \\n\\n\")\n  print(CIB)\n}\n\nTB.conf.int(dat[, 2:5])\n\n\n T-squared CIs: \n\n           [,1]      [,2]       [,3]       [,4]       [,5]      [,6]\nlower -89.46963 -165.0961 -183.66206 -131.61501 -158.56850 -69.54636\nupper  16.62753  -57.0092  -85.70637  -17.64815  -37.95781  22.28320\n\n\n Bonferroni CIs: \n\n            [,1]       [,2]       [,3]       [,4]       [,5]      [,6]\nlower -77.460347 -152.86156 -172.57431 -118.71494 -144.91641 -59.15204\nupper   4.618242  -69.24371  -96.79411  -30.54822  -51.60991  11.88889\n\n\nInterpretation\n\nBoth the T^2 CIs and Bonferroni CIs yields the same conclusions as whenever one interval contains zero, the other also contains zero; and whenever one interval excludes zero, the other also excludes zero.\nThe CIs for \\mu_1-\\mu_2 and \\mu_3-\\mu_4 contains zeros, indicating\n\nthat there is no significant difference in mean times between heartbeats for using high or low CO2 pressure regardless of whether halothane is used or not;\nand that there appears to be no significant effect of high or low pressure of CO2 on mean heart rates, after controlling for the presence or absence of halothane.\n\n\nAll of the other confidence intervals do not contain zero indicating that mean heart rates tend to be slower (mean time between heartbeats is longer) when halothane is used than when halothane is not used, regardless of the CO2 pressure level.\nBoth sets of intervals provide at least 95% simultaneous coverage of the six differences in population means for difference between heartbeats, but the Bonferroni intervals are shorter than the T-squared intervals.\n\n\n\n\n\n\n4.8.3 Exercise 3: Baseball Player Data\nBackground\nA sports science journal from the 1950s established a “classic” physical standard for professional baseball players, claiming the ideal physique had a mean height of 72.5 inches and a mean weight of 209 pounds. The data is from the Lahman::People dataset.\nYour task is to determine if the average physique of modern players has significantly changed from this historical benchmark using a Hotelling’s T² test with a significance level of \\alpha = 0.05. In the analysis, we assume that the random vector follows a multivariate distribution and carry out the analysis, although it actually fails the multivariate Shapiro-Wilk test.\n1. State the Hypotheses\nWrite the null hypothesis (H_0) and the alternative hypothesis (H_1) for this test using proper mathematical notation. Let \\boldsymbol{\\mu} represent the true mean vector of [height, weight] for modern players.\n\n\n\n\n\n\nView Solution\n\n\n\n\n\nThe hypotheses are formulated as:\n\nNull Hypothesis (H_0): The true mean vector of modern players is equal to the historical standard. H_0: \\boldsymbol{\\mu} = \\begin{bmatrix} 72.5 \\\\ 209 \\end{bmatrix}\nAlternative Hypothesis (H_1): The true mean vector of modern players is not equal to the historical standard. H_1: \\boldsymbol{\\mu} \\neq \\begin{bmatrix} 72.5 \\\\ 209 \\end{bmatrix}\n\n\n\n\n2. Prepare the Data\nLoad the necessary R packages. From the Lahman::People dataset, create a final data frame that contains only the height and weight columns for players who debuted in the year 2010 or later, with any missing values removed.\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\nlibrary(Lahman)\nlibrary(dplyr)\nlibrary(ggplot2)\n# Create the final data frame for analysis\nplayer_data &lt;- Lahman::People %&gt;%\n  filter(debut&gt;2010) %&gt;%\n  filter(!is.na(height) & !is.na(weight)) %&gt;%\n  dplyr::select(height, weight)%&gt;%\n  mutate(across(c(height, weight), as.numeric))\n\ndf = player_data\nn = nrow(df)\np = ncol(df)\nhead(df)\n\n\n\n  \n\n\n\nCode\nGGally::ggpairs(df)\n\n\n\n\n\n\n\n\n\n\n\n\n3. Check Key Assumptions\nCreate and examine histograms for both the height and weight distributions in your sample. Do they appear approximately normal?\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nR Code: Normality Check\n## Q-Q plot\npar(mfrow = c(1, 3))\n\n# Loop through the column names and create a plot for each\nfor (col_name in colnames(df)) {\n  qqnorm(df[[col_name]], main = col_name)\n  qqline(df[[col_name]], col = \"red\", lwd = 2)\n}\n\n## Compute Shapiro-Wilk statistic to test normality for each variable\nsapply(colnames(df), function(x) {\n               shapiro.test(df[[x]]) } )\n\n\n          height                        weight                       \nstatistic 0.9834675                     0.9929459                    \np.value   1.309451e-19                  6.188722e-12                 \nmethod    \"Shapiro-Wilk normality test\" \"Shapiro-Wilk normality test\"\ndata.name \"df[[x]]\"                     \"df[[x]]\"                    \n\n\nR Code: Normality Check\n## Perform multivariate normality test\nmvShapiroTest::mvShapiro.Test(as.matrix(df))\n\n\n\n    Generalized Shapiro-Wilk test for Multivariate Normality by\n    Villasenor-Alva and Gonzalez-Estrada\n\ndata:  as.matrix(df)\nMVW = 0.99687, p-value = 1.045e-10\n\n\n\n\n\n\n\n\n\nInterpretation: Both distributions are mound-shaped and reasonably symmetric, supporting the assumption of approximate normality.\n\n\n\n4. Perform the Statistical Test\nRun a one-sample Hotelling’s T^2 test on your prepared data using the historical standard as your hypothesized mean vector.\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nR Code: Hotelling’s T^2 Test\n# Define the hypothesized mean vector from the 1950s standard\nmu0 &lt;- c(72.5, 209.0)\n\n# Perform the test\ntest_result &lt;- DescTools::HotellingsT2Test(x = df, mu = mu0)\n\n# Print the results\nprint(test_result)\n\n\n\n    Hotelling's one sample T2-test\n\ndata:  df\nT.2 = 589.64, df1 = 2, df2 = 3432, p-value &lt; 2.2e-16\nalternative hypothesis: true location is not equal to c(72.5,209)\n\n\n\n\n\n5. Interpret the Results\nWhat is the p-value from your test? Based on this and \\alpha = 0.05, do you reject or fail to reject the null hypothesis? State your conclusion in the context of the problem.\n\n\n\n\n\n\nView Solution\n\n\n\n\n\nThe p-value is exceptionally small and far below the significance level of \\alpha = 0.05.\nConclusion: We strongly reject the null hypothesis. There is overwhelming statistical evidence to conclude that the average physique (the mean vector of height and weight) of modern baseball players is significantly different from the historical standard of 72 inches and 190 pounds.\n\n\n\n6. Draw Conclusions Based on Confidence Region\nCheck if the hypothesized mean is in the 95% confidence ellipse. Explain how it supports your conclusion.\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nR Code: Visualization\n# Calculate the mean of our modern player sample\nxbar &lt;- colMeans(df)\nS = cov(df)\nT2 = n*t(xbar - mu0) %*%solve(S)%*%(xbar - mu0)\nscaledT2 = (n-p)/(p*(n-1)) * T2 \n# Calculate points for the 95% confidence ellipse\nalpha=0.05\nFvalue &lt;- qf(1 - alpha, p, n - p)\n\ncat(\"scaled T2 statistic is\",scaledT2, \", F critical value is\", Fvalue)\n\n\nscaled T2 statistic is 589.6427 , F critical value is 2.998349\n\n\n\n\nCode\n# this is the covariance of xbar\nSigma_ell &lt;- S / n\neig &lt;- eigen(Sigma_ell)\nA &lt;- eig$vectors %*% diag(sqrt(eig$values)) *\nsqrt((n - 1) * p / (n - p) * Fvalue)\n\ntheta &lt;- seq(0, 2 * pi, length.out = 400)\npts &lt;- t(matrix(xbar, nrow = 2, ncol = length(theta)) +\nA %*% rbind(cos(theta), sin(theta)))\n\ndf_ell &lt;- as.data.frame(pts)\ncolnames(df_ell) &lt;- names(df)\ncenter &lt;- data.frame(height = xbar[1], weight = xbar[2])\n\ngg &lt;- ggplot() +\ngeom_path(data = df_ell, aes(height, weight)) +\ngeom_point(\ndata = center,\naes(height, weight),\ncolor = \"red\",\nsize = 3\n) +\ntheme_minimal() +\nlabs(x = \"Height Difference\", y = \"Weight Difference\")\n\nprint(gg)\n\n\n\n\n\n\n\n\n\nInterpretation: This result confirms the test result. The historical standard is far outside the confidence ellipse (the 95% confidence region for the true mean of modern players), confirming that modern players are, on average, both taller and heavier. At this point we do not know which one contributes to the difference, which can be answered using simultaneous confidence intervals.\n\n\nSimultaneous T2 CIs\ncval_T2 = sqrt((n-1)*p/(n-p) * Fvalue) \nse_ii = sqrt(diag(S)/n)\n\nci_T2 = tibble(\nComponent = names(df),\nEstimate = xbar,\nLower = Estimate - cval_T2 * se_ii,\nUpper = Estimate + cval_T2 * se_ii\n)\nprint(ci_T2)\n\n\n# A tibble: 2 × 4\n  Component Estimate Lower Upper\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 height        73.7  73.6  73.8\n2 weight       209.  208.  210. \n\n\n\n\nSimultaneous Bonferroni CIs\nm = 2\ncval_Bon = qt(1-0.05/(2*m), n-1)\nci_Bon = tibble(\nComponent = names(df),\nEstimate = xbar,\nLower = Estimate - cval_Bon * se_ii,\nUpper = Estimate + cval_Bon * se_ii\n)\nprint(ci_Bon)\n\n\n# A tibble: 2 × 4\n  Component Estimate Lower Upper\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 height        73.7  73.6  73.8\n2 weight       209.  208.  210. \n\n\nBased on simultaneous CIs (T2 or Bonferroni), the results indicates that there is a significantly individual difference in height and weight at \\alpha=0.05.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Inference for Single Mean Vector</span>"
    ]
  },
  {
    "objectID": "ch5/05-Inference_for_Means_II.html",
    "href": "ch5/05-Inference_for_Means_II.html",
    "title": "5  Inference for Multiple Mean Vectors",
    "section": "",
    "text": "5.1 Two-Sample Comparison\nData sources and measurements",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inference for Multiple Mean Vectors</span>"
    ]
  },
  {
    "objectID": "ch5/05-Inference_for_Means_II.html#two-sample-comparison",
    "href": "ch5/05-Inference_for_Means_II.html#two-sample-comparison",
    "title": "5  Inference for Multiple Mean Vectors",
    "section": "",
    "text": "Where the data come from\n\nTwo independent samples: Select n_1 units from population 1 and n_2 units from population 2 (samples are independent; n_1 and n_2 may differ).\nRandomized experiment: Randomly assign n_1 units to treatment 1 and n_2 units to treatment 2 (sample sizes need not be equal).\n\nWhat we measure\nFor each unit, record the same set of p variables (traits), forming a p-dimensional measurement vector.\n\n\n\n\n\n\n\nKey Assumptions\n\n\n\nThe following assumptions are needed to make inferences about the difference between two population mean vectors \\boldsymbol \\mu_1 - \\boldsymbol \\mu_2:\n\n\\mathbf{x}_{1j} \\sim N_p(\\boldsymbol{\\mu}_1, \\Sigma_1) independently for j=1,\\dots,n_1; and \\mathbf{x}_{2k} \\sim N_p(\\boldsymbol{\\mu}_2, \\Sigma_2) independently for k=1,\\dots,n_2.\n\n\\Sigma_1=\\Sigma_2=\\Sigma. (Homogeneity of Covariance)\n\\mathbf{x}_{1j}’s are independent of \\mathbf{x}_{2j}’s.\n\n\n\n\n5.1.1 Example: Two Soap Manufacturing Processes\nBackground: A consumer goods company is developing a new method for producing soap. They want to determine whether the new process (Process 2) improves product quality compared to the current standard method (Process 1). Two key performance outcomes are of interest:\n\nLather quality (x_1):\n\nA measure of how much and how long-lasting the foam is when the soap is used.\nMeasured on a continuous scale by laboratory technicians using a standardized test.\n\nMildness (x_2):\n\nA subjective measure of how gentle the soap is on skin, evaluated by a panel of trained users.\nAlso measured on a continuous scale (e.g., skin irritation score, lower is better).\n\n\n\n\n\n\n\n\nExperimental Setup\n\n\n\n\nDesign: A randomized controlled experiment.\nSample sizes:\n\nn_1 = 50 soaps produced using the current process (Process 1)\nn_2 = 50 soaps produced using the new process (Process 2)\n\nMeasurements:\n\nEach bar of soap is tested for both lather and mildness independently.\n\n\n\nn1 = 50\nn2 = 50\np = 2\nxbar1 = c(8.3, 4.1)\nS1 = matrix(c(2,1,1,6),byrow=TRUE,ncol=2)\nxbar2 = c(10.2, 3.9)\nS2 = matrix(c(2,1,1,4), byrow=TRUE, ncol=2)\n\n\n\n\n\n\n\n\n\nResearch Questions\n\n\n\n\nIs there evidence that the new process produces soaps with different overall quality, as measured by both lather and mildness?\nIf a difference exists, which outcome (lather or mildness) contributes more to that difference?\n\n\n\n\n\n5.1.2 State the Hypotheses\nLet \\boldsymbol \\mu_1 and \\boldsymbol \\mu_2 be the population mean vectors for process 1 and process 2, respectively: \\begin{align*}\n\\boldsymbol \\mu_1 = [\\text{Mean Leather}_1, \\text{Mean Mildness}_1]^\\top,\\\\\n\\boldsymbol \\mu_2 = [\\text{Mean Leather}_2, \\text{Mean Mildness}_2]^\\top.\n\\end{align*}\n\n\n\n\n\n\nHypotheses\n\n\n\n\n\n\nH_0: \\boldsymbol \\mu_1 = \\boldsymbol \\mu_2 \\quad \\text{v.s.}\\quad H_1: \\boldsymbol \\mu_1 \\neq \\boldsymbol \\mu_2\n\n\n\n\n\n\n5.1.3 Pool Covariance\n\n\n\n\n\n\nPool Covariance\n\n\n\n\nPoint estimate of \\boldsymbol \\mu_1 - \\boldsymbol \\mu_2 is \\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2.\nThe population covariance matrix of \\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2 is \n\\text{Cov}(\\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2) = \\text{Cov}(\\bar{\\mathbf{x}}_1) + \\text{Cov}(\\bar{\\mathbf{x}}_2) = \\frac{1}{n_1}\\Sigma + \\frac{1}{n_2}\\Sigma,\n\nThe pooled estimate of the population covariance matrix is \nS_{\\text{pool}} = \\frac{(n_1 - 1)}{(n_1 + n_2 - 2)} S_1 + \\frac{(n_2 - 1)}{(n_1 + n_2 - 2)} S_2\n\n\n\n\n\n\nR Code: Pooled Covariance\nDelta = xbar1 - xbar2 \nSp = (n1-1)/(n1+n2-2)*S1 + (n2-1)/(n1+n2-2)*S2\nprint(Sp)\n\n\n     [,1] [,2]\n[1,]    2    1\n[2,]    1    5\n\n\n\n\n5.1.4 Hotelling’s T^2 Statistic\n\n\n\n\n\n\nHotelling’s T^2 Statistic\n\n\n\n\n\nThe test statistic is the Hotelling’s T^2 statistic: \nT^2 = (\\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2 )^\\top\\left[\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right) S_{\\text{pool}}\\right]^{-1}(\\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2)\n\n\n\n\n\n\nR Code: T2 Statistic\nT2 = drop(t(Delta) %*% solve((1/n1+1/n2) * Sp) %*% Delta)\nprint(T2)\n\n\n[1] 52.47222\n\n\n\n\n5.1.5 Decision\n\n\n\n\n\n\nDecision\n\n\n\n\n\nWe reject H_0: \\boldsymbol \\mu_1 - \\boldsymbol \\mu_2 = 0 at level \\alpha using one of the following two approaches:\n\nCritical Region: T^2 &gt; c^2, where \nc^2:= \\frac{(n_1 + n_2 -2)p}{(n_1 + n_2 - p - 1)}F_{(p, n_1 + n_2 -p -1), 1-\\alpha}.\n\nP-value: The p-value is less than \\alpha.\n\n\n\nCode\nalpha =0.05\nc2 = (n1+n2-2)*p / (n1+n2-p-1) * qf(1-alpha, p, n1+n2-p-1)\np_val &lt;- 1 - pf((T2 * (n1 + n2 - p - 1)) / \n                  (p * (n1 + n2 - 2)), p, n1 + n2 - p - 1)\ncat(\"critical value:\", c2, \" with \", \"p-value:\", p_val, \"\\n\")\n\n\ncritical value: 6.244089  with  p-value: 9.286081e-10 \n\n\nInterpretation: Since T^2&gt;c^2, we reject H_0 at \\alpha=0.05 and conclude that the population mean measures on lather and mildness are statistically different, but at this point, we do not know which variable contributes to the difference.\n\n\n\n\n\n5.1.6 Confidence Region\n\n\n\n\n\n\nConfidence Region\n\n\n\n\n\nA 100(1-\\alpha)\\% confidence region for \\boldsymbol \\mu_1 - \\boldsymbol \\mu_2 is given by all values of \\boldsymbol \\mu_1 - \\boldsymbol \\mu_2 that satisfy \n(\\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2 - (\\boldsymbol \\mu_1-\\boldsymbol \\mu_2))'\\left[\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right) S_{\\text{pool}}\\right]^{-1}(\\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2 - (\\boldsymbol \\mu_1-\\boldsymbol \\mu_2)) \\leq c^2.\n where c^2 is defined above.\n\n\n\n\nEigenvalues and eigenvectors of the pooled covariance matrix are\n\n\n\nR Code: Eigenvalues and Eigenvectors\neig_result = eigen(Sp)\nlambda1 = eig_result$values[1]\nnames(eig_result$values) = c(\"lambda1\", \"lambda2\")\n# Eigen values are \nprint(eig_result$values) \n\n\n lambda1  lambda2 \n5.302776 1.697224 \n\n\nR Code: Eigenvalues and Eigenvectors\n# Eigenvectors are \ncolnames(eig_result$vectors) = c(\"eigenvector 1\", \"eigenvector 2\")\nprint(eig_result$vectors)\n\n\n     eigenvector 1 eigenvector 2\n[1,]     0.2897841    -0.9570920\n[2,]     0.9570920     0.2897841\n\n\n\n\nR Code: Axes Lengths\n# semi-major axis and  semi-minor axis: \naxis=c(sqrt(eig_result$values[1]) * sqrt((1/n1+1/n2)*c2),\n       sqrt(eig_result$values[2]) * sqrt((1/n1+1/n2)*c2))\nnames(axis) = c(\"axis 1\", \"axis 2\")\nprint(axis)\n\n\n   axis 1    axis 2 \n1.1508432 0.6510797 \n\n\n\nThe 95% confidence ellipse for the difference between two population mean vectors\n\nis centered at \\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2\nextends \\sqrt{\\lambda_1} \\sqrt{(1/n_1+1/n_2)c^2} = 1.15 and \\sqrt{\\lambda_2} \\sqrt{(1/n_1+1/n_2)c^2} = 0.65 units in the first eigenvector and second eigenvector directions\n\n\n\n\n\n\n\n95% Confidence Ellipse for Mean Difference\n\n\n\n\n\nInterpretation: Because the origin \\mathbf{0} is not inside the ellipse, we conclude that the populations of soaps produced by the two processes are centered at different mean vectors. There appears to be no big difference in mildness means for soaps made by the two processes, but soaps made with the second process produce more lather on average.\n\n\n\n5.1.7 Simultaneous CIs\n\n\n\n\n\n\nSimultaneous CIs\n\n\n\nAs in one population case, we can obtain simultaneous confidence intervals for any linear combination of the components of \\boldsymbol \\mu_1 - \\boldsymbol \\mu_2.\n\nSuppose we are interested in a set of p simultaneous confidence intervals: \n\\mathbf{a}'_j(\\boldsymbol \\mu_1 - \\boldsymbol \\mu_2) =\n\\begin{bmatrix}\n0 & 0 & \\cdots & 1 & \\cdots & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mu_{11} - \\mu_{21} \\\\\n\\mu_{12} - \\mu_{22} \\\\\n\\vdots \\\\\n\\mu_{1p} - \\mu_{2p}\n\\end{bmatrix} = \\mu_{1j} - \\mu_{2j}\n where the vector \\mathbf{a}_j has zeros everywhere except for the one in the jth position.\nTypically, we would be interested in m such comparisons.\n\nSimultaneous T^2 CIs for \\mu_{1j} - \\mu_{2j}:\n\n(\\bar{x}_{1j} - \\bar{x}_{2j}) \\pm\n\\sqrt{\n\\frac{(n_1 + n_2 - 2)p}{n_1 + n_2 - p - 1} \\cdot F_{p,\\; n_1 + n_2 - p - 1;\\; 1 - \\alpha}\n}\n\\cdot\n\\sqrt{\n\\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right) \\cdot S_{\\text{pool},\\, jj}\n}\n * This interval will simultaneously cover the true values of \\mu_{1j} - \\mu_{2j} with confidence at least (1-\\alpha)\\times 100\\%.\nSimultaneous Bonferroni CIs for \\mu_{1j} - \\mu_{2j}: \n(\\bar{x}_{1j} - \\bar{x}_{2j}) \\pm t_{(n_1 + n_2 -2), 1-\\alpha/(2m)} \\sqrt{\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right) S_{\\text{pool}, jj}}\n\n\n\n\n\nR Code: T2 CIs\nse_j = sqrt((1/n1+1/n2)* diag(Sp))\n# T2 CIs\ncval_T2 = sqrt(c2)\n# Bonferroni CIs\nm=2 # only two variables\ncval_Bon = qt(1-alpha/(2*m), n1+n2-2) \n\nci_T2 &lt;- tibble(\n  Component = c(\"Lather\", \"Mildness\"),\n  Estimate = Delta,\n  HalfWidth = cval_T2 * se_j,\n  Lower = Estimate - HalfWidth,\n  Upper = Estimate + HalfWidth\n)\nprint(ci_T2)\n\n\n# A tibble: 2 × 5\n  Component Estimate HalfWidth  Lower Upper\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Lather      -1.90      0.707 -2.61  -1.19\n2 Mildness     0.200     1.12  -0.918  1.32\n\n\n\n\nR Code: Bonferroni CIs\nci_Bon &lt;- tibble(\n  Component = c(\"Lather\", \"Mildness\"),\n  Estimate = Delta,\n  HalfWidth = cval_Bon * se_j,\n  Lower = Estimate - HalfWidth,\n  Upper = Estimate + HalfWidth\n)\n\nprint(ci_Bon)\n\n\n# A tibble: 2 × 5\n  Component Estimate HalfWidth  Lower Upper\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Lather      -1.90      0.644 -2.54  -1.26\n2 Mildness     0.200     1.02  -0.818  1.22\n\n\n\n\n\n\n\n95% Confidence Ellipse with T^2 and Bonferroni CIs. Blue dashed line indicates the limits of T^2 CIs and darkgree dotted line indicates the limits of Bonferroni CIs.\n\n\n\n\nInterpretation: Based on simultaneous T2 CIs and Bonferroni CIs, the results confirm previous finding based on confidence ellipse that there appears to be no big difference in mildness means for soaps made by the two processes, but soaps made with the second process produce more lather on average. We also note that T2 CIs are wider than Bonferrorni CIs.\n\n\n\n\n\n\nOne-At-a-Time CI\n\n\n\n\n\nUsing the univariate approach, we can construct t-intervals for each of mean differences, and obtain the so-called one-at-a-time t intervals \n(\\bar{x}_{1j} - \\bar{x}_{2j}) \\pm t_{(n_1 + n_2 -2), 1-\\alpha/2} \\sqrt{\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right) S_{\\text{pool}, jj}}.\n The one-at-a-time CI does not control the family-wise error at \\alpha=0.05. As shown previously, the key difference between the Bonferroni CI and the one-at-a-time CI is that Bonferroni CI controls the family-wise error by assigning \\alpha/m level to each interval when there are m comparisons.\n\n\n\n\n\n5.1.8 Exercise: Steel Tube Data\n\n\n\n\n\n\nSteel Tube Data\n\n\n\nBackground: In steel manufacturing, the rolling temperature — the temperature at which steel is shaped into tubes — can affect the material’s strength profile. To study this, engineers measured the breaking strength (yield point (ksi)) and ultimate strength (ksi) of steel tubes produced at two different rolling temperatures, where 5 samples of steel are tested with low rolling temperature independently and 7 samples of steel are tested with high rolling temperature. The objective is to determine whether changing the rolling temperature results in a change in the strength profile (i.e., yield point and ultimate strength). Based on the following data, please solve the following problems:\n\n\nCode\nsteel = readr::read_csv(file = \"steel.csv\", show_col_types = FALSE) %&gt;% \n  mutate(temp = as.factor(temp))\n\nn1 &lt;- dim(steel[steel$temp==1, -1])[1]\nn2 &lt;- dim(steel[steel$temp==2, -1])[1]\np &lt;- dim(steel[steel$temp==2, -1])[2]\nhead(steel)\n\n\n\n  \n\n\n\n\n\n\nStep 1. Data visualization\n\n\n\nR Code: Long Format via pivot_longer\nlibrary(dplyr)\ndf_long = steel %&gt;% \n  pivot_longer(cols=c(2:3), \n               names_to=\"var\")\nhead(df_long)\n\n\n\n  \n\n\n\n\n\nR Code: One variable + multiple groups\nggplot(df_long, aes(x=temp, y=value)) + \n  geom_point() + \n  facet_wrap(~var, nrow=1, \n             scales=\"free_y\")\n\n\n\n\n\n\n\n\n\n\n\nR Code: One group + multiple variables\nggplot(steel) + \n  geom_point(aes(x=yield, y=strength, col=temp)) \n\n\n\n\n\n\n\n\n\nQuestion: What do the plots tell you?\n\nStep 2. State the research question(s) and define the null and alternative hypotheses using standard notations.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\nThe research questions for this data can be stated as follows:\n\nDoes the change in rolling temperature result in the change in strength profile of the steel - that is, does it affect either or both of yield point and ultimate strength?\nIf it does affect, which variable is more sensitive to the change in rolling temperature?\n\nBased on the research questions, our objectives are to examine the data and test the null hypothesis that the vectors of means for yield point and ultimate strength are the same for the two rolling temperatures used to produce this type of steel.\n\nLet x_1 denote the yield point and x_2 denote the ultimate strength.\nLet \\mathbf{x}_{1j}: =[x_{1j}, x_{2j}]^\\top denote the measured yield point and ultimate strength under low rolling temperature for j=1,\\ldots, n_1; and \\mathbf{x}_{2j}: =[x_{1j}, x_{2j}]^\\top under high rolling temperature for j=1,\\ldots, n_2.\nLet \\boldsymbol \\mu_1, \\boldsymbol \\mu_2 be the population mean vectors of yield point and ultimate strength under low and high rolling temperature respectively: \n\\boldsymbol \\mu_1: =[\n\\text{Mean Yield}_1, \\text{Mean Strength}_1]^\\top, \\quad\n\\boldsymbol \\mu_2: =[\n\\text{Mean Yield}_2, \\text{Mean Strength}_2]^\\top\n or more precisely, \\boldsymbol \\mu_1 = E(\\mathbf{x}_{1j}) and \\boldsymbol \\mu_2 = E(\\mathbf{x}_{2j}).\n\nThe null and alternative hypothesese are given as follows \nH_0: \\boldsymbol \\mu_1 = \\boldsymbol \\mu_2 \\quad \\text{v.s.}\\quad\nH_1: \\boldsymbol \\mu_1 \\neq \\boldsymbol \\mu_2\n\n\n\n\n\nStep 3. Perform the statistical test.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\nStep 3(a): Compute summary statistics\n\n\n\nCode\n#  Compute sample mean vector and sample\n(xbar1 = sapply(steel[steel$temp==1, -1], mean))\n\n\n   yield strength \n    36.4     62.6 \n\n\nCode\n#  covariance matrix for each temperature\n(xvar1 = var(steel[steel$temp==1 , -1]))\n\n\n         yield strength\nyield      7.3      4.2\nstrength   4.2      4.3\n\n\nCode\n#  Compute sample mean vector and sample\n(xbar2 = sapply(steel[steel$temp==2, -1], mean))\n\n\n   yield strength \n39.00000 60.42857 \n\n\nCode\n#  covariance matrix for each temperature\n(xvar2 = var(steel[steel$temp==2 , -1]))\n\n\n            yield strength\nyield    8.333333 6.666667\nstrength 6.666667 7.619048\n\n\n\nStep 3(b): Check Normality Assumption\n\n\n\nCode\n# check univariate normaltiy \nfor(i in 1:p){\n  apply(steel[steel$temp == i, -1], 2, shapiro.test)\n}\n\n# check bivariate normaltiy\nmvShapiroTest::mvShapiro.Test(as.matrix(steel[ , 2:3]))\n\n\n\n    Generalized Shapiro-Wilk test for Multivariate Normality by\n    Villasenor-Alva and Gonzalez-Estrada\n\ndata:  as.matrix(steel[, 2:3])\nMVW = 0.9584, p-value = 0.8718\n\n\n\nStep 3(c): Check Homogeneity Assumption\n\n\n\nCode\n# Apply Box's M-test to test the null hypothesis of homogeneous covariance matrices. \nbiotools::boxM(steel[ , -1], steel$temp)\n\n\n\n    Box's M-test for Homogeneity of Covariance Matrices\n\ndata:  steel[, -1]\nChi-Sq (approx.) = 0.38077, df = 3, p-value = 0.9442\n\n\n\nStep 3(d): Two-Sample Hotelling’s T^2 Test\n\n\n\nCode\nT2result &lt;- DescTools::HotellingsT2Test(steel[steel$temp == 1, -1], \n             steel[steel$temp == 2, -1])\n\nT2result\n\n\n\n    Hotelling's two sample T2-test\n\ndata:  steel[steel$temp == 1, -1] and steel[steel$temp == 2, -1]\nT.2 = 10.76, df1 = 2, df2 = 9, p-value = 0.004106\nalternative hypothesis: true location difference is not equal to c(0,0)\n\n\n\n\n\n\nStep 4. Construct the confidence region and interpret the results.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\n# sample difference\nDelta = xbar1 - xbar2\n\n# compute pooled covariance matrix \nSp &lt;- ((n1-1)*xvar1 +(n2-1)*xvar2)/(n1+n2-2)\n\n# compute T2 statistic\nT2 = drop(t(Delta)%*%solve((1/n1+1/n2) * Sp)%*%Delta)\nprint(T2)\n\n\n[1] 23.91171\n\n\nCode\n# critical value\nalpha = 0.05\nc2 = (n1+n2-2)*p / (n1+n2-p-1) * qf(1-alpha, p, n1+n2-p-1)\n\n# this is the covariance of Delta \nSigma_ell &lt;-   (1/n1 + 1/n2) * Sp\neig &lt;- eigen(Sigma_ell)\nA &lt;- eig$vectors %*% diag(sqrt(eig$values)) * sqrt(c2)\n\ntheta &lt;- seq(0, 2*pi, length.out = 400)\npts &lt;- t(matrix(Delta, nrow = 2, ncol = length(theta)) + \n           A %*% rbind(cos(theta), sin(theta)))\n\ndf_ell &lt;- as.data.frame(pts)\ncolnames(df_ell) &lt;- c(\"Yield\", \"Strength\")\ncenter &lt;- data.frame(Yield = Delta[1], Strength = Delta[2])\n\ngg &lt;- ggplot() +\n  geom_path(data = df_ell, aes(Yield, Strength)) +\n  geom_point(data = center, aes(Yield, Strength), color = \"red\", size = 3) +\n  coord_equal() +\n  theme_minimal() + \n  labs(x=\"Yield Difference\", y=\"Strength Difference\")\n\nprint(gg)\n\n\n\n\n\n95% Confidence region for population mean difference \\boldsymbol \\mu_1 - \\boldsymbol \\mu_2\n\n\n\n\nInterpretation:\nThe 95\\% confidence ellipse for the difference between two population mean vectors\n\nis centered at -2.6, 2.17,\nand extends 6.45 and 2.11 units in the first and second eigenvectors directions.\n\nSince the origin 0 does not fall into the 95\\% confidence ellipse, we conclude that the change in rolling temperature indeed affect the strength profile significantly at \\alpha=0.05, which is consistent with the Hotelling’s T^2 test. However, at this point we do not know whether individual variable would contribute to this change along.\n\n\n\n\nStep 5. Construct simultaneous T^2 and Bonferroni CIs and interpret the results.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\n# confidence level\nlevel &lt;- 0.95\n\nse_j = sqrt(diag(Sp) * (1/n1 + 1/n2))\n\n# Compute degrees of freedom and the multipliers\ndf1 &lt;- p\ndf2 &lt;- n1+n2-p-1\ndf3 &lt;- n1+n2-2\n\ncval_T2 &lt;-  sqrt((n1+n2-2)*p*qf(level,df1,df2)/(n1+n2-p-1))\n\nm = p \nlevel2 &lt;- 1-(1-level)/(2*m)\ncval_Bon  &lt;- qt(level2, df3)\n\nci_T2 &lt;- tibble(\n  Component = c(\"Yield\", \"Strength\"),\n  Estimate = Delta,\n  Lower = Estimate - cval_T2 * se_j,\n  Upper = Estimate + cval_T2 * se_j\n)\ncat(\"T2 CI:\")\n\n\nT2 CI:\n\n\nCode\nprint(ci_T2)\n\n\n# A tibble: 2 × 4\n  Component Estimate Lower Upper\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Yield        -2.6  -7.67  2.47\n2 Strength      2.17 -2.35  6.69\n\n\nCode\nci_Bon &lt;- tibble(\n  Component = c(\"Yield\", \"Strength\"),\n  Estimate = Delta,\n  Lower = Estimate - cval_Bon * se_j,\n  Upper = Estimate + cval_Bon * se_j\n)\n\ncat(\"Bonferroni CI:\")\n\n\nBonferroni CI:\n\n\nCode\nprint(ci_Bon)\n\n\n# A tibble: 2 × 4\n  Component Estimate Lower Upper\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Yield        -2.6  -6.94  1.74\n2 Strength      2.17 -1.70  6.04\n\n\nInterpretation: The simultaneous T^2 CIs and Bonferroni CIs do include the origin for each population mean parameter. While we get two seemingly contradictory results, they are not wrong. The key reason is that the (joint) confidence region obtained above effectively takes into account the correlation between these two variables: yield point and ultimate strength. From the scatter plot between these two variables, we also notice that there is a strong positive correlation under both rolling temperature. Thus, it is possible that (0,0) is outside the 95\\% confidence ellipse but all individual intervals would contain 0, indicating non-significant difference for individual variables (as these variables are highly correlated.) A powerful and correct way for multivariate test is to use the Hotelling’s T^2 test or joint confidence region to detect a difference that the individual simultaneous intervals would miss.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inference for Multiple Mean Vectors</span>"
    ]
  },
  {
    "objectID": "ch5/05-Inference_for_Means_II.html#comparing-multiple-mean-vectors",
    "href": "ch5/05-Inference_for_Means_II.html#comparing-multiple-mean-vectors",
    "title": "5  Inference for Multiple Mean Vectors",
    "section": "5.2 Comparing Multiple Mean Vectors",
    "text": "5.2 Comparing Multiple Mean Vectors\nIn multivariate analysis, we often want to test whether several groups have the same mean vector for multiple variables. This is the multivariate extension of one-way ANOVA: Multivariate Analysis of Variance (MANOVA). We can extend the comparison of mean vectors to g different groups (or treatments) or populations for p responses.\n\n\n\n\n\n\nKey Assumptions\n\n\n\nThe following assumptions are needed to make inferences about the difference between any two population mean vectors: \\boldsymbol \\mu_{\\ell_1} - \\boldsymbol \\mu_{\\ell_2} for \\ell_1\\neq \\ell_2:\n\nEach observation vector sampled from the \\ell-th population (or group) follows a multivariate normal distribution: for \\ell=1,\\ldots g, \n\\mathbf{x}_{\\ell 1}, \\mathbf{x}_{\\ell 2}, \\dots, \\mathbf{x}_{\\ell n_\\ell} \\overset{ind}{\\sim} N_p(\\boldsymbol{\\mu}_\\ell, {\\Sigma}).\n\nCovariance matrices are homogeneous: \\Sigma_\\ell = \\Sigma for every population.\nObservations from one population (or group) is independent of any observations from other populations.\n\n\n\n\n5.2.1 Example: Iris Data\nBackground\nA botanist wants to determine if the three species of iris flowers (setosa, versicolor, and virginica) have different overall morphologies. Instead of just looking at one measurement, they want to compare the species based on a complete profile of all four available measurements: Sepal.Length, Sepal.Width, Petal.Length, and Petal.Width from the iris data.\nBecause we are comparing a vector of mean responses across more than two groups, this is a classic problem for Multivariate Analysis of Variance (MANOVA).\n\nGroups (g=3): setosa, versicolor, virginica\nResponse Variables (p=4): Sepal.Length, Sepal.Width, Petal.Length, Petal.Width\nResearch Question: Are the mean vectors of these four characteristics the same across all three species?\n\n\n\n5.2.2 Load and Visualize the Data\nBefore testing, it is crucial to visualize the data. A pairs plot is excellent for this, as it shows the relationship between all variables for each species.\n\n\nR Code: Between Group plots\nlibrary(dplyr)\nlibrary(ggplot2)\ndata(iris)\n#head(iris)\ndf = iris\n\ng = length(levels(df$Species))\np = 4\n\ndf_long = df %&gt;% \n  pivot_longer(cols=c(1:4), \n               names_to=\"var\")\ng1 = ggplot(df_long) + \n  geom_point(aes(x=Species, y=value), \n             size=.8, alpha=.8) + \n  facet_wrap(~var, scales=\"free_y\")\nprint(g1)\n\n\n\n\n\n\n\n\n\nInterpretation: These panels suggest that there is a moderate variation across different species (groups) for each of the variables. Within each species (group), the observations also indicates some variations.\n\n\nR Code: Between Group boxplots\ng2 = ggplot(df_long, aes(x=Species, y=value)) + \n  geom_boxplot(fill = 'skyblue') + \n  #geom_jitter(width = 0.1) + \n  facet_wrap(~var, scales=\"free_y\") + \n  labs(y=\"\")\nprint(g2)\n\n\n\n\n\n\n\n\n\nInterpretation: All the variables across different species roughly follow symmetric distributions, except for the Petal.Width from the Setosa, whose distribution seems to be highly skewed to the right.\n\n\nR Code: Pairwise Scatterplot\nlibrary(GGally)\n\n# Create a pairs plot, colored by Species\nggpairs(\n  iris,\n  columns = 1:4,\n  ggplot2::aes(color = Species)\n) +\nlabs(title = \"Pairs Plot of Iris Measurements by Species\") +\ntheme_bw()\n\n\n\n\n\n\n\n\n\nInterpretation: The plot shows some separation among the three species, especially for the petal measurements. There are strong linear associations between Petal.Length and Petal.Width among all the three species. In general, there is also a linear association between Sepal.Length and Petal.Length, between Septal.Length and Petal.Width.\n\n\n5.2.3 State the Hypotheses\nFormulate the null and alternative hypotheses for the MANOVA test.\n\n\n\n\n\n\nHypotheses\n\n\n\n\n\nThe null hypothesis states that the true mean vectors for the full morphology profile are identical for all three species. The alternative hypothesis states that at least two of the species have different mean vectors.\n\nNull Hypothesis (H_0):  H_0: \\boldsymbol{\\mu}_{\\text{setosa}} = \\boldsymbol{\\mu}_{\\text{versicolor}} = \\boldsymbol{\\mu}_{\\text{virginica}} \nAlternative Hypothesis (H_1):  H_1: \\text{At least one } \\boldsymbol{\\mu}_{k} \\neq \\boldsymbol{\\mu}_{\\ell} \\text{ for } k \\neq \\ell \n\n\n\n\n\n\n5.2.4 Check Assumptions\nMANOVA relies on two key assumptions: multivariate normality within each group and the homogeneity of their covariance matrices.\n\n\nR Code: Normality Checks\n# check univariate normaltiy \niris %&gt;%\n  pivot_longer(\n    cols = 1:4, \n    names_to = \"Variable\", \n    values_to = \"Value\"\n  ) %&gt;%\n  # Group by both Species and the new Variable column\n  group_by(Species, Variable) %&gt;%\n  # Run the shapiro.test for each group\n  dplyr::summarise(\n    p_value = stats::shapiro.test(Value)$p.value, \n    .groups = \"drop\" \n  )\n\n\n\n  \n\n\n\nR Code: Normality Checks\n# check multivariate normaltiy\nmntest = c()\nfor(i in levels(iris$Species)){\nmntest[i] = mvShapiroTest::mvShapiro.Test(\n  as.matrix(iris[iris$Species == i, -5]))$p.value\n}\n# p values: \nprint(mntest)\n\n\n    setosa versicolor  virginica \n 0.0120325  0.3182897  0.9652400 \n\n\n\n\nR Code: Test Homogeneity of Covariance Matrices\n# Note: Box's M-test is very sensitive, especially with larger datasets.\nbiotools::boxM(iris[, 1:4], iris$Species)\n\n\n\n    Box's M-test for Homogeneity of Covariance Matrices\n\ndata:  iris[, 1:4]\nChi-Sq (approx.) = 140.94, df = 20, p-value &lt; 2.2e-16\n\n\nInterpretation:\n\nNormality: The p-values for all three species are relatively large (&gt;0.01), indicating that the data within each group are roughly consistent with a multivariate normal distribution.\nBox’s M-Test: The p-value is very small (p &lt; 0.001), indicating that the assumption of equal covariance matrices is violated. However, MANOVA is generally robust to this violation when group sizes are equal (as they are here, n=50 for each), so we can proceed, but we should acknowledge this limitation in a formal report.\n\n\n\n5.2.5 One-Way MANOVA\nWe will now perform the MANOVA to formally test the null hypothesis. The most common test statistic is Wilk’s \\Lambda, which essentially compares the variability within groups to the total variability. Small values of Wilk’s \\Lambda suggest that the group means are different.\nThe one-way MANOVA model is given by: \n\\mathbf{x}_{\\ell j} = \\boldsymbol{\\mu} + \\boldsymbol{\\tau}_{\\ell} + \\boldsymbol{\\epsilon}_{\\ell j}, \\quad \\ell=1,\\ldots, g; \\quad j=1,\\ldots, n_{\\ell}\n where\n\n\\boldsymbol{\\tau}_{\\ell} represents the treatment (group) effect with the constraint that \\sum_{\\ell=1}^{g} n_{\\ell} \\boldsymbol{\\tau}_{\\ell} = \\mathbf{0},\nthe error terms are independently distributed as \\boldsymbol{\\epsilon}_{\\ell j} \\sim N_p(\\mathbf{0}, {\\Sigma}),\n\\boldsymbol \\mu is the overall (or grand) mean across all populations, and \\boldsymbol \\mu_{\\ell} := \\boldsymbol \\mu + \\boldsymbol{\\tau}_{\\ell} is the group mean for the \\ellth population.\n\n\n\n\n\n\n\nMANOVA Table\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource of Variation\nMatrix of Sums of Squares and Cross-Products (SSP)\nDegrees of Freedom\n\n\n\n\nTreatment\nB = \\sum_{\\ell} n_\\ell (\\bar{\\mathbf{x}}_\\ell - \\bar{\\mathbf{x}})(\\bar{\\mathbf{x}}_\\ell - \\bar{\\mathbf{x}})'\ng - 1\n\n\nResidual\nW = \\sum_{\\ell} \\sum_{j} (\\mathbf{x}_{\\ell j} - \\bar{\\mathbf{x}}_\\ell)(\\mathbf{x}_{\\ell j} - \\bar{\\mathbf{x}}_\\ell)'\nn - g\n\n\nTotal (Corrected)\nB + W = \\sum_{\\ell} \\sum_{j} (\\mathbf{x}_{\\ell j} - \\bar{\\mathbf{x}})(\\mathbf{x}_{\\ell j} - \\bar{\\mathbf{x}})'\nn - 1\n\n\n\n\nThe within-group SSP matrix, \\mathbf{W}, can be expressed as: \\begin{align*}\n\\mathbf{W} &= \\sum_{\\ell = 1}^g \\sum_{j = 1}^{n_\\ell} (\\mathbf{x}_{\\ell j} - \\bar{\\mathbf{x}}_\\ell)(\\mathbf{x}_{\\ell j} - \\bar{\\mathbf{x}}_\\ell)' \\\\\n&= (n_1 - 1)S_1 + (n_2 - 1) S_2 + \\cdots + (n_g - 1) S_g \\\\\n&= (n-g) S_{\\text{pool}}\n\\end{align*} where n = \\sum_{\\ell=1}^g n_\\ell.\nThe pooled covariance matrix is calculated as: \nS_{\\text{pool}} = \\sum_{\\ell = 1}^g \\left[\\frac{(n_\\ell - 1)}{\\sum_{j=1}^g (n_j - 1)}\\right] S_\\ell.\n\n\n\n\n\n\n\n\n\n\n\nWilk’s \\Lambda Test Statistic\n\n\n\n\n\n\nOne test of the null hypothesis is carried out using a statistic called Wilk’s \\Lambda (a likelihood ratio test): \n\\Lambda = \\frac{|W|}{|B + W|}.\n\nIf B is “small” relative to W, then \\Lambda will be close to 1. Otherwise, \\Lambda will be small.\nWe reject the null hypothesis when \\Lambda is small.\nExact Distribution of Wilk’s \\Lambda\n\n\n\n\n\n\n\n\n\nNo. of Variables (p)\nNo. of Groups (g)\nSampling Distribution for Multivariate Normal Data\n\n\n\n\np = 1\ng \\geq 2\n\\left(\\frac{n - g}{g - 1}\\right)\\left(\\frac{1 - \\Lambda}{\\Lambda}\\right) \\sim F_{g - 1,\\ n - g}\n\n\np = 2\ng \\geq 2\n\\left(\\frac{n - g - 1}{g - 1}\\right)\\left(\\frac{1 - \\sqrt{\\Lambda}}{\\sqrt{\\Lambda}}\\right) \\sim F_{2(g - 1),\\ 2(n - g - 1)}\n\n\np \\geq 1\ng = 2\n\\left(\\frac{n - p - 1}{p}\\right)\\left(\\frac{1 - \\Lambda}{\\Lambda}\\right) \\sim F_{p,\\ n - p - 1}\n\n\np \\geq 1\ng = 3\n\\left(\\frac{n - p - 2}{p}\\right)\\left(\\frac{1 - \\sqrt{\\Lambda}}{\\sqrt{\\Lambda}}\\right) \\sim F_{2p,\\ 2(n - p - 2)}\n\n\n\n\nF Approximation to the Sampling Distribution Wilk’s \\Lambda\n\nWhen the null hypothesis of equal population mean vectors is true, the distribution of the Wilks’ Lambda statistic can be approximated by an F-distribution: \n  \\frac{ 1 - \\Lambda^{1/b}}{\\Lambda^{1/b}} \\cdot \\frac{ ab - c}{p(g - 1)} \\sim F_{p(g - 1),\\ ab - c}\n where \\begin{align*}\n  a &= (n - g) - \\frac{p - g + 2}{2} \\\\\n  b &= \\sqrt{ \\frac{p^2 (g - 1)^2 - 4}{p^2 + (g - 1)^2 - 5} } \\\\\n  c &= \\frac{p(g - 1) - 2}{2}\n\\end{align*}\n\n\n\n\n\nR Code: MANOVA Test\n# The manova() function fits the model.\n# The formula cbind(Y1, Y2, Y3, Y4) ~ Group tells R to use all four\n# measurements as the multivariate response vector.\nmanova_fit &lt;- manova(\n  cbind(Sepal.Length, Sepal.Width, \n        Petal.Length, Petal.Width) \n  ~ Species, \n  data = iris)\n\n# The summary() function generates the output from the statistical test.\n# We specify test = \"Wilks\" to get the result for Wilk's Lambda.\n# Other options include \"Pillai\", \"Hotelling-Lawley\", and \"Roy\".\nsummary(manova_fit, test = \"Wilks\")\n\n\n           Df    Wilks approx F num Df den Df    Pr(&gt;F)    \nSpecies     2 0.023439   199.15      8    288 &lt; 2.2e-16 ***\nResiduals 147                                              \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInterpret the MANOVA Results\nBased on the F-statistic and p-value from the test, what is your conclusion?\n\n\n\n\n\n\nInterpretation\n\n\n\n\n\nThe p-value is reported as &lt; 2.2e-16, which is exceptionally small. We reject the null hypothesis at significance level 0.05.\nConclusion: There is a statistically significant difference in the overall morphology (the mean vector of the four measurements) among the three iris species.\n\n\n\n\n\n5.2.6 Pairwse Comparison\nA significant MANOVA result tells us that a difference exists, but not where that difference lies. We need to perform follow-up tests to understand the result more deeply.\n\n\n\n\n\n\nMethod 1: Univariate ANOVA\n\n\n\n\n\nA simple first step is to look at the results for each response variable individually to see which ones are contributing to the overall difference.\n\n\nR Code: Univariate Follow-up\n# The summary.aov() function provides the results \n# for each dependent variable separately.\nsummary.aov(manova_fit)\n\n\n Response Sepal.Length :\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies       2 63.212  31.606  119.26 &lt; 2.2e-16 ***\nResiduals   147 38.956   0.265                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response Sepal.Width :\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies       2 11.345  5.6725   49.16 &lt; 2.2e-16 ***\nResiduals   147 16.962  0.1154                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response Petal.Length :\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies       2 437.10 218.551  1180.2 &lt; 2.2e-16 ***\nResiduals   147  27.22   0.185                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response Petal.Width :\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies       2 80.413  40.207  960.01 &lt; 2.2e-16 ***\nResiduals   147  6.157   0.042                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInterpretation: The univariate ANOVAs show extremely small p-values for all four variables (Sepal.Length, Sepal.Width, Petal.Length, and Petal.Width). This indicates that the group means are significantly different for every measurement when considered one at a time.\n\n\n\n\n\n\n\n\n\nMethod 2: Pairwise Group Comparisons\n\n\n\nTo find out which specific species are different from each other, we could perform pairwise comparisons. This could involve running two-sample Hotelling’s T² tests for each pair (e.g., setosa vs. versicolor) with a Bonferroni correction to the alpha level to control for multiple comparisons. Given the clear separation in the plots, we can be confident that all three species are significantly different from one another on their overall morphology profile.\n\nIf we wish to carry out all pairwise comparisons, there will be p g (g - 1)/2 of them.\nTo maintain a simultaneous type I error level of no more than \\alpha we can use \nt_{(n-g), 1-\\frac{\\alpha}{2m}} \\quad \\text{ where } \\quad  m = \\frac{pg(g-1)}{2}.\n\nFormulas for the simultaneous Bonferroni CIs are \n\\left( \\bar{x}_{ik} - \\bar{x}_{i \\ell} \\right)  \\ \\pm \\  t_{(n-g), 1-\\frac{\\alpha}{2m}} \\sqrt{\\left(\\frac{1}{n_k} + \\frac{1}{n_\\ell}\\right) S_{pool,ii}}\n\n\n\n\n\n\nR Code: Bonferroni CIs\nlibrary(dplyr)\nn = iris %&gt;%\n  count(Species) %&gt;%\n  pull(n, name = Species)\n\np = 4 # variables\ng = length(levels(iris$Species))\n\nlevel &lt;- 0.95\n\nm &lt;- p * g * (g - 1) / 2\nlevel2 &lt;- 1 - (1 - level) / (2 * m)\ndf &lt;- sum(n) - g\nc_bon  &lt;- qt(level2, df)\n\n# compute pooled covariance matrix\nSp = summary(manova_fit)$SS$Residuals / df\nSp_ii = diag(Sp)\n\n# sample mean\nxbar = iris %&gt;%\n  group_by(Species) %&gt;%\n  dplyr::summarise(across(where(is.numeric), mean)) %&gt;%\n  as.matrix()\n\n# Get group sample sizes (n_k) and means (x_bar_k)\niris_summaries &lt;- iris %&gt;%\n  group_by(Species) %&gt;%\n  dplyr::summarise(across(where(is.numeric), \n                          list(mean = mean, n = ~ n())), \n                   .groups = \"drop\")\n\n# Get all unique pairs of species\nspecies_pairs &lt;- combn(unique(iris$Species), \n                       2, simplify = FALSE)\n# Use purrr::map_df to loop through pairs \nall_intervals &lt;- map_df(species_pairs, function(pair) {\n  species1 &lt;- pair[1]\n  species2 &lt;- pair[2]\n  \n  # Get means and sample sizes for the two species being compared\n  summary1 &lt;- iris_summaries %&gt;% \n    filter(Species == species1)\n  summary2 &lt;- iris_summaries %&gt;% \n    filter(Species == species2)\n  \n  n1 &lt;- summary1$Sepal.Length_n\n  n2 &lt;- summary2$Sepal.Length_n\n  \n  mean_diffs &lt;- as.numeric(\n    dplyr::select(summary1, \n                  ends_with(\"_mean\"))) -\n    as.numeric(\n      dplyr::select(summary2, \n                    ends_with(\"_mean\")))\n  \n  # Calculate margin of error for this pair\n  margin_of_error &lt;- c_bon * \n    sqrt((1 / n1 + 1 / n2) * Sp_ii)\n  \n  # Create a tibble for this pair's results\n  tibble(\n    Comparison = paste(species1, \"vs.\", species2),\n    Variable = names(Sp_ii),\n    Mean_Difference = mean_diffs,\n    Lower_CI = mean_diffs - margin_of_error,\n    Upper_CI = mean_diffs + margin_of_error\n  )\n})\n\nknitr::kable(\n  all_intervals, digits = 3, \n  caption = \"Simultaneous 95% Bonferroni Confidence Intervals\")\n\n\n\nSimultaneous 95% Bonferroni Confidence Intervals\n\n\n\n\n\n\n\n\n\nComparison\nVariable\nMean_Difference\nLower_CI\nUpper_CI\n\n\n\n\nsetosa vs. versicolor\nSepal.Length\n-0.930\n-1.230\n-0.630\n\n\nsetosa vs. versicolor\nSepal.Width\n0.658\n0.460\n0.856\n\n\nsetosa vs. versicolor\nPetal.Length\n-2.798\n-3.049\n-2.547\n\n\nsetosa vs. versicolor\nPetal.Width\n-1.080\n-1.199\n-0.961\n\n\nsetosa vs. virginica\nSepal.Length\n-1.582\n-1.882\n-1.282\n\n\nsetosa vs. virginica\nSepal.Width\n0.454\n0.256\n0.652\n\n\nsetosa vs. virginica\nPetal.Length\n-4.090\n-4.341\n-3.839\n\n\nsetosa vs. virginica\nPetal.Width\n-1.780\n-1.899\n-1.661\n\n\nversicolor vs. virginica\nSepal.Length\n-0.652\n-0.952\n-0.352\n\n\nversicolor vs. virginica\nSepal.Width\n-0.204\n-0.402\n-0.006\n\n\nversicolor vs. virginica\nPetal.Length\n-1.292\n-1.543\n-1.041\n\n\nversicolor vs. virginica\nPetal.Width\n-0.700\n-0.819\n-0.581\n\n\n\nSimultaneous 95% Bonferroni CIs. Intervals not crossing the dashed line (zero) are statistically significant.\n\n\nR Code: Bonferroni CIs\nggplot(all_intervals,\n       aes(x = Mean_Difference, y = Variable, \n           color = Variable)) +\n  geom_errorbar(aes(xmin = Lower_CI, xmax = Upper_CI),\n                 width = 0.2,\n                 linewidth = 1) +\n  geom_point(size = 3.5) +\n  geom_vline(xintercept = 0,\n             linetype = \"dashed\",\n             color = \"black\") +\n  facet_wrap( ~ Comparison) +\n  labs(\n    x = \"Mean Difference\",\n    y = \"Variable\"\n  ) +\n  theme_bw(base_size = 12) +\n  theme(legend.position = \"none\",\n        axis.text.y = element_text(angle=60)) \n\n\n\n\n\nSimultaneous 95% Bonferroni CIs. Intervals not crossing the dashed line (zero) are statistically significant.\n\n\n\n\nInterpretation: This visualization makes the conclusions from our analysis immediately obvious:\n\nNo intervals cross the zero line. Every single error bar for every comparison is clearly to the left or right of the vertical dashed line.\nThis provides powerful visual evidence that, after controlling for all 12 comparisons, all three iris species are significantly different from each other on all four measured variables. For example, in the versicolor v.s. virginica panel, the point for Petal.Length is around -1.3, and its confidence interval from approximately -1.6 to -1.0 is far from zero.\nThe simultaneous Bonferroni CIs confirm previous MANOVA test and provide more details on which variables are significantly different.\n\n\n\n5.2.7 Exercise: College Student Study\nBackground: In a college student study, a sample of first-year university students was selected from three popular and critical fields of study. Each student was administered a standardized academic assessment battery upon entry. The goal is to see if the overall academic profile differs significantly among these groups. Perform detailed statistical analysis for the data below.\n\nmorel = readr::read_csv(file = \"morel.csv\", \n                        show_col_types = FALSE) %&gt;% \n  mutate(group = as.factor(group))\nhead(morel)\n\n\n  \n\n\n\n\nData visualization\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nR Code: Data Visualization\ndf = morel \nresponse_vars = setdiff(colnames(df), \"group\")\n\np = length(response_vars)\ng = length(levels(df$group))\n\ndf_long = df %&gt;%\n  pivot_longer(cols=c(2:5),\n               names_to=\"var\")\nggplot(df_long, aes(x=group, y=value)) + \n  geom_point() + \n  facet_wrap(~var, scales=\"free_y\") + \n  labs(y=\"\")\n\n\n\n\n\n\n\n\n\nR Code: Data Visualization\nGGally::ggpairs(\n  df,\n  columns = 2:5, # The four test score variables\n  ggplot2::aes(color = group),\n  upper = list(continuous = \"cor\"),\n  lower = list(continuous = \"points\")\n) +\nlabs(title = \"Academic Score Profiles by Field of Study\") +\ntheme_bw()\n\n\n\n\n\n\n\n\n\nThe plot suggests there may be differences. For example, the distribution of math scores for Architecture students appears shifted compared to the other groups.\n\n\n\n\nState the hypotheses using standard notations.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\nThe null hypothesis states that the true mean vectors of test scores are the same for all three student populations. The alternative states that at least two groups have different mean vectors.\n\nNull Hypothesis (H_0):  H_0: \\boldsymbol{\\mu}_{\\text{Technology}} = \\boldsymbol{\\mu}_{\\text{Architecture}} = \\boldsymbol{\\mu}_{\\text{Medical Tech}} \nAlternative Hypothesis (H_1):  H_1: \\text{At least one } \\boldsymbol{\\mu}_{k} \\neq \\boldsymbol{\\mu}_{\\ell} \\text{ for } k \\neq \\ell \n\n\n\n\n\nCheck Assumptions\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\n# check univariate normaltiy \ndf %&gt;%\n  pivot_longer(\n    cols = 2:5, \n    names_to = \"Variable\", \n    values_to = \"Value\"\n  ) %&gt;%\n  group_by(group, Variable) %&gt;%\n  dplyr::summarise(\n    p_value = stats::shapiro.test(Value)$p.value, \n    .groups = \"drop\" \n  )\n\n\n\n  \n\n\n\nCode\n# check multivariate normaltiy\nmntest = c()\nfor(i in levels(df$group)){\nmntest[i] = mvShapiroTest::mvShapiro.Test(\n  as.matrix(df[df$group == i, colnames(df) !=\"group\"]))$p.value\n}\n# p values: \nprint(mntest)\n\n\n         1          2          3 \n0.00388046 0.00389287 0.01745476 \n\n\nCode\nbiotools::boxM(df[, colnames(df) !=\"group\"], df$group)\n\n\n\n    Box's M-test for Homogeneity of Covariance Matrices\n\ndata:  df[, colnames(df) != \"group\"]\nChi-Sq (approx.) = 33.493, df = 20, p-value = 0.02977\n\n\n\n\n\n\nPerform the one-way MANOVA test\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\n# fit one-way ANOVA to each of the response\nformula = paste(\"cbind(\", \n                paste(response_vars, collapse = \", \"), \n                \") ~ group\")\n\nfit.lm = manova(as.formula(formula), data = df)\n\n# fit MANOVA \nfit.manova = manova(fit.lm)\nsummary(fit.manova, test=\"Wilks\")\n\n\n          Df   Wilks approx F num Df den Df    Pr(&gt;F)    \ngroup      2 0.54345   6.7736      8    152 1.384e-07 ***\nResiduals 79                                             \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInterpretation: The p-value is less than our significance level of \\alpha = 0.05. We therefore reject the null hypothesis. We conclude that there is a statistically significant difference in the mean academic profiles among the three groups of students (Technology, Architecture, and Medical Technology).\n\n\n\n\nFollow-up analysis with pairwise comparisons\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\nn = df %&gt;%\n  count(group) %&gt;%\n  pull(n, name = group)\n\nlevel = 0.95\nm = p * g * (g - 1) / 2\nlevel2 = 1 - (1 - level) / (2 * m)\ndof = sum(n) - g\nc_bon  = qt(level2, dof)\n\n# compute pooled sample covariance\nSp = summary(fit.manova)$SS$Residuals / dof\nSp_ii = diag(Sp)\n\ndf_summaries &lt;- df %&gt;%\n  group_by(group) %&gt;%\n  dplyr::summarise(across(where(is.numeric), list(mean = mean, n = ~ n())), .groups = \"drop\")\n\n# Get all unique pairs\ngroup_pairs &lt;- combn(unique(df$group), 2, simplify = FALSE)\nall_intervals &lt;- purrr::map_df(group_pairs, function(pair) {\n  group1 &lt;- pair[1]\n  group2 &lt;- pair[2]\n  \n  summary1 &lt;- df_summaries %&gt;% filter(group == group1)\n  summary2 &lt;- df_summaries %&gt;% filter(group == group2)\n  \n  n1 &lt;- summary1$math_n\n  n2 &lt;- summary2$math_n\n  \n  mean_diffs &lt;- as.numeric(dplyr::select(summary1, ends_with(\"_mean\"))) -\n    as.numeric(dplyr::select(summary2, ends_with(\"_mean\")))\n  \n  margin_of_error &lt;- c_bon * sqrt((1 / n1 + 1 / n2) * Sp_ii)\n  \n  tibble(\n    Comparison = paste(group1, \"vs.\", group2),\n    Variable = names(Sp_ii),\n    Mean_Difference = mean_diffs,\n    Lower_CI = mean_diffs - margin_of_error,\n    Upper_CI = mean_diffs + margin_of_error\n  )\n})\n\nknitr::kable(all_intervals, digits = 3, caption = \"Simultaneous 95% Bonferroni Confidence Intervals\")\n\n\n\nSimultaneous 95% Bonferroni Confidence Intervals\n\n\nComparison\nVariable\nMean_Difference\nLower_CI\nUpper_CI\n\n\n\n\n1 vs. 2\naptitude\n-28.158\n-48.736\n-7.580\n\n\n1 vs. 2\nmath\n-3.793\n-14.388\n6.802\n\n\n1 vs. 2\nlanguage\n-6.681\n-12.732\n-0.629\n\n\n1 vs. 2\ngen_know\n-2.309\n-9.669\n5.051\n\n\n1 vs. 3\naptitude\n11.571\n-11.938\n35.081\n\n\n1 vs. 3\nmath\n9.296\n-2.808\n21.400\n\n\n1 vs. 3\nlanguage\n2.466\n-4.448\n9.380\n\n\n1 vs. 3\ngen_know\n-7.973\n-16.382\n0.436\n\n\n2 vs. 3\naptitude\n39.729\n18.550\n60.909\n\n\n2 vs. 3\nmath\n13.089\n2.185\n23.993\n\n\n2 vs. 3\nlanguage\n9.147\n2.918\n15.375\n\n\n2 vs. 3\ngen_know\n-5.664\n-13.239\n1.911\n\n\n\n\n\nCode\nggplot(all_intervals,\n       aes(x = Mean_Difference, y = Variable, color = Variable)) +\n  geom_errorbar(aes(xmin = Lower_CI, xmax = Upper_CI),\n                 width = 0.2,\n                 linewidth = 1) +\n  geom_point(size = 3.5) +\n  geom_vline(xintercept = 0,\n             linetype = \"dashed\",\n             color = \"black\") +\n  facet_wrap( ~ Comparison) +\n  labs(\n    title = \"Simultaneous 95% Bonferroni CIs\",\n    subtitle = \"Intervals not crossing the dashed line (zero) are statistically significant.\",\n    x = \"Mean Difference\",\n    y = \"Measurement Variable\"\n  ) +\n  theme_bw(base_size = 12) +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inference for Multiple Mean Vectors</span>"
    ]
  },
  {
    "objectID": "ch5/05-Inference_for_Means_II.html#permulation-test",
    "href": "ch5/05-Inference_for_Means_II.html#permulation-test",
    "title": "5  Inference for Multiple Mean Vectors",
    "section": "5.3 Permulation Test",
    "text": "5.3 Permulation Test\n\n5.3.1 Introduction\nA permutation test is a type of non-parametric statistical test. It is “distribution-free,” meaning it does not rely on assumptions that the data are drawn from a given probability distribution (like the normal distribution). This makes it an incredibly robust and versatile tool for hypothesis testing.\nWhen to Use a Permutation Test:\n\nWhen your sample size is small.\nWhen your data does not meet the assumptions of parametric tests (e.g., it’s not normally distributed).\nWhen you are working with an unusual test statistic for which the theoretical distribution is unknown.\n\n\n\n5.3.2 The Steps of a Permutation Test\nEvery permutation test follows the same fundamental logic:\n\nCalculate the Observed Statistic: Compute the test statistic on your original, unshuffled data (e.g., the difference in means between two groups).\nCreate a Null Distribution:\n\nPool all the data together.\nRepeatedly (e.g., 10,000 times) shuffle the pooled data and randomly reassign it to groups of the original sizes.\nFor each shuffle, re-calculate the test statistic. The collection of these statistics forms the null distribution—the distribution of what your statistic looks like when the null hypothesis (of no effect) is true.\n\nCalculate the p-value: The p-value is the proportion of statistics from the null distribution that are as extreme or more extreme than your originally observed statistic.\n\n\n\n5.3.3 Example: Two-Sample Comparison\nBackground: We have data on the fuel efficiency (MPG) for a sample of 4-cylinder and 8-cylinder cars from the mtcars dataset. We want to test if there is a significant difference in the mean MPG between these two groups.\n\n\nR Code: Two-Sample Permutation Test\nlibrary(dplyr)\nlibrary(ggplot2)\ndata(mtcars)\n\n# Prepare the data\ncars_data &lt;- mtcars %&gt;%\n  filter(cyl %in% c(4, 8)) %&gt;%\n  dplyr::select(mpg, cyl)\n\ngroup1 &lt;- cars_data %&gt;% filter(cyl == 4) %&gt;% pull(mpg)\ngroup2 &lt;- cars_data %&gt;% filter(cyl == 8) %&gt;% pull(mpg)\nn1 &lt;- length(group1)\nn2 &lt;- length(group2)\n\n# Calculate the OBSERVED difference in means\nobserved_diff &lt;- mean(group1) - mean(group2)\n\n# Create the Null Distribution\nset.seed(4750) # For reproducibility\nn_permutations &lt;- 10000\npermutation_diffs &lt;- numeric(n_permutations)\nall_data &lt;- c(group1, group2)\n\nfor (i in 1:n_permutations) {\n  # Shuffle the data\n  shuffled_data &lt;- sample(all_data)\n  \n  # Assign to new sham groups\n  new_group1 &lt;- shuffled_data[1:n1]\n  new_group2 &lt;- shuffled_data[(n1 + 1):(n1 + n2)]\n  \n  # Calculate and store the difference for this permutation\n  permutation_diffs[i] &lt;- mean(new_group1) - mean(new_group2)\n}\n\n# Calculate the p-value\np_value &lt;- sum(abs(permutation_diffs) &gt;= \n                 abs(observed_diff)) / n_permutations\n\n\nggplot(data.frame(diffs = permutation_diffs), \n       aes(x = diffs)) +\n  geom_histogram(aes(y = ..density..), \n                 bins = 30, fill = \"lightblue\", \n                 color = \"black\") +\n  geom_density(color = \"blue\", size = 1) +\n  geom_vline(xintercept = observed_diff, \n             color = \"red\", linetype = \"dashed\", size = 1.2) +\n  annotate(\"text\", x = observed_diff - 1.5, y = 0.1, \n           label = paste(\"Observed Difference\\np-value =\", \n                         p_value), color = \"red\") +\n  labs(\n    title = \"Permutation Test for MPG Difference (4-cyl vs. 8-cyl)\",\n    x = \"Difference in Mean MPG\",\n    y = \"Density\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nInterpretation: The observed difference (the red dashed line) is far out in the tail of the null distribution, and the p-value is effectively zero. This tells us that it is extremely unlikely to get a difference this large by random chance alone. We can confidently conclude that 4-cylinder cars have a significantly higher mean MPG than 8-cylinder cars.\n\n\n5.3.4 Exercise: Testing a Correlation\nBackground: Is there a significant correlation between a car’s weight (wt) and its fuel efficiency (mpg)? The null hypothesis is that the true correlation is zero.\nThe permutation logic is slightly different here: if there’s no relationship between weight and MPG, then we should be able to shuffle the order of one variable without affecting the correlation.\n\n\nR Code: Correlation Permutation Test\n# Prepare the data\nwt_data &lt;- mtcars$wt\nmpg_data &lt;- mtcars$mpg\n\n# Calculate the OBSERVED correlation\nobserved_cor &lt;- cor(wt_data, mpg_data)\n\n# Create the Null Distribution\nset.seed(123)\nn_permutations &lt;- 10000\npermutation_cors &lt;- numeric(n_permutations)\n\nfor (i in 1:n_permutations) {\n  # Shuffle ONLY one of the variables\n  shuffled_mpg &lt;- sample(mpg_data)\n  \n  # Calculate and store the correlation for this permutation\n  permutation_cors[i] &lt;- cor(wt_data, shuffled_mpg)\n}\n\n# Calculate the p-value\np_value_cor &lt;- sum(abs(permutation_cors) &gt;= \n                     abs(observed_cor)) / n_permutations\n\n\nggplot(data.frame(cors = permutation_cors), aes(x = cors)) +\n  geom_histogram(aes(y = ..density..), bins = 30, \n                 fill = \"lightgreen\", color = \"black\") +\n  geom_density(color = \"darkgreen\", size = 1) +\n  geom_vline(xintercept = observed_cor, color = \"red\", \n             linetype = \"dashed\", size = 1.2) +\n  annotate(\"text\", x = observed_cor + 0.3, y = 1.5, \n           label = paste(\"Observed Correlation =\", \n                         round(observed_cor, 2), \n                         \"\\np-value =\", p_value_cor), \n           color = \"red\") +\n  labs(\n    title = \"Permutation Test for Correlation (MPG vs. Weight)\",\n    x = \"Correlation Coefficient\",\n    y = \"Density\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nInterpretation: The observed correlation of -0.87 is an extreme outlier compared to the null distribution of correlations centered at zero. The p-value is effectively zero. We can conclude there is a highly significant negative correlation between a car’s weight and its fuel efficiency.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inference for Multiple Mean Vectors</span>"
    ]
  },
  {
    "objectID": "ch6/06-PCA.html",
    "href": "ch6/06-PCA.html",
    "title": "6  Principal Component Analysis",
    "section": "",
    "text": "6.1 Why PCA?\nPCA is a powerful exploratory statistical tool that can",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "ch6/06-PCA.html#why-pca",
    "href": "ch6/06-PCA.html#why-pca",
    "title": "6  Principal Component Analysis",
    "section": "",
    "text": "reduce many variables (p) into a few linear combinations (principal components) that preserve most variation,\nbuild summary indices and visualize structure (clusters, outliers),\nand serve as a first step before applying other statistical methods (e.g., regression, clustering, or classification).\n\n\n6.1.1 Motivating Examples (Saccenti 2023)\n\n\n\n\n\nA 2-d example. The left figure is based on a set of 4,608 observations of two variables. Right figure is a 65^\\circ rotation of the left figure.\n\n\n\n\n\n\n\n\n\nA 3-d example. The top panel is based on a set of 36,876 observations of three variables. The bottom panel is a rotation of the top panel.\n\n\n\n\n\n\nRotation can change the amount of information along different dimensions.\nSome rotations can provide more information than others.\nRotation does not change the distances between points.\nPCA is a statistical tool to detect and visualize the structure of high-dimensional data and obtain its low-dimensional representation",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "ch6/06-PCA.html#geometric-interpretation",
    "href": "ch6/06-PCA.html#geometric-interpretation",
    "title": "6  Principal Component Analysis",
    "section": "6.2 Geometric Interpretation",
    "text": "6.2 Geometric Interpretation\n\n6.2.1 Basic Idea\n\n\nPCA searches for a line (e.g., the red dashed line) that minimizes the distances from the data points (circles) to the line.\nEquivalently, PCA searches for a line (e.g., the red dashed line) that maximizes the distances from the projected points (cross signs) to origin.\nThe average of the sum of squared distances is called the eigenvalue.\nPC1 is the direction that captures the most variation in the data (or that makes the data looks the most spread out).\nPC2 is the next best, but perpendicular, so it explains whatever variation PC1 does not capture.\nPC scores are just the coordinates of each point in that rotated system.\n\n\n\n6.2.2 Illustrating Example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\n\n\n\nGeometric view of PCA for iris data: axes, variance, and projection. Arrows = PC directions at the mean; dashed line = projection (PC score) onto PC1.\n\n\n\n\n\n\n\n\n\n6.2.3 Review of Spectral Decomposition\nRecall from Chapter 3 that for any p\\times p symmetric matrix A, its spectral decomposition is  A = \\lambda_1 \\mathbf{e}_1 \\mathbf{e}_1^{\\top} +  \\lambda_2 \\mathbf{e}_2 \\mathbf{e}_2^{\\top} +  \\cdots +  \\lambda_p \\mathbf{e}_p \\mathbf{e}_p^{\\top}\n\n\n\\lambda_1\\geq \\lambda_2\\geq \\cdots \\geq \\lambda_p are eigenvalues of A.\n\\mathbf{e}_j’s are corresponding eigenvectors of A.\n\n\n\n\n\n\n\nSome Facts\n\n\n\nSuppose that you obtain a p\\times p covariance (or correlation) matrix for p variables from the data.\n\nThe number of eigenvalues are the same as the number of observed variables.\nThe sum of the eigenvalues equals the trace of the covariance matrix.\n\nFor a correlation matrix, this would be p, since all diagonal is 1’s.\nThe sum of the eigenvalues gives the total variance of the data.\n\nThe product of the eigenvalues equals the determinant of the covariance matrix.\n\nThe determinant of covariance matrix measures the generalized variance in the data.\n\nThe number of non-zero eigenvalues is the rank of the matrix.\n\n\n\n\n\n\n\n\n\nInterpretation of Covariance Matrix\n\n\n\n\nThe first eigenvector represents the direction of maximum variation and the first eigenvalue represents the amount of variation in the data.\nThe second eigenvector is the director of maximum variation that is orthogonal to the first eigenvector, and the second eigenvalue represents its variation.\nAll eigenvectors are directions of variation, orthogonal to all other eigenvectors. The eigenvalues are their variances.\n\n\n\n\n\n6.2.4 The Mathematics Behind PCA\n\nLet X=(X_1, X_2, \\ldots, X_p)^\\top denote a random vector with covariance matrix \\Sigma.\nWe seek weight vectors \\mathbf{a}_1,\\dots,\\mathbf{a}_p and scores Y_k = \\mathbf{a}_k'X such that:\n\nY_1 has max variance among all unit-length \\mathbf{a}_1,\nY_2 has max variance subject to being uncorrelated with Y_1,\n… and so on.\n\nSuppose that \\Sigma has spectral decomposition with eigenpairs (\\lambda_k, \\mathbf{e}_k), k=1,\\ldots, p, and \\lambda_1 \\ge \\cdots \\ge \\lambda_p.\n\nThe k-th principal component (PC) is Y_k = \\mathbf{e}_k'X.\n\\mathrm{Var}(Y_k) = \\lambda_k represents the variance of the score Y_k.\nProportion of total variance explained by PC k is \\lambda_k / \\sum_{j=1}^p \\lambda_j.\n\\mathbf{e}_k represents the PC direction with its elements called loadings.\n\nIn practice, we have multiple observations \\mathbf{x}_1, \\ldots, \\mathbf{x}_n from the random vector X. Same idea applies but with notations changed.\n\nWe seek weight vectors \\mathbf{a}_1,\\dots,\\mathbf{a}_p and scores y_{ik} = \\mathbf{a}_k^\\top \\mathbf{x}_i subject to the same constraints above.\n\nPCA can be applied for raw data or standardized data (z-scores).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "ch6/06-PCA.html#pca-via-prcomp",
    "href": "ch6/06-PCA.html#pca-via-prcomp",
    "title": "6  Principal Component Analysis",
    "section": "6.3 PCA via prcomp",
    "text": "6.3 PCA via prcomp\nLet’s start by creating a simple 2D dataset where the two variables, X1 and X2, are clearly correlated.\n\n\nR Code: Simulation and Visualization\nlibrary(MASS)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(4750)\n\n# Create correlated data\ncov_matrix &lt;- matrix(c(10, 8, 8, 10), nrow = 2)\ndata_orig &lt;- as.data.frame(MASS::mvrnorm(n = 200, mu = c(0, 0), \n                                         Sigma = cov_matrix))\ncolnames(data_orig) &lt;- c(\"X1\", \"X2\")\n\nggplot(data_orig, aes(x = X1, y = X2)) +\n  geom_point(alpha = 0.7, color = \"blue\") +\n  coord_fixed(xlim = c(-10, 10), ylim = c(-10, 10)) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) +\n  labs(title = \"Original Correlated Data\") +\n  theme_bw()\n\ncov_orig &lt;- cov(data_orig)\ncat(\"Original Covariance Matrix:\\n\")\nprint(cov_orig)\n\n\n\n\n\n\n\n\n\n\n\nOriginal Covariance Matrix:\n\n\n          X1       X2\nX1 10.964268 8.577499\nX2  8.577499 9.737786\n\n\nThe data points form a tilted oval shape, indicating a strong positive correlation between X1 and X2. The covariance matrix confirms this, with a large positive off-diagonal value of 8.58. The variances of X1 and X2 are 10.96 and 9.74 respectively.\nNow, we perform PCA to find the new axes (the principal components) that best align with the data’s spread.\n\n# Perform PCA\nfit &lt;- prcomp(data_orig, center=TRUE)\n\nstr(fit)\n\nList of 5\n $ sdev    : num [1:2] 4.35 1.32\n $ rotation: num [1:2, 1:2] 0.732 0.681 0.681 -0.732\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:2] \"X1\" \"X2\"\n  .. ..$ : chr [1:2] \"PC1\" \"PC2\"\n $ center  : Named num [1:2] 0.031 -0.0625\n  ..- attr(*, \"names\")= chr [1:2] \"X1\" \"X2\"\n $ scale   : logi FALSE\n $ x       : num [1:200, 1:2] -1.008 -0.675 8.176 -4.017 -1.704 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"PC1\" \"PC2\"\n - attr(*, \"class\")= chr \"prcomp\"\n\n\nOutput interpretation:\n\nsdev: standard deviations of the PC scores (or square roots of eigenvalues of the covariance matrix). This variable can be used to produce scree plot, see Section 6.5.2.\ncenter: The mean of each column of data matrix.\nrotation: The rotation matrix that contains the PC directions (or elements of eigenvectors).\nx: a n\\times p matrix of PC scores which is the centered and scaled (if requested) data multiplied by rotation matrix.\n\n\nfit$rotation\n\n         PC1        PC2\nX1 0.7318853  0.6814278\nX2 0.6814278 -0.7318853\n\n\nThe fit$rotation gives the PC directions with the PC1 in the first column, PC2 in the second column, and so on.\n\n\n\n\n\n\nPCA via Spectral Decomposition\n\n\n\n\n\nThe PCA via the prcomp can be performed via Spectral Decomposition with the R code below.\n\nX = as.matrix(data_orig)\nXc = X - matrix(1, nrow(data_orig), ncol=1) %*% \n  t(colMeans(data_orig))\nS = cov(Xc)\nE = eigen(S)\nE \n\neigen() decomposition\n$values\n[1] 18.950420  1.751635\n\n$vectors\n           [,1]       [,2]\n[1,] -0.7318853  0.6814278\n[2,] -0.6814278 -0.7318853\n\nY = Xc%*%E$vectors\nhead(Y)\n\n           [,1]       [,2]\n[1,]  1.0076182 -2.7013675\n[2,]  0.6745194 -1.0377092\n[3,] -8.1757410 -0.7689785\n[4,]  4.0166282  0.4896621\n[5,]  1.7039008 -0.2043875\n[6,]  3.6023146 -1.3037205\n\n\n\nE$values: the eigenvalues of covariance matrix of centered data, which are the same as the square of fit$sdev.\nE$vector: the eigenvectors of covariance matrix of centered data, which are the same as fit$rotation.\nY: a matrix of PC scores, which is the same as fit$x.\n\n\n\n\n\n\nR Code: Visualize New Axes\nggplot(data_orig, aes(x = X1, y = X2)) +\n  geom_point(alpha = 0.7, color = \"blue\") +\n  coord_fixed(xlim = c(-10, 10), ylim = c(-10, 10)) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) +\n  # Add the principal component axes as red vectors\n  geom_segment(data = as.data.frame(fit$rotation), \n               aes(x = 0, y = 0, xend = PC1*10, \n                   yend = PC2*10), \n               arrow = arrow(length = unit(0.2, \"cm\")), \n               color = \"red\", linewidth = 1) +\n  annotate(\"text\", x = 7, y = 8, label = \"PC1 Axis\", \n           color = \"red\", size = 5) +\n  annotate(\"text\", x = 8, y = -8, label = \"PC2 Axis\", \n           color = \"red\", size = 5) +\n  labs(title = \"Original Data with New PC Axes\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nNote: PCA has found two new orthogonal axes. PC1 points along the direction of maximum variance (the long axis of the data cloud), which is defined by (0.73, 0.68). PC2 is perpendicular to PC1 and points along the direction of the next largest variance, which is defined by (0.68, -0.73).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "ch6/06-PCA.html#pc-scores",
    "href": "ch6/06-PCA.html#pc-scores",
    "title": "6  Principal Component Analysis",
    "section": "6.4 PC Scores",
    "text": "6.4 PC Scores\nThe final step is to project our original data points onto these new axes. This is equivalent to rotating the entire dataset so that the principal component axes become our new x and y axes. The resulting coordinates are the principal component scores.\nFor i-th observation \\mathbf{x}_i, the PC score y_{ik} corresponding to the k-th PC is given by \n\\begin{aligned}\ny_{ik} &= \\mathbf{e}_k^\\top (\\mathbf{x}_i -\\bar{\\mathbf{x}}) = \\sum_{j=1}^p e_{kj} (x_{ij}-\\bar{x}_j) = e_{k1}(x_{i1}-\\bar{x}_1)  + \\ldots + e_{kp}(x_{ip}-\\bar{x}_p),\n\\end{aligned}\n where \\mathbf{e}_k is the k-th eigenvector of centered data. PC score defined in the equation above provides a way to interpret its meaning.\n\nThe coefficients in the eigenvector \\mathbf{e}_k are called loadings and determine how all the p variables in each data point are weighted into the new data point (PC score).\nEach observation \\mathbf{x}_i is projected into the k-th PC axis direction defined by \\mathbf{e}_k.\n\n\n\n\n\n\n\nInterpretation of PC Scores\n\n\n\n\nThe sign of a PC score (e.g., y_{ik}) indicates which side (negative or positive) of the PC axis the data point is on (relative to the data mean).\nThe magnitude of a PC score measures how far the observation lies along that PC’s direction.\nFor the k-th PC, e_{kj} is called a loading:\n\nIf e_{kj}&gt;0 and an observation has a large positive score, it indicates an above average impact on variable j.\nIf e_{kj}&lt;0 and the score is positive, it indicates an below average impact on variable j.\nA large negative score flips those statements.\n\nNear zero score means average impact along that PC.\n\n\n\n\n# PC Scores\nhead(fit$x)\n\n            PC1        PC2\n[1,] -1.0076182 -2.7013675\n[2,] -0.6745194 -1.0377092\n[3,]  8.1757410 -0.7689785\n[4,] -4.0166282  0.4896621\n[5,] -1.7039008 -0.2043875\n[6,] -3.6023146 -1.3037205\n\n# Calculate the new covariance matrix\ncov_rotated = cov(as.matrix(fit$x))\ncov_rotated \n\n              PC1           PC2\nPC1  1.895042e+01 -9.466464e-15\nPC2 -9.466464e-15  1.751635e+00\n\n\nThe covariance of PC scores is a diagonal matrix theoretically because all the PC directions are perpendicular to each other. The off-diagonal element is now essentially zero (-9.5e-15). This is the magic of PCA! After the rotation, the new variables (PC1 and PC2) are uncorrelated.\nFurthermore, the variances have been redistributed. The variance is now maximized along the PC1 axis (Var(PC1) = 18.95) and minimized along the PC2 axis (Var(PC2) = 1.75). Notice that the total variance remains the same (20.7 vs. 20.7). The figure below shows the PC scores on the PC axes.\n\n\nR Code: Visualize PC Scores\ndf.PC = as.matrix(fit$x)\nggplot(df.PC, aes(x = PC1, y = PC2)) +\n  geom_point(alpha = 0.7, color = \"purple\") +\n  coord_fixed(xlim = c(-10, 10), ylim = c(-5, 5)) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) +\n  labs(\n    title = \"Rotated Data (Principal Component Scores)\",\n    x = \"PC1\",\n    y = \"PC2\"\n  ) +\n  theme_bw()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "ch6/06-PCA.html#pc-plots",
    "href": "ch6/06-PCA.html#pc-plots",
    "title": "6  Principal Component Analysis",
    "section": "6.5 PC Plots",
    "text": "6.5 PC Plots\n\n6.5.1 Biplot\nA biplot is a 2-D plot that shows both observations and variables from a multivariate dataset on the same axes—most commonly the first two principal components (PC1 & PC2). It is a compact way to read:\n\nwhere samples sit relative to each other (scores), and\nhow variables relate to those axes (loadings).\n\n\nlibrary(factoextra)\nfviz_pca(fit,\n         repel=TRUE,\n         labelsize = 1.1, alpha=0.5) + \n  labs(title=\"Biplot of simulated data\") \n\n\n\n\n\n\n\n\n\nThe fviz_pac function produces a ggplot2 graph.\nDim1 and Dim2 are the first two principal component axes.\n\n\n\n6.5.2 Scree Plot\nA scree plot shows the component variations against each component number so that it indicates how much the variations vary across different PCs. The component variations can be obtained via two ways:\n\nIf the function prcomp is used for PCA, the component variation can be obtained via prcomp’s output sdev.\nIf a spectral decomposition is used, the component variations are just eigenvalues of the sample covariance matrix.\n\n\n# use R built-in dataset `mtcars` (engines & performance)\ndat = mtcars[, c(\"mpg\",\"disp\",\"hp\",\"wt\",\"qsec\")]\nfit.mtcars = prcomp(dat)\n\nsummary(fit.mtcars)\n\nImportance of components:\n                            PC1      PC2     PC3     PC4    PC5\nStandard deviation     136.5192 38.12970 3.03933 1.20657 0.2991\nProportion of Variance   0.9271  0.07232 0.00046 0.00007 0.0000\nCumulative Proportion    0.9271  0.99946 0.99992 1.00000 1.0000\n\ndf.scree = data.frame(index=1:length(fit.mtcars$sdev), \n                      var=fit.mtcars$sdev^2)\np1 = ggplot(df.scree, aes(x=index, y=var)) + \n  geom_point() + geom_line() + \n  labs(x=\"Component Number\", \n       y=\"Component Variation\")\ndf.scree$eigenval = eigen(cov(dat))$values\np2 = ggplot(df.scree, aes(x=index, y=eigenval)) + \n  geom_point() + geom_line() + \n  labs(x=\"Component Number\", \n       y=\"Component Variation\")\npatchwork::wrap_plots(p1,p2,ncol=2) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow Many PCs Do We Need?\n\n\n\nIn practice, one can choose the number of PCs based on scree plot, since one can decide the trade-off between the number of PCs and the percentage of variations. This rule is often used in practice.\n\nIf we keep a small number of PCs, we could achieve substantial amoung of dimension reduction in the new datasets (PC scores); at the same time, the percentage of variation preserved might be small (say less than 80\\%), resulting in severe information loss.\nIf we keep a large number of PCs, we could preserve most variations (say more than 95\\%) but there might be no computational gain in terms of dimension reduction.\nA good rule of thumb is to select the number of PCs such that at least 85% of variation is captured.\n\n\n\n\n\n6.5.3 When to Standardize?\n\nStandardize (work with the correlation matrix) when variables are on very different scales,\nor you want to emphasize correlations over raw variances. Otherwise, use the covariance matrix.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "ch6/06-PCA.html#pca-with-covariancecorrelation-matrix",
    "href": "ch6/06-PCA.html#pca-with-covariancecorrelation-matrix",
    "title": "6  Principal Component Analysis",
    "section": "6.6 PCA with Covariance/Correlation Matrix",
    "text": "6.6 PCA with Covariance/Correlation Matrix\nIn many real-world applications, the data might not be directly available to users due to various reasons (e.g., privacy issues, regulatory policy); however, a covariance matrix or correlation matrix might be available. In this case, PCA can still be performed via the R function princomp().\nLet us illustrate princomp() with the iris dataset.\n\n\nR Code: Iris data\n# Let's pretend we only have the covariance matrix \ndata(\"iris\")\nS = cov(iris[,1:4])\n\n\n\n\nR Code: PCA via princomp()\nfit = princomp(covmat=S, cor=FALSE, scores=TRUE)\nsummary(fit)\n\n\nImportance of components:\n                          Comp.1     Comp.2     Comp.3      Comp.4\nStandard deviation     2.0562689 0.49261623 0.27965961 0.154386181\nProportion of Variance 0.9246187 0.05306648 0.01710261 0.005212184\nCumulative Proportion  0.9246187 0.97768521 0.99478782 1.000000000\n\n\n\n\nR Code: PCA loadings\nprint(fit$loadings)\n\n\n\nLoadings:\n             Comp.1 Comp.2 Comp.3 Comp.4\nSepal.Length  0.361  0.657  0.582  0.315\nSepal.Width          0.730 -0.598 -0.320\nPetal.Length  0.857 -0.173        -0.480\nPetal.Width   0.358        -0.546  0.754\n\n               Comp.1 Comp.2 Comp.3 Comp.4\nSS loadings      1.00   1.00   1.00   1.00\nProportion Var   0.25   0.25   0.25   0.25\nCumulative Var   0.25   0.50   0.75   1.00\n\n\n\n\nR Code: Scree diagram\nplot(fit, \n     main=\"Scree diagram\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPC scores are not available\n\n\n\nSince we only pass the covariance/correlation to the R function princomp(), PC scores could not be obtained.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "ch6/06-PCA.html#example-road-race-data",
    "href": "ch6/06-PCA.html#example-road-race-data",
    "title": "6  Principal Component Analysis",
    "section": "6.7 Example: Road Race Data",
    "text": "6.7 Example: Road Race Data\n\n\n\n\n\n\nRoad Race Data\n\n\n\nThis analysis examines scatter plot matrices and computes principal components for the 10k segments of a 100k road race. The data come from Everitt (1994) stored in the file race100k.csv. There is one line of data for each of 80 racers with eleven numbers on each line. The first ten columns give the times (minutes) to complete successive 10k segments of the race. The last column has the racer’s age (in years). Answer the following questions.\n\n\n\ndat = read.csv(\"race100k.csv\")\nhead(dat)\n\n\n  \n\n\n\n\n\n\n\n\n\nStep 1: Data Visualization\n\n\n\nUse the pairs function to create a scatter plot matrix and interpret it. Note that the columns to be included in the plot are put into the variable choose=c(1,2,6,11). The panel.smooth function uses locally weighted regression to pass a smooth curve through each plot. The abline function uses least squares to fit a straight line to each plot. Including the line helps you to see if most of the marginal association between two variables on can be described by a straight line.\n\n\n\n\nR Code: Data Visualization\np1 = ncol(dat) \np = p1 - 1\nn = nrow(dat)\n\nchoose = c(1,2,6,10,11)\npar(pch=1,cex=1.0)\npairs(dat[ ,choose],\n      labels=c(\"0-10k time\", \n               \"10-20k time\",\n               \"50-60k time\", \n               \"90-100k time\",\"age\"),\n      panel=function(x,y){\n        panel.smooth(x,y) \n        abline(lsfit(x,y),lty=2) \n        })\n\n\n\n\n\n\n\n\n\nInterpretations:\n\nThere is a very strong positive correlation between the times to complete the first two 10k legs of the race.\n\nThe correlations of the completion times for the sixth and tenth segments of the race with the first segment of the race are weaker and they appear to be curved trends in the plots.\nSimilar patterns exist for the relationship between the completion times for the second segment of the race with the completion times for the sixth and tenth segments of the race.\nThere is a moderately strong positive correlation between the completion times for the sixth and tenth segments of the race.\nThere is a group of at nine runners who relatively long completion times (running relatively slowly) for the first two legs of the race but had middle completion times for the sixth and tenth legs of the race (average finishers).\n\n\n\n\n\n\n\nStep 2: Perform PCA\n\n\n\nCompute principal components from the covariance matrix.\n\n\n\n\nR Code: PCA\nfit = prcomp(dat[, -p1])\n\n\n\n\nR Code: PC Score Standard Deviations\nfit$sdev\n\n\n [1] 27.123463  9.923923  7.297834  6.102917  5.102212  4.151834  2.834300\n [8]  2.060942  1.547235  1.135819\n\n\n\n\nR Code: PC Directions\nfit$rotation\n\n\n               PC1         PC2        PC3          PC4          PC5         PC6\nX0.10k   0.1287926 -0.21059911 -0.3615464 -0.033543077 -0.147271116  0.20575194\nX10.20k  0.1519795 -0.24907923 -0.4168216 -0.070771273 -0.223835122  0.13094125\nX20.30k  0.1991613 -0.31427990 -0.3411287 -0.053862467 -0.247016251 -0.05256055\nX30.40k  0.2397402 -0.33004401 -0.2026687 -0.006573526 -0.004696149 -0.14386151\nX40.50k  0.3144251 -0.30213368  0.1350869  0.110735209  0.356368957 -0.28455724\nX50.60k  0.4223146 -0.21465890  0.2222736 -0.086787834  0.373032863 -0.29158828\nX60.70k  0.3358642  0.04958843  0.1936251 -0.601557104  0.189706738  0.64355431\nX70.80k  0.4066759  0.00858601  0.5380052  0.128950685 -0.719755126 -0.03482145\nX80.90k  0.3990475  0.26746202 -0.1491748  0.717507174  0.209788222  0.41424585\nX90.100k 0.3853990  0.68882130 -0.3482143 -0.278947530 -0.054501733 -0.40508138\n                 PC7         PC8          PC9         PC10\nX0.10k    0.43236280  0.28021009  0.038988136  0.690088073\nX10.20k   0.32564920  0.22935824  0.046365827 -0.712785245\nX20.30k  -0.34345700 -0.45763871 -0.586752802  0.082695763\nX30.40k  -0.44789166 -0.10450365  0.745122555  0.070880091\nX40.50k  -0.24499887  0.64624348 -0.306079913 -0.005450226\nX50.60k   0.53900089 -0.44941277  0.037927421 -0.022906843\nX60.70k  -0.18441808  0.02193712 -0.019261057 -0.019014886\nX70.80k   0.02932921  0.08174832  0.036551593  0.018002601\nX80.90k  -0.04611157 -0.11324179 -0.002675703 -0.039845245\nX90.100k -0.03004076  0.09451298 -0.002468560  0.032020909\n\n\n\n\n\n\n\n\nStep 3: Interpret PCs\n\n\n\nInterpret the meanings of the first three principal components.\n\n\n\n\nR Code: Heat Map\nload_df &lt;- as.data.frame(fit$rotation) %&gt;%\n  rownames_to_column(\"variable\") %&gt;%\n  pivot_longer(-variable, names_to = \"PC\", values_to = \"loading\") %&gt;%\n  mutate(abs_loading = abs(loading))\n\nK = 3\ntops = load_df %&gt;%\n  group_by(PC) %&gt;%\n  slice_max(abs_loading, n = K, with_ties = FALSE) |&gt;\n  ungroup() %&gt;%\n  mutate(highlight = TRUE)\n\nload_df = load_df %&gt;%\n  left_join(tops %&gt;% dplyr::select(variable, PC, highlight),\n            by = c(\"variable\", \"PC\")) %&gt;%\n  mutate(highlight = ifelse(is.na(highlight), FALSE, TRUE))\n\nload_df = load_df %&gt;%\n  mutate(PC_num = as.integer(str_extract(PC, \"\\\\d+\")),\n         PC = fct_reorder(PC, PC_num, .fun = min))  # PC1, PC2, PC3, ...\n\nggplot(load_df, aes(PC, fct_rev(variable), fill = loading)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient2(\n    low = \"tomato\",\n    mid = \"white\",\n    high = \"steelblue\",\n    midpoint = 0\n  ) +\n  geom_point(\n    data = subset(load_df, highlight),\n    shape = 21,\n    size = 4,\n    stroke = 1.1,\n    color = \"black\",\n    fill = NA\n  ) +\n  labs(\n    x = NULL,\n    y = NULL,\n    fill = \"Loading\",\n    title = paste0(\"PCA loadings (top \", K, \" loading per PC highlighted)\")\n  )  +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\n\n\nR Code: PC Loadings\nL = as.data.frame(fit$rotation[, 1:3]) %&gt;% \n  rownames_to_column(\"segment\")\nL$km = seq(5, 95, by = 10) # midpoints per 10k, adjust as you like\n\nggplot(\n  L %&gt;% \n    tidyr::pivot_longer(\n      starts_with(\"PC\"), \n      names_to = \"PC\", \n      values_to =\"loading\"),\n  aes(km, loading, color = PC)\n) +\n  geom_hline(yintercept = 0, color = \"grey60\") +\n  geom_line(linewidth = 1.1) + geom_point() +\n  labs(x = \"Distance (km)\", y = \"Loading\", \n       title = \"PCA loadings across the 100K race\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nInterpretations: Large values of the data indicates slower than average in a segment. If loadings for early segments are negative and late segments positive, a large positive score comes from a runner who runs fast in early segments and slow in late segments. A negative score has the reverse explanation (slow early, strong finish). We should not focus on the signs of PCs since flipping the sign of a PC reverses the signs of both its loadings and scores.\n\nPC1 indicates an overall performance component with emphasis on the completion times for the last six legs of the race. Runners with small values (large negative values) of this component were the fastest runners overall and runners with large values for this component were the slowest runner overall.\nPC2 indicates that runners with large values for the second component had relatively fast (small) times for the first six legs of the race and relatively slow times for the last two legs of the race. These runners started relatively fast but finished relatively slow. Runners with negative values for this component started relatively slow and finished relatively fast. They conserved energy in the early stages of the race and had something left for a strong finish.\nPC3 indicates that runners with large positive values for the third component were relatively slow for the first four legs of the race, ran relatively fast during the next four legs of the race, and then had a weak finish. Runners with negative values for this component started fast, ran relatively slow in the middle of the race, and finished strong.\n\n\n\n\n\n\n\nStep 4: How Many PCs\n\n\n\nCalculate and report the total variation explained by each principal component, and the accumulative variation explained by the first three PCs.\n\n\n\n\nPCA Fit Summary\nsummary(fit)\n\n\nImportance of components:\n                           PC1    PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     27.1235 9.9239 7.29783 6.10292 5.10221 4.15183 2.83430\nProportion of Variance  0.7477 0.1001 0.05413 0.03785 0.02646 0.01752 0.00816\nCumulative Proportion   0.7477 0.8478 0.90194 0.93980 0.96625 0.98377 0.99194\n                           PC8     PC9    PC10\nStandard deviation     2.06094 1.54723 1.13582\nProportion of Variance 0.00432 0.00243 0.00131\nCumulative Proportion  0.99626 0.99869 1.00000\n\n\n\n\nScree Plot\nplot(fit$sdev^2, \n     xlab=\"Component Number\",\n     ylab=\"Component Variance (eigenvalue)\",\n    main=\"Scree Diagram\", type=\"b\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 5: Visualize PC Scores\n\n\n\nPlot component scores for the first three principal components.\n\n\n\n\nPC Scores\npar(pch=1, cex=1)\npairs(fit$x[,c(1,2,3)], labels=c(\"PC1\",\"PC2\",\"PC3\"))\n\n\n\n\n\n\n\n\n\nPC Scores\n# or use biplots\ng1 = factoextra::fviz_pca(fit,\n         axes = c(1,2), \n         repel=TRUE,\n         labelsize = 1.1, alpha=0.5) \ng2 = factoextra::fviz_pca(fit,\n         axes = c(1,3), \n         repel=TRUE,\n         labelsize = 1.1, alpha=0.5) \ng3 = factoextra::fviz_pca(fit,\n         axes = c(2,3), \n         repel=TRUE,\n         labelsize = 1.1, alpha=0.5) \npatchwork::wrap_plots(g1,g2,g3,ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptional: Perform PCA on Standardized Data\n\n\n\nThis step is optional as all measurements have scales quite similar.\n\n\n\n\nStandardization and Visualization\ndat_sd = scale(dat, center=T, scale=T)\n   \nchoose = c(1,2,5,10,11)\n\npairs(\n  dat_sd[, choose],\n  labels = c(\"0-10k time\", \n             \"10-20k time\", \n             \"50-60k time\", \n             \"90-100k time\", \n             \"age\"),\n  panel = function(x, y) {\n    panel.smooth(x, y)\n    abline(lsfit(x, y), lty = 2)\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\nCorrelation Matrix\nracecor = var(dat_sd)\nracecor\n\n\n            X0.10k   X10.20k    X20.30k    X30.40k     X40.50k     X50.60k\nX0.10k   1.0000000 0.9510603 0.84458736 0.78585596  0.62053457  0.61789171\nX10.20k  0.9510603 1.0000000 0.89031061 0.82612495  0.64144268  0.63276548\nX20.30k  0.8445874 0.8903106 1.00000000 0.92108596  0.75594631  0.72509902\nX30.40k  0.7858560 0.8261249 0.92108596 1.00000000  0.88690905  0.84185641\nX40.50k  0.6205346 0.6414427 0.75594631 0.88690905  1.00000000  0.93641488\nX50.60k  0.6178917 0.6327655 0.72509902 0.84185641  0.93641488  1.00000000\nX60.70k  0.5313965 0.5409319 0.60502621 0.69065419  0.75419742  0.83957633\nX70.80k  0.4773723 0.5054520 0.61998205 0.69821518  0.78578147  0.84032251\nX80.90k  0.5423438 0.5338073 0.58357645 0.66735326  0.74134973  0.77257354\nX90.100k 0.4142609 0.4381283 0.46725334 0.50857719  0.54174220  0.65591894\nage      0.1491725 0.1271041 0.01218286 0.04680206 -0.01607529 -0.04241971\n             X60.70k    X70.80k    X80.90k   X90.100k         age\nX0.10k    0.53139648  0.4773723  0.5423438  0.4142609  0.14917250\nX10.20k   0.54093190  0.5054520  0.5338073  0.4381283  0.12710409\nX20.30k   0.60502621  0.6199821  0.5835765  0.4672533  0.01218286\nX30.40k   0.69065419  0.6982152  0.6673533  0.5085772  0.04680206\nX40.50k   0.75419742  0.7857815  0.7413497  0.5417422 -0.01607529\nX50.60k   0.83957633  0.8403225  0.7725735  0.6559189 -0.04241971\nX60.70k   1.00000000  0.7796014  0.6972448  0.7191956 -0.04059097\nX70.80k   0.77960144  1.0000000  0.7637562  0.6634709 -0.20674428\nX80.90k   0.69724482  0.7637562  1.0000000  0.7797619 -0.12320048\nX90.100k  0.71919560  0.6634709  0.7797619  1.0000000 -0.11289354\nage      -0.04059097 -0.2067443 -0.1232005 -0.1128935  1.00000000\n\n\n\n\nPCA on Standardized Data\nfit1 = prcomp(dat_sd[,-p1])\nfit1$sdev\n\n\n [1] 2.6912189 1.1331038 0.7439637 0.5451001 0.4536530 0.4279130 0.3300239\n [8] 0.2204875 0.1984028 0.1923427\n\n\nPCA on Standardized Data\nplot(\n  fit1$sdev^2,\n  xlab = \"Component Number\",\n  ylab = \"Component Variance (eigenvalue)\",\n  main = \"Scree Diagram\",\n  type = \"b\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 6: Follow-Up Findings\n\n\n\nUse the principal component scores from the raw data to look for differences among mature (age &lt; 40) and senior (age &gt; 40) runners. Mature runners will be indicated by M and senior runners will be indicated by S. Can we separate those two groups by using the first two principal component scores?\n\n\n\n\nPC Scores by Age Group\nrace.type &lt;- rep(\"M\", n)\nrace.type[dat[, p1] &gt;= 40] &lt;- \"S\"\nrace.col &lt;- rep(\"red\", n)\nrace.col[dat[, p1] &gt;= 40] &lt;- \"blue\"\n\nplot(\n  fit$x[, 1],\n  fit$x[, 2],\n  xlab = \"PC1: Overall speed\",\n  ylab = \"PC2: Change in speed \",\n  type = \"n\"\n)\ntext(\n  fit$x[, 1],\n  fit$x[, 2],\n  labels = race.type,\n  cex = 0.9,\n  col = race.col\n)\n\n\n\n\n\n\n\n\n\nInterpretations:\n\nThe four fastest runners were mature runners who were consistently fast across all 10 segments of the race. The mature runners have the most variability in scores for the second principal component, and they have a greater tendency to start fast and finish slow than the senior runners.\nOne interesting further statistical analysis would be using hypothesis testing to compare the two groups (mature and senior) based on the PC scores.\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\n\nThe above question provides one way to extract information from the data. In practice, we often need to define our own questions related to the dataset and perform statistical analyses.\nLooking backwarks, if one is given the dataset without providing the research question, how can one formulate interesting research questions for the dataset and address them via PCA and multivariate statistical methods?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "ch6/06-PCA.html#exercises",
    "href": "ch6/06-PCA.html#exercises",
    "title": "6  Principal Component Analysis",
    "section": "6.8 Exercises",
    "text": "6.8 Exercises\n\n6.8.1 Exercise 1: mtcars Data\nLet us use the mtcars dataset to perform PCA and interpret the results.\n\ndat = mtcars[, c(\"mpg\",\"disp\",\"hp\",\"wt\",\"qsec\")]\nhead(dat)\n\n\n  \n\n\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nR Code: Scatterplot Matrix\nGGally::ggpairs(dat)\n\n\n\n\n\n\n\n\n\nAs the scatterplot indicates that the variables are on very different scales, it is helpful to perform PCA on standardized data.\n\n\nR Code: PCA via prcomp\nfit = prcomp(dat, center=TRUE, scale=TRUE)\nsummary(fit)\n\n\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5\nStandard deviation     1.9227 0.9803 0.39310 0.36215 0.23764\nProportion of Variance 0.7394 0.1922 0.03091 0.02623 0.01129\nCumulative Proportion  0.7394 0.9316 0.96247 0.98871 1.00000\n\n\n\n\nR Code: Scree Plot\nplot(fit$sdev^2, xlab=\"Component Index\", ylab=\"Component Variation\")\n\n\n\n\n\n\n\n\n\n\n\nR Code: Biplot\nfactoextra::fviz_pca_biplot(fit,\n    repel=TRUE,\n    labelsize = 1.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.8.2 Exercise 2: PCA for Image Compression\nAn image is a matrix of pixels with values that represent the intensity of the pixel, where 0=white and 1=black. The jpeg format image has three channels: red, green, blue. If the image size is too large or for regulatory or privacy issues, one is often interested in compressing the image size and working with a low-resolution image. This task can be achieved using PCA introduced in this chapter. Here you can use any image for this task, but I will use an image of the Great Wall.\n\nLoad the image into R and extract the green channel.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\nlibrary(jpeg)\n#the jpeg has 3 channels: red, green, blue\n#for simplicity of the example, I am only\n#reading the green channel\ndat &lt;- readJPEG(\"./figures/greatwall.jpg\")[,,2]\n#You can have a look at the image\nplot(1:2, type='n', axes=F, ann=F)\nrasterImage(dat, 1, 2, 2, 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck the dimension of the image and plot heatmap of the correlation matrix using the function heatmap.2() from the R package gplots.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\ndim(dat)\n\n\n[1] 750 750\n\n\n\n\nCode\nlibrary(gplots)\n# compute correlations in terms of red, yellow and darkgreen\ncol.correlation &lt;- colorRampPalette(\n  c(\"red\",\"yellow\",\"darkgreen\"), \n  space = \"rgb\")(30)\n\n# This could be time consuming if image dimension is too large\ngplots::heatmap.2(cor(dat),\n          Rowv = F, Colv = F,\n          dendrogram = \"none\",\n          trace=\"none\",\n          col=col.correlation)\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerform PCA on the correlation matrix of the image and plot the scree diagram.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\n# PCA\ndat.pca = prcomp(dat, center=FALSE)\nplot(dat.pca$sdev[1:20]^2, xlab=\"Component Number\",\n     ylab=\"Component Variance (eigenvalue)\",\n     main=\"Scree Diagram\", type=\"b\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecover the image using the first few PCs.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\n#The intensities given by the first m components\nm = 10\ndat.pca2 &lt;- dat.pca$x[,1:m] %*% t(dat.pca$rotation[,1:m])\ndat.pca2[dat.pca2&gt;1] &lt;-1\ndat.pca2[dat.pca2&lt;0] &lt;-0\n\n#You can have a look at the image\npar(mfrow=c(1,2))\nplot(1:2, type='n', axes=F, ann=F)\ntitle (\"Original image\")\nrasterImage(dat, 1, 2, 2, 1)\n\nplot(1:2, type='n', axes=F, ann=F)\ntitle(paste0(\"PCA constructed image \\n with \", m, \" components\"))\nrasterImage(dat.pca2, 1, 2, 2, 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompression for the colored image with three channels simultaneously. Plot the PCA constructed image using 50 principal components. You could try different number of principal components and compare their differences.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\ndat = readJPEG(\"./figures/greatwall.jpg\")\n\n# dat is now a list with three elements \n#corresponding to the channels RBG\n#we will do PCA in each element\ndat.rbg.pca&lt;- apply(dat, 3, prcomp, center = FALSE) \n\n\n\n\nCode\n#Computes the intensities using m components\nm = 50\ndat.pca2 &lt;- lapply(dat.rbg.pca, \n                   function(channel.pca) {\n                      jcomp = channel.pca$x[,1:m] %*% t(channel.pca$rotation[,1:m])\n                      jcomp[jcomp&gt;1] &lt;-1\n                      jcomp[jcomp&lt;0] &lt;-0\n                      return(jcomp)}\n                   )\n\n#Transforms the above list into an array\ndat.pca2&lt;-array(as.numeric(unlist(dat.pca2)), \n               dim=dim(dat))\n\n#You can have a look at the image\npar(mfrow=c(1,2))\nplot(1:2, type='n', axes=F, ann=F)\ntitle (\"Original image\")\nrasterImage(dat, 1, 2, 2, 1)\nplot(1:2, type='n', axes=F, ann=F)\ntitle (paste0(\"PCA constructed image \\n with \", m, \" components\"))\nrasterImage(dat.pca2, 1, 2, 2, 1)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "ch7/07-FactorAnalysis.html",
    "href": "ch7/07-FactorAnalysis.html",
    "title": "7  Factor Analysis",
    "section": "",
    "text": "7.1 Why Factor Analysis?\nImagine you’re a psychologist who has just created a new survey to measure “student engagement.” You’ve written 10 different questions, like\nAfter collecting responses, you notice that students who answer “often” to one question tend to answer similarly to a few others. It seems like your questions aren’t measuring 10 completely different things, but rather, they are tapping into a few underlying, unobservable traits.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "ch7/07-FactorAnalysis.html#why-factor-analysis",
    "href": "ch7/07-FactorAnalysis.html#why-factor-analysis",
    "title": "7  Factor Analysis",
    "section": "",
    "text": "“How often do you participate in class discussions?”,\n“How much time do you spend studying?”,\nand “Do you find your courses interesting?”\n\n\n\nThis is the exact scenario where you would use Exploratory Factor Analysis.\nFactor analysis is a statistical method used to uncover the latent structure (or “factors”) from a set of variables. There are two types of factor analysis: exploratory factor analysis (EFA) and confirmatory factor analysis. This course will focus on EFA.\n\n\n7.1.1 Factor Analysis v.s. PCA: What’s the Difference?\n\nPCA is a variance-focused technique with the goal to reduce dimensionality by creating a smaller set of components that capture maximum amount of total variance in the original data.\nFactor analysis is a covariance-focused technique with the goal to explain the shared variance (covariance) among the original variables by modeling the underlying latent factors.\n\n\n\n7.1.2 Spearman’s Exam Marks\nSuppose we observe three exam scores for each student: Classics (X_1), French (X_2), and English (X_3). Researchers collect exam scores from many different students across these subjects and would like to investigate if what could affect students’ performances.\n\n\n\n\n\n\nResearch Question\n\n\n\n\n\nIs there a single latent factor that drives students’ performance across different subjects?\n\n\n\n\n\n\n\n\n\nExperimental Data\n\n\n\n\n\nHistorical reports give approximate correlations as follows:\n\nR &lt;- matrix(c(1.00, 0.83, 0.78,\n              0.83, 1.00, 0.67,\n              0.78, 0.67, 1.00),\n            nrow = 3, byrow = TRUE,\n            dimnames = list(c(\"Classics\",\"French\",\"English\"),\n                            c(\"Classics\",\"French\",\"English\")))\nR\n\n         Classics French English\nClassics     1.00   0.83    0.78\nFrench       0.83   1.00    0.67\nEnglish      0.78   0.67    1.00\n\n\nInterpretation: The variables are strongly correlated—suggesting a single, latent “general ability” factor F that drives performance across subjects.\n\n\n\n\n\n\n\n\n\nAn Orthogonal One-Factor Model\n\n\n\n\nLet \\mathbf{z} = (z_1,\\ldots,z_p)^\\top be standardized variable of \\mathbf{x}.\nLet R=\\text{Var}(z) denote the p\\times p correlation matrix. An orthogonal one-factor model has the form \\begin{equation*}\n\\mathbf{z} = L f + \\boldsymbol \\varepsilon,\\\\\n\\text{Var}(\\varepsilon)=\\boldsymbol \\Psi=\\text{diag}\\{\\psi_1,\\ldots,\\psi_p\\},\\quad\nR = LL^\\top + \\boldsymbol \\Psi.\n\\end{equation*}\nf is called the common factor assumed with mean E(f)=0 and variance \\text{Var}(f)=1.\nL=[\\ell_1, \\ldots, \\ell_p]^\\top is the p\\times 1 matrix of factor loadings.\n\\boldsymbol\\varepsilon is a p-dimensional vector of measurement errors (also called specific factor).\nh_i^2:= \\ell_{i}^2 is called the communality of X_i and represents the variance explained by the factor f.\n\\psi_i=1-h_i^2 is called the uniqueness.\n\n\n\nGiven the correlation matrix only, we can use it to fit the one-factor model via R function factanal.\n\nfit1 = factanal(factors = 1, covmat = R)\nstr(fit1)\n\nList of 10\n $ converged   : logi TRUE\n $ loadings    : 'loadings' num [1:3, 1] 0.983 0.844 0.793\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:3] \"Classics\" \"French\" \"English\"\n  .. ..$ : chr \"Factor1\"\n $ uniquenesses: Named num [1:3] 0.0337 0.2871 0.3704\n  ..- attr(*, \"names\")= chr [1:3] \"Classics\" \"French\" \"English\"\n $ correlation : num [1:3, 1:3] 1 0.83 0.78 0.83 1 0.67 0.78 0.67 1\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:3] \"Classics\" \"French\" \"English\"\n  .. ..$ : chr [1:3] \"Classics\" \"French\" \"English\"\n $ criteria    : Named num [1:3] 4.26e-12 1.20e+01 1.20e+01\n  ..- attr(*, \"names\")= chr [1:3] \"objective\" \"counts.function\" \"counts.gradient\"\n $ factors     : num 1\n $ dof         : num 0\n $ method      : chr \"mle\"\n $ n.obs       : logi NA\n $ call        : language factanal(factors = 1, covmat = R)\n - attr(*, \"class\")= chr \"factanal\"\n\n\nOutput interpretation:\n\nloadings: a matrix of loadings with one column for each factor. The factors are in decreasing order of communality.\n\nuniquenesses: a vector of estimated measurement error variances.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "ch7/07-FactorAnalysis.html#the-exploratory-factor-model",
    "href": "ch7/07-FactorAnalysis.html#the-exploratory-factor-model",
    "title": "7  Factor Analysis",
    "section": "7.2 The Exploratory Factor Model",
    "text": "7.2 The Exploratory Factor Model\n\n\n\n\n\n\nExample: bfi Data\n\n\n\nBackground: A classroom/teaching dataset for personality measurement under the Big Five model: Agreeableness (A), Conscientiousness (C), Extraversion (E), Neuroticism (N), and Openness (O). The bfi data is collected via the SAPA Project and is available from the R package psych.\n\n25 items: 5 per trait, named A1-A5, C1-C5, E1-E5, N1-N5, O1-O5.\nResponses: 6-point Likert (1=Very Inaccurate, …, 6=Very Accurate).\nDemographics: gender, education, age\nSample size: 2800, but with some missing values.\n\n\nlibrary(psych)\ndata(bfi, package=\"psych\")\ndim(bfi)\n\n[1] 2800   28\n\nhead(bfi)\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nResearch Questions\n\n\n\n\n\n\nWhat are the underlying latent structures measured by the set of survey items?\nDo the latent structures show any differences across demographic variables (e.g., gender, education, and age group)?\n\n\n\n\n\n\n\n\n\n\n\nThe Factor Model\n\n\n\n\nLet \\mathbf{z}_i = (z_{i1},\\ldots,z_{ip})^\\top denote the vector of p=25 standardized variables for the i-th observation.\nAssume that there are m common factors denoted by the m-dimensional vector \\mathbf{f}_i for the i-th observation.\nThe m-factor model is \n\\mathbf{z}_{i} = L \\mathbf{f}_i + \\boldsymbol \\varepsilon_i, i=1,\\ldots, n\n\n\nL is the p\\times m matrix of factor loadings with (i,j)th entry \\ell_{ij}.\n\\mathbf{f}_i is the m-dimensional vector of common factors.\n\\boldsymbol \\varepsilon_i is the p-dimensional vector of specific factors.\n\nIn matrix notation, stacking all observations together yields \nZ = F L^\\top + \\boldsymbol \\varepsilon\n\n\nZ=[\\mathbf{x}_1, \\ldots, \\mathbf{x}_n]^\\top is the n\\times p matrix of standardized data;\nF=[\\mathbf{f}_1, \\ldots, \\mathbf{f}_n]^\\top is the n\\times m matrix of common factors for all observations.\n\\boldsymbol \\varepsilon=[\\boldsymbol \\varepsilon_1, \\ldots, \\boldsymbol \\varepsilon_n]^\\top is the n\\times p of specific factors for all observations.\n\nThe communality of X_i is \nh_i^2: = \\ell_{i1}^2 + \\ell_{i2}^2 + \\cdots + \\ell_{im}^2\n\n\nit measures the variation explained by the m common factors;\n\nThe proportion of the total variance explained by the jth factor is given by \n\\frac{\\sum_{i=1}^p \\ell_{ij}^2}{\\text{trace}(R)}\n\n\n\n\n\n\n\n\n\n\nAssumptions\n\n\n\n\nE(F) = \\mathbf{0}, \\text{Var}(F) = I_{m\\times m}.\nE(\\boldsymbol \\varepsilon) = \\mathbf{0}, \\text{Var}(\\boldsymbol \\varepsilon) = \\boldsymbol \\Psi = \\text{diag}\\{\\psi_1, \\ldots, \\psi_p\\}\nF and \\boldsymbol \\varepsilon are independent.\n\n\n\nStep: Data Preparation and Visualization\n\n\nR Code: Prepare Data\n# select the 25 personality items (columns 1-25)\ndf = bfi %&gt;% drop_na()\ndat = df[, 1:25]\nhead(dat)\n\n\n\n  \n\n\n\n\n\nR Code: Visualization\ncorrplot::corrplot(cor(dat))",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "ch7/07-FactorAnalysis.html#measures-of-sampling-adequacy",
    "href": "ch7/07-FactorAnalysis.html#measures-of-sampling-adequacy",
    "title": "7  Factor Analysis",
    "section": "7.3 Measures of Sampling Adequacy",
    "text": "7.3 Measures of Sampling Adequacy\nStep: Check Assumptions\nBefore performing factor analysis, we need to ensure our data is suitable. Two common methods are introduced below.\n\n\n\n\n\n\nMeasure of Sampling Adequacy (MSA)\n\n\n\nThe first method is measure of sampling adequacy (MSA). MSA (Kaiser 1970, Psychometrica) is a statistic that measures the relative sizes of the pairwise correlations to the partial correlations between all pairs of variables: \nMSA = 1- \\frac{\\sum_j \\sum_{k \\neq j} q_{jk}^2}{\\sum_j \\sum_{k \\neq j} r_{jk}^2 }\n\n\nr_{jk} is the marginal sample correlation between variables j and k;\nq_{jk} is the partial correlation between the two variables after accounting for all other variables in the data;\nMSA indicates the proportion of relationships in the data that is shared and common among groups of variables\n\nA high MSA (close to 1) suggests that there are underlying factors influencing groups of variables and the dataset is suitable for factor analysis.\nA low MSA (close to 0) suggestions that most of the correlations are just unique, one-on-one relationships that cannot be well explained by common factors.\n\nThe MSA can take on values between 0 and 1. Kaiser proposed the following guidelines for interpretation:\n\n\n\nMSA Range\nInterpretation\n\n\n\n\n0.9 to 1.0\nMarvelous\n\n\n0.8 to 0.9\nMeritorious\n\n\n0.7 to 0.8\nMiddling\n\n\n0.6 to 0.7\nMediocre\n\n\n0.5 to 0.6\nMiserable\n\n\n0.0 to 0.5\nUnacceptable\n\n\n\n\n\n\n\nR Code: MSA\n# compute measures of sampling adequacy (MSA)\npsych::KMO(dat) \n\n\nKaiser-Meyer-Olkin factor adequacy\nCall: psych::KMO(r = dat)\nOverall MSA =  0.85\nMSA for each item = \n  A1   A2   A3   A4   A5   C1   C2   C3   C4   C5   E1   E2   E3   E4   E5   N1 \n0.74 0.83 0.87 0.87 0.90 0.84 0.79 0.85 0.82 0.86 0.84 0.88 0.89 0.88 0.89 0.78 \n  N2   N3   N4   N5   O1   O2   O3   O4   O5 \n0.78 0.86 0.89 0.86 0.86 0.78 0.83 0.78 0.76 \n\n\nInterpretation: The overall MSA is 0.85, which is “meritorious.” This confirms our data is appropriate for factor analysis.\n\n\n\n\n\n\nCronbach’s Alpha\n\n\n\nThe second method is called Cronbach’s alpha. For n samples of p-dimensional observations \\mathbf{X}=(X_1, \\ldots, X_p)^\\top, Cronbach’s alpha is \n\\alpha = \\frac{p \\bar{r}}{1+(p-1)\\bar{r}}\n\n\n\\bar{r} is the average correlation: \n\\bar{r} =  \\frac{\\frac{1}{\\frac{p(p-1)}{2}} \\sum \\sum_{i&lt; j} \\text{Cov}(X_i, X_j)}{\\frac{1}{p}\\sum_{i=1}^p \\text{Var}(X_i)}\n\nA high alpha value (close to 1) suggests that the items are all measuring the same underlying latent factor.\nA low alpha value (close to 0) suggests that the items may be measuring different things.\n\n\n\n\n\nR Code: Cronbach’s Alpha\nsummary(psych::alpha(dat, check.keys=TRUE))\n\n\nWarning in psych::alpha(dat, check.keys = TRUE): Some items were negatively correlated with the first principal component and were automatically reversed.\n This is indicated by a negative sign for the variable name.\n\n\n\nReliability analysis   \n raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r\n      0.82      0.82    0.86      0.15 4.6 0.0054  4.2 0.61     0.13",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "ch7/07-FactorAnalysis.html#how-many-factors-should-be-used",
    "href": "ch7/07-FactorAnalysis.html#how-many-factors-should-be-used",
    "title": "7  Factor Analysis",
    "section": "7.4 How Many Factors Should Be Used?",
    "text": "7.4 How Many Factors Should Be Used?\nThis is one of the most crucial and perhaps subjective steps. We will introduce two common methods:\n\nThe first is to decide the number of factors, m, prior to the analysis using the idea from PCA.\nThe second is to use a formal likelihood ratio test.\n\n\n7.4.1 Scree Plot for Choosing the Number of Factors\nPrior to the analysis, one often needs to decide how many factors should be used. A good exploratory way is to use scree plot to decide m so that the contribution of each potential factor to the total variation is examined.\n\nScree Plot: We look for the “elbow” or point of inflection in the plot of eigenvalues of correlation matrix.\n\n\n\nR Code: Scree Plot\neigenvalues = eigen(cor(dat))$values \nscree_data &lt;- data.frame(\n  Factor = 1:length(eigenvalues),\n  Eigenvalue = eigenvalues\n)\n\ncumsum(eigenvalues)/sum(eigenvalues)\n\n\n [1] 0.2027406 0.3132398 0.3993447 0.4750381 0.5357394 0.5788925 0.6121288\n [8] 0.6443088 0.6728723 0.7009338 0.7281675 0.7541265 0.7793767 0.8028980\n[15] 0.8255366 0.8473302 0.8681275 0.8878823 0.9071917 0.9248917 0.9420466\n[22] 0.9583305 0.9738855 0.9892760 1.0000000\n\n\nR Code: Scree Plot\nggplot(scree_data, aes(x = Factor, y = Eigenvalue)) +\n  geom_point() +\n  geom_line() +\n  ggtitle(\"Scree Plot\") +\n  xlab(\"Factor\") + \n  ylab(\"Eigenvalue\")\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nAs seen from the scree pot, the cumulative eigenvalues do not increase too much and “elbow” point seem to occur at either 5 or 6; so we might want to try the factor model with m=6 factors.\nThe scree plot often gives somewhat subjective answers, so we might also try different number of factors, fit the factor analysis model for each selection and compare the results among all the choices.\n\n\n\n7.4.2 Likelihood Ratio Test\nA formal way to choose the number of factors is to use a likelihood ratio test.\n\n\n\n\n\n\nLikelihood Ratio Test (LRT)\n\n\n\n\nWe wish to test whether the m factor model appropriately describes the correlations among the p variables.\nNull hypothesis: m factors are sufficient, i.e, \nH_0: R_{p \\times p} = L_{p \\times m}L'_{m \\times p} + \\Psi_{p \\times p}\n\nAlternative hypothesis: the correlation matrix can be any positive definite matrix  H_a: R_{p \\times p}   \\text{ is a positive definite matrix} \nThe test statistic is \n         -2\\ln \\Lambda := \\{n - 1 - (2p+4m+5)/6\\} \\log \\frac{|\\hat{L}\\hat{L}' + \\hat{\\Psi}|}{|\\hat{R}|},\n where \\hat{L}, \\hat{\\Psi} and \\hat{R} denote the their estimates from the data.\n-2\\ln \\Lambda has an approximate \\chi^2 distribution under the null hypothesis with degrees of freedom \\text{df}=[(p - m)^2 - p - m]/2.\nTo have df &gt;0, we must have m &lt; \\frac{1}{2}(2p + 1 - \\sqrt{8p + 1}).\nWe reject H_0 at level \\alpha if\n\n[n - 1 - (2p+4m+5)/6] \\log \\left( \\frac{|{L}{L}' + {\\Psi}|}{|{R}|} \\right) &gt; \\chi^2_{\\text{df}, 1-\\alpha}\n\nThis test has two major drawbacks:\n\nIt is very sensitive to sample size n. With large sample sizes, the LRT tends to suggest that large number of factors be include.\nIt replies on the assumption of multivariate normality. If the assumption is violated, the LRT also tends to indicate the need for too many factors.\n\nIn practice, this LRT test should only be used as a guideline for selecting the number of factors, and we should also other things such as the interpretation of the factors, proportion of variance explained, and the desire for simplicity.\n\n\n\n\n\nR Code: Multivariate Normality Test\nmvShapiroTest::mvShapiro.Test(as.matrix(dat))\n\n\n\n    Generalized Shapiro-Wilk test for Multivariate Normality by\n    Villasenor-Alva and Gonzalez-Estrada\n\ndata:  as.matrix(dat)\nMVW = 0.95906, p-value &lt; 2.2e-16\n\n\n\n\nR Code: Likelihood Ratio Test\n# compute p-values for different number of factors\nsapply(1:15, function(f) \nfactanal(dat, factors = f, method =\"mle\")$PVAL)\n\n\n    objective     objective     objective     objective     objective \n 0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00 1.881649e-177 \n    objective     objective     objective     objective     objective \n 1.411671e-85  4.776750e-51  3.708668e-31  4.993573e-19  5.950411e-11 \n    objective     objective     objective     objective     objective \n 3.479191e-05  7.032046e-03  1.427903e-01  1.500403e-01  6.859678e-01 \n\n\nInterpretation: Since the data fails the Shapiro Wilk’s test, the multivariate normality is violated. As expected, the LRT indicates at least 14 factors to be included with p-value &gt; 0.2, which are too many to be included in the factor analysis model. Based on the result from scree plot, we will use 6 factors in the follow-up analysis.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "ch7/07-FactorAnalysis.html#estimation-for-factor-loadings",
    "href": "ch7/07-FactorAnalysis.html#estimation-for-factor-loadings",
    "title": "7  Factor Analysis",
    "section": "7.5 Estimation for Factor Loadings",
    "text": "7.5 Estimation for Factor Loadings\nIn the factor analysis model, we often first estimate the factor loadings L, and then estimate the factor scores F given L. To estimate the factor loadings, three methods are introduced below.\n\nThe principal component method\nThe iterative principal factor method\nMaximum likelihood estimation (assumes multivariate normality)\n\n\n7.5.1 The Principal Component Method\nThe basic idea of the principal component method is to approximate the correlation matrix using a rank-m approximation via eigenvalues and eigenvectors.\n\nLet (\\lambda_i, \\mathbf{e}_i) denote the eigenpairs of the p\\times p correlation matrix R.\nThe rank-m approximation to R is \n\\hat{R}^{(m)} : = \\lambda_1 \\mathbf{e}_1 \\mathbf{e}_1^\\top + \\ldots + \\lambda_m \\mathbf{e}_m \\mathbf{e}_m^\\top.  \n\nThe columns of L is given by the columns \\sqrt{\\lambda_1} \\mathbf{e}_1, \\ldots, \\sqrt{\\lambda_m}\\mathbf{e}_m.\n\\Psi is estimated by \\hat{\\Psi} = \\text{diag}(R - LL^\\top).\n\n\n\nR Code: Estimation via the PC Method\nfit.pc = prcomp(dat, center=TRUE, scale=TRUE)\n\nplot(fit.pc$sdev^2, xlab=\"Component Number\",\n        ylab=\"Component Variance (eigenvalue)\",  \n        main=\"Scree Diagram\", type=\"b\")\n\n\n\n\n\n\n\n\n\n\n\nR Code: Factor Loadings\nm = 6\nfit.pc$loadings = fit.pc$rotation %*% diag(fit.pc$sdev)\ncorrplot::corrplot(fit.pc$loadings[, 1:m])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Factor Loadings\n\n\n\n\n\n\n\nWriting an R Function\nplt_loadings = function(loadings, K = 3,\n  with_ties = FALSE, angle=30,\n  palette = c(\"tomato\", \"white\", \"steelblue\")) \n{\n  # Required packages\n  require(dplyr)\n  require(tidyr)\n  require(tibble)\n  require(stringr)\n  require(forcats)\n  require(ggplot2)\n\n  # Ensure matrix form\n  #L = as.matrix(loadings)\n  L = suppressWarnings(as.matrix(unclass(loadings)))\n  \n  # Default names\n  if (is.null(rownames(L))) \n    rownames(L) = paste0(\"var\", seq_len(nrow(L)))\n  if (is.null(colnames(L))) \n    colnames(L) = paste0(\"Factor\", seq_len(ncol(L)))\n  \n  var_levels = rownames(L)\n\n  # Build long-format data frame\n  load_df &lt;- as.data.frame(L) %&gt;%\n    tibble::rownames_to_column(\"variable\") %&gt;%\n    pivot_longer(-variable, \n                        names_to = \"Factor\", \n                        values_to = \"loading\") %&gt;%\n    mutate(abs_loading = abs(loading))\n\n  # Select top-K per Factor\n  tops = load_df %&gt;%\n    group_by(Factor) %&gt;%\n    slice_max(abs_loading, n = K, \n                     with_ties = with_ties) %&gt;%\n    ungroup() %&gt;%\n    mutate(highlight = TRUE)\n\n  # Mark highlighted entries\n  load_df = load_df %&gt;%\n    left_join(\n      tops %&gt;% dplyr::select(variable, Factor, highlight),\n      by = c(\"variable\", \"Factor\")\n    ) %&gt;%\n    mutate(\n      highlight = ifelse(is.na(highlight), \n                         FALSE, TRUE))\n\n  # Reorder factors numerically \n  # if they are like \"Factor 1\", \"Factor 2\", ...\n  load_df = load_df %&gt;%\n    mutate(\n      PC_num = \n          as.integer(stringr::str_extract(Factor, \"\\\\d+\")),\n      PC = if (all(!is.na(PC_num))) {\n        forcats::fct_reorder(Factor, PC_num, .fun = min)\n      } else {\n        factor(Factor, levels = rev(var_levels))\n      }\n    )\n\n  # Plot\n  p = ggplot(load_df, \n    aes(x = PC, y = fct_rev(variable), fill = loading)) +\n    geom_tile(color = \"white\") +\n    scale_fill_gradient2(\n      low = palette[1], \n      mid = palette[2], \n      high = palette[3], \n      midpoint = 0,\n      name = \"Loading\"\n    ) +\n    geom_point(\n      data = subset(load_df, highlight),\n      shape = 21, size = 4, stroke = 1.1, \n      color = \"black\", fill = NA\n    ) +\n    labs(\n      x = NULL, y = NULL,\n      title = \n        paste0(\"Top \", \n               K, \" loadings per column highlighted\")\n    ) +\n    theme_minimal(base_size = 12) + \n    theme(\n      axis.text.x = \n        element_text(\n          face = \"bold\",\n          angle = angle, \n          vjust = 0.5, \n          hjust = 1)\n      )\n\n  print(p)\n}\n\n\n\n\n\n\nplt_loadings(fit.pc$loadings[, 1:m], K=4)\n\n\n\n\n\n\n\n\n\n\n7.5.2 The Principal Factor Method\nThe principal factor method is an iterative modification of the principal components method that allows for greater focus on explaining correlations among observed traits.\n\nThe general algorithm is as follows:\n\n\\Psi is estimated via the PC method above.\nL is estimated to approximate R - \\Psi.\nGiven L, \\Psi is estimated.\nRepeat Step 2-3 until convergence.\n\nThere are two options during estimation:\n\nHEYWOOD: Set any estimated communality larger than one equal to 1 and contiute iterations with the remaining variables.\nULTRAHEYWOOD: Continue iterations with all of the variables and hope that iterations eventually give allowable parameter estimates. (Doing nothing)\n\n\n\n\n\n\n\n\nfactanal v.s. fn\n\n\n\nIn R, both functions factanal and fa (from the psych package) implement factor analysis, but they use different estimation methods. The principal factor method is available in fa but not factanal.\n\n\n\n\nR Code: Principal Factor Method\nlibrary(psych)\nfit.pa = psych::fa(dat, nfactors=5, fm=\"pa\", rotate=\"none\")\nsummary(fit.pa)\n\n\n\nFactor analysis with Call: psych::fa(r = dat, nfactors = 5, rotate = \"none\", fm = \"pa\")\n\nTest of the hypothesis that 5 factors are sufficient.\nThe degrees of freedom for the model is 185  and the objective function was  0.63 \nThe number of observations was  2236  with Chi Square =  1400.57  with prob &lt;  1.5e-185 \n\nThe root mean square of the residuals (RMSA) is  0.03 \nThe df corrected root mean square of the residuals is  0.04 \n\nTucker Lewis Index of factoring reliability =  0.878\nRMSEA index =  0.054  and the 90 % confidence intervals are  0.052 0.057\nBIC =  -26.23\n\n\n\n\nR Code: Plot Factor Loadings\nplt_loadings(fit.pa$loadings, K=4)\n\n\n\n\n\n\n\n\n\n\n\n7.5.3 Maximum Likelihood Estimation\nThe maximum likelihood estimation (MLE) method is a probabilistic method by assuming multivariate normal for the data and then maximizing the likelihood function to obtain the estimates.\n\n\n\n\n\n\nMaximum Likelihood Estimation\n\n\n\n\nThe distributional assumptions for the factor model are: \n\\mathbf{x}_j \\sim \\text{N}_p(\\boldsymbol{\\mu}, {\\Sigma}), \\;\\; \\mathbf{f}_j \\sim \\text{N}_m(\\mathbf{0}, \\mathbf{I}_m), \\;\\; \\boldsymbol{\\epsilon}_j \\sim \\text{N}_p(\\mathbf{0}, \\boldsymbol{\\Psi}_{p \\times p}),\n\n\n\\mathbf{x}_j = \\mathbf{L} \\mathbf{f}_j + \\boldsymbol{\\varepsilon}_j,\n{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top + \\boldsymbol{\\Psi},\nand \\mathbf{f}_j is independent of \\boldsymbol{\\varepsilon}_j.\nAlso, \\boldsymbol{\\Psi} is a diagonal matrix.\n\nThe log-likelihood function for the data is given by: \\begin{align*}\n\\ell(\\boldsymbol{\\mu}, {\\Sigma}) &:= -\\frac{n}{2}\\log |2\\pi {\\Sigma}| - \\frac{1}{2} \\sum_{i=1}^n (\\mathbf{x}_i - \\boldsymbol{\\mu})^\\top {\\Sigma}^{-1}(\\mathbf{x}_i - \\boldsymbol{\\mu}) \\\\\n&= -\\frac{n}{2}\\log |2\\pi {\\Sigma}| - \\frac{n}{2} \\text{tr}({\\Sigma}^{-1}{S}) - \\frac{n}{2}(\\bar{\\mathbf{x}}-\\boldsymbol{\\mu})^\\top{\\Sigma}^{-1} (\\bar{\\mathbf{x}}-\\boldsymbol{\\mu}).\n\\end{align*} where S is the sample covariance matrix.\n\n\n\n\n\nR Code: MLE via factanal\nfit.mle = factanal(dat, factors=m, method=\"mle\", \n                   rotation=\"none\")\nprint(fit.mle$loadings)\n\n\n\nLoadings:\n   Factor1 Factor2 Factor3 Factor4 Factor5 Factor6\nA1  0.223          -0.134          -0.374   0.344 \nA2 -0.392   0.359   0.161   0.152   0.347  -0.227 \nA3 -0.456   0.392   0.222   0.123   0.288         \nA4 -0.373   0.201           0.289   0.171         \nA5 -0.534   0.283   0.247           0.179         \nC1 -0.277   0.192  -0.458                   0.147 \nC2 -0.260   0.245  -0.504   0.193   0.122   0.222 \nC3 -0.286   0.127  -0.383   0.252                 \nC4  0.451           0.532  -0.189           0.215 \nC5  0.481           0.351  -0.245   0.122         \nE1  0.362  -0.317  -0.230           0.256   0.179 \nE2  0.587  -0.234  -0.175           0.326         \nE3 -0.465   0.439   0.119  -0.204           0.187 \nE4 -0.566   0.330   0.287   0.119  -0.163   0.135 \nE5 -0.421   0.427  -0.121          -0.177         \nN1  0.589   0.577                  -0.169         \nN2  0.573   0.566                  -0.127  -0.138 \nN3  0.519   0.492                                 \nN4  0.590   0.226                   0.292   0.102 \nN5  0.420   0.302           0.178   0.193   0.107 \nO1 -0.274   0.225  -0.167  -0.405           0.122 \nO2  0.196           0.278   0.407           0.154 \nO3 -0.326   0.334  -0.102  -0.503           0.105 \nO4  0.127   0.161  -0.103  -0.276   0.327         \nO5  0.178           0.235   0.457  -0.121   0.233 \n\n               Factor1 Factor2 Factor3 Factor4 Factor5 Factor6\nSS loadings      4.421   2.392   1.571   1.302   0.977   0.544\nProportion Var   0.177   0.096   0.063   0.052   0.039   0.022\nCumulative Var   0.177   0.273   0.335   0.387   0.427   0.448\n\n\n\n\nVisualize Factor Loadings\nplt_loadings(fit.mle$loadings, K=5)\n\n\n\n\n\n\n\n\n\nFactor Loading Interpretations: A loading represents how strongly a variable is associated with a factor. We could consider the loading |\\ell_{ij}| &gt; 0.3 or 0.4 so that each factor could have meaningful interpretations. The signs of factor loadings could be flipped for all factors.\n\nFactor 1: Items that load strongly on this factor could include A3-A5, C4,C5, E2-E5, N1-N5. In general, it is difficult to interpret since most variables depend this factor. It may suggest that there is a common factor to explain all the variables. This could be further verified using Quartimax rotation.\n\nFactor 2: Items N1-N5 load strongly on this factor. (Neuroticism)\nFactor 3: Items C1-C5 load strongly here. (Conscientiousness)\nFactor 4: Items O1-O5 load strongly here. O2 and O5 are reverse-coded. (Openness)\nFactor 5: Items that strongly load on this include A1-A3, E2, O4. It could be more related to Agreeableness.\n\n\nFactor Correlations: The table at the bottom shows the correlations between the factors. This could tell us how the latent factors are correlated. For orthogonal rotations, the factors will show essentially zero values; for non-orthogonal rotations, this could be used to check the validity of the rotation method.\n\n\nR Code: Correlation Among Factor Loadings\ncorrplot::corrplot(cor(fit.mle$loadings))\n\n\n\n\n\n\n\n\n\nIn constast to factanal(), one can also use fa() to get maximum likelihood estimates.\n\n\nR Code: MLE via fa\nfit.mle2 = fa(dat, nfactors=m, fm=\"ml\")\nfit.mle2$loadings\n\n\n\nLoadings:\n   ML2    ML1    ML3    ML5    ML4    ML6   \nA1  0.108 -0.131        -0.562         0.273\nA2                       0.696              \nA3        -0.119         0.608         0.127\nA4                0.194  0.406 -0.153  0.137\nA5 -0.163 -0.203         0.446         0.225\nC1                0.546         0.181       \nC2         0.150  0.668                0.193\nC3                0.561                     \nC4               -0.662                0.287\nC5  0.131  0.188 -0.555                     \nE1 -0.136  0.579  0.108 -0.148              \nE2         0.671                            \nE3        -0.361         0.139  0.372  0.262\nE4        -0.546         0.218         0.274\nE5  0.177 -0.409  0.261         0.226       \nN1  0.855                                   \nN2  0.853                                   \nN3  0.639  0.163                       0.116\nN4  0.381  0.463 -0.135                0.119\nN5  0.400  0.252         0.163 -0.131  0.189\nO1                              0.549       \nO2  0.119                0.100 -0.448  0.273\nO3        -0.108                0.661       \nO4         0.350         0.143  0.359       \nO5                             -0.517  0.324\n\n                 ML2   ML1   ML3   ML5   ML4   ML6\nSS loadings    2.321 1.988 1.974 1.722 1.656 0.716\nProportion Var 0.093 0.080 0.079 0.069 0.066 0.029\nCumulative Var 0.093 0.172 0.251 0.320 0.386 0.415\n\n\nR Code: MLE via fa\nplt_loadings(fit.mle2$loadings, K=5)\n\n\n\n\n\n\n\n\n\nFinal Conclusion: The exploratory factor analysis (EFA) successfully identified three of the “Big Five” personality traits (Conscientiousness, Neuroticism, and Openness) as the primary latent structures within the 25 survey items using the maximum likelihood estimation method without rotation on the factors.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "ch7/07-FactorAnalysis.html#rotation-of-factors",
    "href": "ch7/07-FactorAnalysis.html#rotation-of-factors",
    "title": "7  Factor Analysis",
    "section": "7.6 Rotation of Factors",
    "text": "7.6 Rotation of Factors\nIn the factor model with m&gt;1, there is no unique set of loadings and thus there is ambiguity associated with the factor model. This can be seen by introducing any m\\times m orthogonal matrix T in the factor model \n\\mathbf{z}_i = L\\mathbf{f}_i + \\boldsymbol \\varepsilon = L T T^\\top \\mathbf{f}_i +  \\boldsymbol \\varepsilon = L^* \\mathbf{f}_i^* + \\boldsymbol \\varepsilon_i\n\n\nTT^\\top = I and T^\\top T = I\nL^*:=LT and \\mathbf{f}_i^*:=T^\\top \\mathbf{f}_i\nThis indicates that loadings are not uniquely determined.\n\nIn what follows, we introduce three different factor rotations to better interpret the results.\n\n\n\n\n\n\nVarimax Rotation\n\n\n\nVarimax rotation aims to have each one of the p variables load highly on only one factor and have moderate to negligible loads on all other factors.\n\nThe transformations in Varimax rotation is an orthogonal transformation.\nAfter varimax rotation, each of the p variables should load highly on at most one of the rotated factors, but this may not always be true.\n\n\n\n\n\nR Code: Varimax Rotation\nfit.varimax = factanal(dat, factors=m, method=\"mle\", rotation=\"varimax\")\nfit.varimax$loadings\n\n\n\nLoadings:\n   Factor1 Factor2 Factor3 Factor4 Factor5 Factor6\nA1                         -0.534  -0.113   0.124 \nA2  0.260           0.128   0.645                 \nA3  0.384           0.127   0.568           0.153 \nA4  0.239           0.236   0.387  -0.152   0.102 \nA5  0.446  -0.137   0.107   0.435           0.227 \nC1                  0.549           0.188         \nC2                  0.665                   0.158 \nC3                  0.551                         \nC4          0.222  -0.633  -0.101  -0.120   0.305 \nC5 -0.184   0.273  -0.548                   0.137 \nE1 -0.575                  -0.133           0.178 \nE2 -0.675   0.233                           0.124 \nE3  0.594           0.112   0.141   0.250   0.231 \nE4  0.678  -0.140   0.114   0.215  -0.108   0.140 \nE5  0.515           0.306           0.197         \nN1          0.815          -0.162          -0.124 \nN2          0.802          -0.122          -0.186 \nN3          0.714                                 \nN4 -0.342   0.562  -0.160                   0.198 \nN5 -0.149   0.516                  -0.165   0.164 \nO1  0.232           0.134           0.487   0.158 \nO2          0.168                  -0.500   0.138 \nO3  0.343                           0.581   0.178 \nO4 -0.163   0.210           0.125   0.348   0.170 \nO5                                 -0.580   0.148 \n\n               Factor1 Factor2 Factor3 Factor4 Factor5 Factor6\nSS loadings      2.728   2.718   2.049   1.560   1.537   0.617\nProportion Var   0.109   0.109   0.082   0.062   0.061   0.025\nCumulative Var   0.109   0.218   0.300   0.362   0.424   0.448\n\n\n\n\nCode\nplt_loadings(fit.varimax$loadings, K=5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation of Varimax Factor Loadings\n\n\n\n\n\nThe goal is to name each factor by identifying the common theme among the variables that have high loadings on it according to a cutoff. This cutoff should be chosen on a case-by-case basis so that one can achieve good interpretability\n\nFactor 1: Extraversion️ This factor is defined by high loadings defined by E1-E5. Note the strong negative loadings for the introversion-keyed items.\n\nE1: Don’t talk a lot\nE2: Find it difficult to approach others\nE3: Know how to captivate people\nE4: Make friends easily\nE5: Am the life of the party\n\nFactor 2: Neuroticism This factor is defined by high positive loadings from all the Neuroticism items. Individuals scoring high on this factor tend to experience negative emotions.\n\nN1: Get stressed out easily\nN2: Worry about things\nN3: Am easily disturbed\nN4: Get upset easily\nN5: Change my mood a lot\n\nFactor 3: Conscientiousness This factor is defined by the Conscientiousness items. The negative loadings for items C4 and C5 indicate that people high on this factor are not likely to do things halfway or waste time.\n\nC1: Am always prepared\nC2: Pay attention to details\nC3: Get chores done right away\nC4: Do things in a half-way manner\nC5: Waste my time\n\nFactor 4: Agreeableness This factor clearly represents the Agreeableness trait. A1, which is reverse-keyed, has a moderate negative loading as expected.\n\nA2: Am interested in people\nA3: Sympathize with others’ feelings\nA4: Have a soft heart\nA5: Take time out for others\nA1: Am indifferent to the feelings of others\n\nFactor 5: Openness to Experience This factor is defined by the Openness items. The negative loadings for O2 and O5 are consistent with being low on this trait.\n\nO1: Have a rich vocabulary\nO3: Have a vivid imagination\nO4: Am full of ideas\nO2: Am not interested in abstract ideas\nO5: Do not enjoy going to art museums\n\nSummary of Variance Explained The first 5 factors explains 42.4% variations, however, the factor 6 has very small loadings (e.g., \\ell_{ij}&lt; 0.5), which indicates that factor 6 may not be interpretable.\n\n\n\n\n\n\n\n\n\n\nQuartimax Rotation\n\n\n\n\nThe varimax rotation destroy an “overall” factor.\nIn contrast, the quartimax rotation tries to\n\nPreserve an overall factor such that each of the p variables has a high loading on the overall factor;\nCreate other factors such that each of the p variables has a high loading on at most one factor.\n\n\n\n\n\n\nR Code: Quartimax Rotation\n# install.packages(\"GPArotation\")\nlibrary(GPArotation)\nquartimax(fit.mle$loadings)\n\n\nOrthogonal rotation method Quartimax converged.\nLoadings:\n     Factor1  Factor2 Factor3  Factor4 Factor5 Factor6\nA1 -0.081420  0.13326  0.0298 -0.07668 -0.5368  0.0907\nA2  0.414291  0.00380  0.1164  0.00996  0.5641  0.0119\nA3  0.538823 -0.02658  0.0862  0.00556  0.4371  0.1420\nA4  0.360211 -0.08975  0.2026 -0.17057  0.2819  0.1140\nA5  0.578553 -0.16346  0.0535  0.01204  0.2745  0.1745\nC1  0.123787 -0.00478  0.5341  0.17723 -0.0410  0.1257\nC2  0.120050  0.04917  0.6377  0.07489  0.0044  0.2339\nC3  0.108611 -0.04848  0.5416 -0.03148  0.0619  0.0670\nC4 -0.086235  0.20244 -0.6681 -0.08046 -0.0915  0.2475\nC5 -0.211614  0.25231 -0.5494  0.05720  0.0290  0.1233\nE1 -0.551133 -0.01293  0.0594 -0.07073 -0.0246  0.2748\nE2 -0.660465  0.18776 -0.0682 -0.04615  0.0657  0.2478\nE3  0.640897  0.00252  0.0567  0.26261 -0.0161  0.1358\nE4  0.734463 -0.12557  0.0541 -0.10235  0.0134  0.0346\nE5  0.519389  0.07199  0.2892  0.19436 -0.0317 -0.1067\nN1 -0.030707  0.83647 -0.0443 -0.05125 -0.0901 -0.0622\nN2 -0.068316  0.82158 -0.0145  0.01652 -0.0318 -0.1130\nN3 -0.076434  0.69737 -0.0612  0.00601  0.0503  0.1580\nN4 -0.323978  0.51793 -0.1650  0.07876  0.1068  0.2898\nN5 -0.110418  0.48374 -0.0586 -0.15774  0.1268  0.2465\nO1  0.244793 -0.02933  0.1169  0.49435 -0.0613  0.1088\nO2  0.038826  0.15911 -0.1272 -0.48961  0.0145  0.1579\nO3  0.361420  0.00777  0.0597  0.59050 -0.0241  0.1084\nO4 -0.111197  0.16732 -0.0292  0.34745  0.1744  0.2112\nO5 -0.000448  0.06525 -0.0853 -0.56502 -0.1232  0.1566\n\n               Factor1 Factor2 Factor3 Factor4 Factor5 Factor6\nSS loadings      3.320   2.641   1.975   1.514   1.063   0.695\nProportion Var   0.133   0.106   0.079   0.061   0.043   0.028\nCumulative Var   0.133   0.238   0.317   0.378   0.421   0.448\n\n\n\n\n\n\n\n\nInterpretation of Quartimax Rotated Loadings\n\n\n\n\n\nThe goal of a Quartimax rotation is to simplify the rows of the factor loading matrix, making it easier to see which factor each variable is associated with. Based on the loadings (where values &gt; 0.4 are considered significant), we can interpret the six factors as follows.\n\nFactor 1: Extraversion & Agreeableness (A “Sociability” Factor): This is a very broad factor that combines items from two distinct personality traits. It captures a general tendency towards being sociable, outgoing, and agreeable. Quartimax rotation can sometimes produce a large general factor, which seems to have happened here.\n\nE4: Make friends easily\nE2: Find it difficult to approach others\nE3: Know how to captivate people\nA5: Take time out for others\nA3: Sympathize with others’ feelings\nE1: Don’t talk a lot\nA2: Am interested in people\n\nFactor 2: Neuroticism: This is a very strong and clear factor, defined by high positive loadings from all the Neuroticism items.\n\nN1: Get stressed out easily\nN2: Worry about things\nN3: Am easily disturbed\nN4: Get upset easily\nN5: Change my mood a lot\n\nFactor 3: Conscientiousness: This factor is clearly defined by the Conscientiousness items. The negative loadings for C4 and C5 are expected as they are reverse-keyed items.\n\nC4: Do things in a half-way manner\nC2: Pay attention to details\nC5: Waste my time\nC3: Get chores done right away\nC1: Am always prepared\n\nFactor 4: Openness to Experience: This factor is defined by the Openness items. The negative loadings for the reverse-keyed items (O2, O5) fit the pattern perfectly.\n\nO3: Have a vivid imagination\nO5: Do not enjoy going to art museums\nO1: Have a rich vocabulary\nO2: Am not interested in abstract ideas\n\nFactor 5: Agreeableness: This factor has high loadings for A1-A3, which means describes the agreeablness trait. A1 is reverse coded.\n\nA1: Am indifferent to the feelings of others\nA2: Am interested in people\nA3: Sympathize with others’ feelings\n\nFactor 6: A Weak or Ill-Defined Factor: This factor has no high loadings (all are below 0.4). This suggests that it does not represent a clear, substantial underlying trait. In a real data analysis, you would likely conclude that a 5-factor solution is more appropriate than a 6-factor solution, as this sixth factor is not interpretable and its SS loading is below the typical cutoff of 1.0.\nSummary of Variance Explained: The five factors together explain 42.1% of the total variance in the personality items. However, since the fifth factor is not interpretable, a 4-factor solution explaining 38.2% of the variance would likely be the more practical and parsimonious choice.\n\n\n\n\n\n\n\n\n\n\nPromax Transformation\n\n\n\n\nThe varimax and quartimax rotations produce uncorrelcted factors.\nPromax is a non-orthogonal (oblique) transformation that\n\nis not a rotation,\ncan produce correlated factors,\nand tries to force each of the p variables to load highly on one of the factors.\n\n\n\n\n\n\nR Code: Promax Transformation\nresult = stats::promax(fit.mle$loadings)\nresult  \n\n\n$loadings\n\nLoadings:\n   Factor1 Factor2 Factor3 Factor4 Factor5 Factor6\nA1 -0.169                   0.109  -0.606   0.171 \nA2 -0.124                           0.658         \nA3 -0.289                           0.513   0.165 \nA4 -0.165          -0.194   0.190   0.332         \nA5 -0.394  -0.191                   0.330   0.283 \nC1                 -0.592  -0.134                 \nC2                 -0.735                         \nC3                 -0.610                  -0.183 \nC4                  0.704          -0.113   0.582 \nC5  0.101   0.102   0.576                   0.327 \nE1  0.630  -0.210  -0.183          -0.117   0.114 \nE2  0.729                                         \nE3 -0.604                  -0.236           0.336 \nE4 -0.726                   0.128           0.222 \nE5 -0.497   0.225  -0.233  -0.169                 \nN1 -0.136   0.926                          -0.123 \nN2          0.938                          -0.211 \nN3          0.654                           0.101 \nN4  0.359   0.347                           0.240 \nN5  0.148   0.380           0.172   0.118   0.174 \nO1 -0.184                  -0.480           0.199 \nO2                          0.504           0.181 \nO3 -0.297                  -0.579           0.258 \nO4  0.250                  -0.350   0.141   0.191 \nO5                          0.586  -0.124   0.183 \n\n               Factor1 Factor2 Factor3 Factor4 Factor5 Factor6\nSS loadings      2.788   2.611   2.255   1.516   1.396   1.186\nProportion Var   0.112   0.104   0.090   0.061   0.056   0.047\nCumulative Var   0.112   0.216   0.306   0.367   0.423   0.470\n\n$rotmat\n            [,1]       [,2]       [,3]       [,4]       [,5]         [,6]\n[1,]  0.45115168  0.5423513  0.2258156  0.1249663 -0.2001219  0.002288356\n[2,] -0.53744441  0.8627228 -0.1458693 -0.1403394  0.2335533  0.066627707\n[3,] -0.52840876 -0.1799881  0.9929386  0.2843336  0.2698631  0.523885615\n[4,]  0.09623074  0.1553164 -0.5313589  0.9322012  0.2464121 -0.314175730\n[5,]  0.78664249 -0.3674958 -0.0713201 -0.1901115  0.7539700  0.345176079\n[6,] -0.19004259 -0.5154202 -0.1272708  0.1613486 -0.6725570  1.069855011\n\n\nR Code: Promax Transformation\n# compute correlations for factor loadings \ncorfac = solve(t(result$rotmat) %*% result$rotmat)\ncorfac\n\n\n           [,1]        [,2]        [,3]          [,4]       [,5]          [,6]\n[1,]  1.0000000  0.34614290  0.34343748  0.1567108779 -0.3308908  0.1470709666\n[2,]  0.3461429  1.00000000  0.06268382  0.0414459766 -0.0860179  0.4838349238\n[3,]  0.3434375  0.06268382  1.00000000  0.1842433801 -0.2723639 -0.2866271867\n[4,]  0.1567109  0.04144598  0.18424338  1.0000000000 -0.0640062  0.0005207452\n[5,] -0.3308908 -0.08601790 -0.27236391 -0.0640061974  1.0000000  0.2205450811\n[6,]  0.1470710  0.48383492 -0.28662719  0.0005207452  0.2205451  1.0000000000\n\n\n\n\n\n\n\n\nInterpretation of Promax Rotated Loadings\n\n\n\n\n\nA Promax rotation is used when we expect the underlying latent factors to be correlated. The interpretation focuses on the Pattern Matrix, which shows the unique contribution of each variable to a factor. We will consider loadings with an absolute value above 0.4 as significant for naming the factors.\nInterpretation of Factors\n\nFactor 1: Extraversion (vs. Introversion) This factor is clearly defined by the Extraversion items. The signs are flipped compared to the previous analyses, so high scores on this factor indicate Introversion.\n\nE2: Find it difficult to approach others\nE1: Don’t talk a lot\nE4: Make friends easily\nE3: Know how to captivate people\nE5: Am the life of the party\n\nFactor 2: Neuroticism This is a very strong and clear factor, capturing emotional stability. It is defined by high positive loadings from the Neuroticism items.\n\nN1: Get stressed out easily\nN2: Worry about things\nN3: Am easily disturbed\n\nFactor 3: Conscientiousness This factor is clearly defined by the Conscientiousness items. The negative loadings for C1-C3 are expected as they are reverse-keyed. This factor captures the opposite side of conscientiousness, such as lack of direction or irresponsibility.\n\nC2: Pay attention to details\nC4: Do things in a half-way manner\nC3: Get chores done right away\nC5: Waste my time\nC1: Am always prepared\n\nFactor 4: Openness to Experience (Reversed) This factor is defined by the Openness items, but the signs are flipped. High scores on this factor indicate a lower degree of openness or a preference for the concrete over the abstract.\n\nO3: Have a vivid imagination\nO1: Have a rich vocabulary\nO5: Do not enjoy going to art museums\nO2: Am not interested in abstract ideas\n\nFactor 5: Agreeableness This factor is clearly defined by the Agreeableness items. The negative loading for the reverse-keyed item A1 is consistent with the trait.\n\nA3: Sympathize with others’ feelings\nA2: Am interested in people\nA5: Take time out for others\nA1: Am indifferent to the feelings of others\n\n\nInterpreting Factor Correlations: A critical step in an oblique rotation is to examine the Factor Correlation Matrix. This matrix shows how the five personality factors are related to each other.\nHere are the key relationships shown in the matrix:\n\nIntroversion and Neuroticism: There is a moderate positive correlation between Introversion (Factor 1) and Neuroticism (Factor 2). This is a classic finding in personality psychology: individuals who are more introverted also tend to be more prone to experiencing negative emotions like anxiety and stress.\nIntroversion and opposite Conscientiousness: There is a moderate negative correlation between Introversion (Factor 1) and Conscientiousness (Factor 3). This suggests that individuals who are more introverted tend to be slightly less conscientious (less organized, less disciplined). Conversely, more extraverted individuals tend to be more conscientious.\nNeuroticism and Conscientiousness: There is a small negative correlation between Neuroticism (Factor 2) and Conscientiousness (Factor 3). This indicates that individuals who are more emotionally stable (low Neuroticism) tend to be more organized and self-disciplined (high Conscientiousness).\nConscientiousness and Openness: The matrix shows a small positive correlation between Factor 3 (lack of Conscientiousness) and Factor 4 (Low Openness). This actually means there is a small positive correlation between Conscientiousness and Openness. People who are more disciplined and organized also tend to be slightly more open to new experiences.\nIntroversion and Agreeableness: There is a small negative correlation between Introversion (Factor 1) and Agreeableness (Factor 5). This suggests that more extraverted individuals tend to be slightly more agreeable and sympathetic towards others.\n\nOverall, these correlations align well with established findings in personality research and confirm that allowing the factors to correlate was a sensible decision for this analysis.\n\nSummary of Variance Explained: The five factors together explain 42.3% of the total variance in the personality items. Note that in an oblique rotation, the variance explained by each factor is not simply additive because the factors themselves share some variance.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "ch7/07-FactorAnalysis.html#estimation-of-factor-scores",
    "href": "ch7/07-FactorAnalysis.html#estimation-of-factor-scores",
    "title": "7  Factor Analysis",
    "section": "7.7 Estimation of Factor Scores",
    "text": "7.7 Estimation of Factor Scores\nFor each observation \\mathbf{x}_i (or its z-score \\mathbf{z}_i), we can estimate the (vector of) factor scores \\mathbf{f}_i once L is estimated. In general, there are three methods to estimating factor scores:\n\nordinary least squares (OLS),\nWeighted least squares (WLS),\nand regression method.\n\nThese methods can be specified via the option scores in the factanal function. In practice, one use often the regression method, which estimate \\mathbf{f}_i by the conditional mean given the observations: \n\\hat{\\mathbf{f}}_i:=E[\\mathbf{f}_i| \\mathbf{x}_i] = \\hat{L}^\\top (\\hat{L}\\hat{L}^\\top + \\hat{\\Psi})(\\mathbf{x}_i - \\bar{\\mathbf{x}}).\n\n\n\nR Code: Factor Scores Estimation via Regression Method\nfit = factanal(dat, factors=m, method=\"mle\", \n               rotation=\"varimax\", scores=\"regression\")\nhead(fit$scores)\n\n\n        Factor1    Factor2    Factor3    Factor4    Factor5     Factor6\n[1,]  1.1256448  0.3347749  1.3311765 -0.3292141  0.2411510 -0.18141115\n[2,] -1.4676037  0.4234724 -0.8052180 -1.6220757 -0.2979351 -0.76894404\n[3,]  0.3348235 -0.1676162 -0.2626270 -0.2672289 -0.3954644 -0.04196801\n[4,] -0.1021384 -0.4126529  0.6523772 -1.6680618 -0.2782254 -0.40778415\n[5,]  0.2615627 -0.9838066 -1.6811813  0.7779028  0.4237653 -0.02633796\n[6,]  0.4588120  1.0659874 -0.2737617  0.8797621  0.7791719  0.45639287\n\n\n\n\nR Code: Plot Factor Scores\nscores = fit$scores\ndf = cbind(df, scores)\n\nggplot(df, aes(x = Factor1, y = Factor2, color = factor(gender))) +\n  geom_point(alpha = 0.5) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  scale_color_manual(values = c(\"blue\", \"red\"), name = \"Gender\", labels = c(\"Male\", \"Female\")) +\n  labs(\n    title = \"Factor Scores: Agreeableness vs. Conscientiousness\",\n    subtitle = \"Points colored by gender\",\n    x = \"Factor 1: Agreeableness\",\n    y = \"Factor 2: Conscientiousness\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nR Code: Plot Factor Scores\nGGally::ggpairs(\n  df,\n  columns = 29:33, # The 6 factor score columns \n  aes(color = factor(gender), alpha = 0.6),\n  title = \"Pairwise Scatter Plot of Six Factor Scores by Gender\"\n) +\n  theme_bw()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "ch7/07-FactorAnalysis.html#exercise-track-record-data",
    "href": "ch7/07-FactorAnalysis.html#exercise-track-record-data",
    "title": "7  Factor Analysis",
    "section": "7.8 Exercise: Track Record Data",
    "text": "7.8 Exercise: Track Record Data\nThere are 54 countries’ track records for men and women. The dataset for women in records.women.csv includes variables x_1,\\dots,x_7 for 100 m, 200 m, 400 m (seconds) and 800 m, 1500 m, 3000 m, marathon (minutes). The dataset for men in records.men.csv includes variables x_1,\\dots,x_8 for 100 m, 200 m, 400 m (seconds) and 800 m, 1500 m, 5000 m, 10000 m, marathon (minutes). The first three times are in seconds and the remaining times are in minutes.\n\n\n\n\n\n\nResearch Question\n\n\n\n\n\nAre there any latent structures to explain the the performance difference between men and women?\n\n\n\nStep 1: Data Preparation and Visualization\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nData Preparation\ndat.m = read.csv(\"records.men.csv\")\ndat.w = read.csv(\"records.women.csv\")\nGGally::ggpairs(dat.m[,-1])\n\n\n\n\n\n\n\n\n\nData Preparation\nGGally::ggpairs(dat.w[,-1])\n\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Check Assumptions\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nR Code: Measures of Sampling Adequacy\ndat = dat.m[,-1]\npsych::KMO(dat.m[,-1]) \n\n\nKaiser-Meyer-Olkin factor adequacy\nCall: psych::KMO(r = dat.m[, -1])\nOverall MSA =  0.89\nMSA for each item = \n  x1   x2   x3   x4   x5   x6   x7   x8 \n0.84 0.84 0.97 0.90 0.94 0.85 0.85 0.95 \n\n\nR Code: Measures of Sampling Adequacy\npsych::KMO(dat.w[,-1])\n\n\nKaiser-Meyer-Olkin factor adequacy\nCall: psych::KMO(r = dat.w[, -1])\nOverall MSA =  0.82\nMSA for each item = \n  x1   x2   x3   x4   x5   x6   x7 \n0.89 0.78 0.86 0.85 0.74 0.76 0.88 \n\n\nInterpretation: Both of the overall MSAs are suggesting the data are appropriate for factor analysis.\n\n\nR Code: Multivariate Normality Test\nmvShapiroTest::mvShapiro.Test(as.matrix(dat))\n\n\n\n    Generalized Shapiro-Wilk test for Multivariate Normality by\n    Villasenor-Alva and Gonzalez-Estrada\n\ndata:  as.matrix(dat)\nMVW = 0.95037, p-value = 1.277e-06\n\n\n\n\n\nStep 3: Determine the Number of Factors to Extract\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nScree Plot and Hypothesis Tests\nfit.pc = prcomp(dat, center=TRUE, scale=TRUE)\nplot(fit.pc$sdev^2, type=\"b\")\n\n\n\n\n\n\n\n\n\nScree Plot and Hypothesis Tests\nsapply(1:3, function(f) \nfactanal(dat, factors = f, method =\"mle\")$PVAL)\n\n\n   objective    objective    objective \n5.849375e-16 1.734258e-02 2.226606e-01 \n\n\n\n\n\nStep 4: Extract and Rotate the Factors\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nR Code: Varimax Rotation\nfit2 = factanal(dat, factors=2, method=\"mle\", rotation=\"varimax\")\nfit3 = factanal(dat, factors=3, method=\"mle\", rotation=\"varimax\")\n\ndata.frame(\n  m     = c(2,3),\n  stat  = c(fit2$STATISTIC, fit3$STATISTIC),\n  df    = c(fit2$dof,       fit3$dof),\n  pval  = c(fit2$PVAL,      fit3$PVAL)\n)\n\n\n\n  \n\n\n\n\n\nFactor Loadings\nfit3$loadings\n\n\n\nLoadings:\n   Factor1 Factor2 Factor3\nx1 0.366   0.866   0.187  \nx2 0.374   0.829   0.322  \nx3 0.472   0.676   0.302  \nx4 0.538   0.441   0.715  \nx5 0.671   0.494   0.443  \nx6 0.842   0.426   0.307  \nx7 0.870   0.400   0.278  \nx8 0.837   0.377   0.266  \n\n               Factor1 Factor2 Factor3\nSS loadings      3.403   2.816   1.179\nProportion Var   0.425   0.352   0.147\nCumulative Var   0.425   0.777   0.925\n\n\n\n\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nR Code: Promax Rotation\nfit = factanal(dat, factors=3, method=\"mle\", rotation=\"promax\")\nfit$loadings\n\n\n\nLoadings:\n   Factor1 Factor2 Factor3\nx1          1.037  -0.130 \nx2          0.906   0.119 \nx3  0.202   0.633         \nx4  0.112           0.887 \nx5  0.505   0.179   0.327 \nx6  0.918                 \nx7  0.999                 \nx8  0.966                 \n\n               Factor1 Factor2 Factor3\nSS loadings      3.085   2.334   0.935\nProportion Var   0.386   0.292   0.117\nCumulative Var   0.386   0.677   0.794\n\n\n\n\nR Code: Factor Correlation Matrix\nM = fit$rotmat\n# correlation \ncorfac = solve(t(M) %*% M)\ncorfac\n\n\n          [,1]      [,2]      [,3]\n[1,] 1.0000000 0.8126219 0.7657261\n[2,] 0.8126219 1.0000000 0.7710439\n[3,] 0.7657261 0.7710439 1.0000000\n\n\n\n\n\nStep 5: Interpret Factor Loadings\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\nplt_loadings(fit$loadings, K=3, angle=0)\n\n\n\n\n\n\n\n\n\nThe data can be divided into three physiological groups:\n\nSprints: 100, 200, 400 (anaerobic exercise)\nMiddle distance: 800, 1500 (mixed energy)\nLong distance: 3000 (women), 5000/10000, marathon (aerobic endurance)\nThe varimax rotation aims to explain the data using one overall common theme. We will consider a loading above 0.6 (or below -0.6) as significant.\n\nFactor 1: This factor is defined by high positive loadings corresponding to middle distance and long distance. This measures aerobic endurance.\nFactor 2: This factor is defined by high positive loadings corresponding to sprints. This measures anaerobic speed/power.\nFactor 3: This factor has very high positive load on the variable of 800m. This indicates that both anaerobic speed/power and aerobic endurance are very important contributors for 800m.\n\nWhat about interpretations using other rotations?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "ch8/08-MDS.html",
    "href": "ch8/08-MDS.html",
    "title": "8  Multidimensional Scaling",
    "section": "",
    "text": "8.1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "ch8/08-MDS.html#introduction",
    "href": "ch8/08-MDS.html#introduction",
    "title": "8  Multidimensional Scaling",
    "section": "",
    "text": "The goal of multidimensional scaling (MDS) is to produce a low-dimensional configuration (spatial map) that preserves proximities (similarities/dissimilarities) in original data.\nMDS is widely used for various applications in psychology, marketing research, ecology and biology, social science, and visualization.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "ch8/08-MDS.html#motivation",
    "href": "ch8/08-MDS.html#motivation",
    "title": "8  Multidimensional Scaling",
    "section": "8.2 Motivation",
    "text": "8.2 Motivation\nWe often observe a proximity (similarity or dissimilarity) between pairs of items and want a geometric picture. Examples include:\n\nDistances along networks (e.g., airline route mileage),\nSensory judgments among products,\nVoting similarity/dissimilarity among legislators.\n\n\n# Motivating Example: Airline Distances\n## Corrected airline distances (ATL–SEA = 2180)\ncities &lt;- c(\"ATL\",\"ORD\",\"DEN\",\"HOU\",\"LAX\",\"MIA\",\"JFK\",\"SFO\",\"SEA\",\"IAD\")\nD &lt;- matrix(c(\n  0,587,1212,701,1936,604,748,2139,2180,543,\n  587,0,920,940,1745,1188,713,1858,1737,597,\n  1212,929,0,879,831,1726,1631,949,1021,1494,\n  701,940,879,0,1374,968,1420,1645,1891,1220,\n  1936,1745,831,1374,0,2339,2451,347,959,2300,\n  604,1188,1726,968,2339,0,1092,2594,2734,923,\n  748,713,1631,1420,2451,1092,0,2571,2408,205,\n  2139,1858,949,1645,347,2594,2571,0,678,2442,\n  2180,1737,1021,1891,959,2734,2408,678,0,2329,\n  543,597,1494,1220,2300,923,205,2442,2329,0\n), nrow=10, byrow=TRUE, dimnames=list(cities, cities))\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: The 2D configuration arranges cities roughly like the U.S. map: West Coast (SFO, LAX, SEA) on one side, East Coast (JFK, IAD, MIA) on the other, with DEN/ORD more central and HOU/ATL in the south. That’s the intuitive power of MDS: a big, unreadable distance table becomes a meaningful picture.\nNote: While the cities are roughly configured like the U.S. map, the exact geometry is still distorted.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "ch8/08-MDS.html#multidimensional-scaling-mds",
    "href": "ch8/08-MDS.html#multidimensional-scaling-mds",
    "title": "8  Multidimensional Scaling",
    "section": "8.3 Multidimensional Scaling (MDS)",
    "text": "8.3 Multidimensional Scaling (MDS)\n\n8.3.1 Basic idea\nMDS is a collection of different algorithms, designed to arrive at optimal low-dimensional (usually 2 or 3 dimensions) representation of the data, whose inter-point distances (or dissimilarities) are as close as possible to that in the original space. The figure below illustrates the core idea of MDS. \n\nGiven a n\\times p data matrix {Y}=[\\mathbf{y}_1, \\mathbf{y}_2, \\ldots, \\mathbf{y}_n]'. (This is often not required for MDS!)\nLet D=(d_{ij})_{i,j=1,\\ldots, n} denote an n\\times n distance matrix (for {Y}).\nMDS aims to find m (&lt; p) and a set of m-dimensional points \\mathbf{x}_1, \\ldots, \\mathbf{x}_n in the Euclidean space \\mathbb{R}^m (also called ordination space) such that \\begin{align*}\n\\delta_{ij}:=\\|\\mathbf{x}_i - \\mathbf{x}_j \\|_2 \\approx d_{ij} \\text{ as close as possible}.\n\\end{align*}\nMDS returns coordinates (also called scores) {\\mathbf{x}_1,\\ldots,\\mathbf{x}_n} in \\mathbb{R}^m so that the Euclidean distances \\delta_{ij}=\\|\\mathbf{x}_i-\\mathbf{x}_j\\|_2 reflect the given proximities (d_{ij}) as closely as possible (larger dissimilarity \\Rightarrow farther apart).\n\n\n\n8.3.2 How to Construct the Distance/Proximity Matrix D\n\nTwo common choices of metric distances:\n\nEuclidean distance: \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_2\nL_1 distance (also called the Manhattan or taxicab metric): \\|\\mathbf{y}_i - \\mathbf{y}_j\\|_1\n\nCommon non-metric distances: rank order\n\nA non-metric distance (or dissimilarity) is an input measure where the absolute difference between two values is not assumed to be numerically meaningful, but their rank order is preserved.\nA non-metric distance can describe “A is more dissimilar to B than C is to D,” without specifying how much more dissimilar.\n\n\n\n\n\n\n\n\nCommon metric distances\n\n\n\n\n\n\nLet \\mathbf{y}_i=(y_{i1}, \\ldots, y_{ip})^\\top denote an observation of p variables.\nThe Euclidean distance between \\mathbf{y}_i and \\mathbf{y}_j is defined as \\begin{align*}\n\\|\\mathbf{y}_i -\\mathbf{y}_j\\|_2: = \\sqrt{(\\mathbf{y}_i - \\mathbf{y}_j)^\\top(\\mathbf{y}_i - \\mathbf{y}_j)}\n=\\sqrt{\\sum_{k=1}^p (y_{ik}-y_{jk})^2}\n\\end{align*}\nThe L_1 distance (also called Manhattan or taxicab metric) between \\mathbf{y}_i and \\mathbf{y}_j is defined as \\begin{align*}\n\\|\\mathbf{y}_i -\\mathbf{y}_j\\|_1:  \n=\\sqrt{\\sum_{k=1}^m |y_{ik}-y_{jk}|}\n\\end{align*}\nWhen to use Euclidean distance: When the relationship between variables is continuous and comparable (in the same units) or the shortest, straight-line distance is preferred.\nWhen to use Manhattan distance: When the movement is constraint to a grid, or the variables are in comparable (e.g., temperature v.s. pressure).\n\n\n\n\n\n\n\n\n\n\nNon-metric distance: Bray-Curtis dissimilarity\n\n\n\n\n\n\nBray-Curtis dissimilarity (BC): This is the most common dissimilarity metric in community ecology (e.g., analyzing species abundance).\nIn community ecology, the data is represented in a Community Matrix\n\nRows represents samples (or sites/community, denoted by i, j)\nColumns represents variables (or species/taxa, denoted by k)\n\nFor two sites, i and j across all species, k, BC is defined as \\begin{align*}\n\\text{BC}_{ij} = \\frac{\\sum_{k=1}^p |y_{ik}-y_{jk}|}{\\sum_{k=1}^p (y_{ik}+y_{jk})}\n\\end{align*}\n\ny_{ik} is the abundance of species k at site i.\ny_{jk} is the abundance of species k at site j.\nThe numerator is the sum of the absolute differences in abundance for every species.\nThe denominator is the total abundance of all species found across both sites.\n\n\n\n\n\n\n\n\n\n\n\nNon-metric distance: Jaccard index\n\n\n\n\n\n\nJaccard Index (J): This index focuses purely on presence or absence data (binary data) and is widely used when abundance doesn’t matter, only identity.\n\nThe Jaccard index is calculated using binary data (presence = 1, absence =0): for two sites (samples) A and B \\begin{align*}\nJ = \\frac{|A \\cap B|}{|A \\cup B|}\n\\end{align*}\n\n|A \\cap B| (intersection): the number of species present in both Site A and Site B.\n|A \\cup B| (Union): the total number of unique species found in either Site A or Site B.\n\nThe Jaccard Dissimilarity (D_J): Since MDS requires a measure of dissimilarity (distance), the standard Jaccard index J is converted to a dissimilarity measure D_J:=1-J.\n\nIdentical sites: D_J=0\\rightarrow All species present are shared.\nMaximally dissimilar: D_J=1\\rightarrow The sites share no species in common.\n\n\n\n\n\n\n\n\n\n\n\nTips\n\n\n\n\n\n\nFor continuous data, we may also use PCA to extract latent dimensions. In fact, classical MDS via Euclidean distance is equivalent to PCA.\nFor non-continuous data such as counts or binary data, PCA is not appropriate, however, non-metric MDS can be used.\nBray-Curtis Dissimilarity (\\text{BC}) is used to study community composition (do they share the same list of species and are those species equally abundant?). BC can be used when species counts are reliable and the dominance of one or two species is ecologically important.\nJaccard Dissimilarity (D_J) is used to study species identity (e.g., do they share the same list of species?). D_J can be used when species counts are unreliable, or when one is interested in richness and the sheer presence of taxa (e.g., genetic markers).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "ch8/08-MDS.html#illustrative-examples",
    "href": "ch8/08-MDS.html#illustrative-examples",
    "title": "8  Multidimensional Scaling",
    "section": "8.4 Illustrative Examples",
    "text": "8.4 Illustrative Examples\n\n8.4.1 Air Distances\n\n\n\n\n\n\nVisualizing Air Distances\n\n\n\n\n\n\n## Corrected airline distances (ATL–SEA = 2180)\ncities &lt;- c(\"ATL\",\"ORD\",\"DEN\",\"HOU\",\"LAX\",\"MIA\",\"JFK\",\"SFO\",\"SEA\",\"IAD\")\nD &lt;- matrix(c(\n  0,587,1212,701,1936,604,748,2139,2180,543,\n  587,0,920,940,1745,1188,713,1858,1737,597,\n  1212,929,0,879,831,1726,1631,949,1021,1494,\n  701,940,879,0,1374,968,1420,1645,1891,1220,\n  1936,1745,831,1374,0,2339,2451,347,959,2300,\n  604,1188,1726,968,2339,0,1092,2594,2734,923,\n  748,713,1631,1420,2451,1092,0,2571,2408,205,\n  2139,1858,949,1645,347,2594,2571,0,678,2442,\n  2180,1737,1021,1891,959,2734,2408,678,0,2329,\n  543,597,1494,1220,2300,923,205,2442,2329,0\n), nrow=10, byrow=TRUE, dimnames=list(cities, cities))\n\n\n\n\nFor classical MDS, one can use the function cmdscale from the stats package in R.\n\nfit = cmdscale(d=D, k=2, eig=TRUE)\nprint(fit)\n\n$points\n          [,1]       [,2]\nATL  -718.1542  141.81292\nORD  -382.4282 -340.63839\nDEN   481.8355  -23.90197\nHOU  -161.5062  572.84413\nLAX  1203.7381  389.77983\nMIA -1133.6017  582.24559\nJFK -1072.3608 -518.86708\nSFO  1420.5980  112.16849\nSEA  1341.3548 -579.88076\nIAD  -979.7349 -335.31213\n\n$eig\n [1] 9581454.2358 1686306.1047    9002.2384    3772.7630    1232.1055\n [6]    -154.5591    -565.5305   -2442.6892   -6312.9188  -35087.7998\n\n$x\nNULL\n\n$ac\n[1] 0\n\n$GOF\n[1] 0.9948288 0.9987584\n\n\n\nThe output points gives the n\\times k matrix up to k columns whose rows give the coordinates of the points.\nThe output eig gives the eigenvalues. This can be used to check if there are any negative eigenvalues.\n\n\n\nVisualize the scores\nX2 &lt;- fit$points\nplot(X2, type=\"n\", xlab=\"Coordinate 1\", ylab=\"Coordinate 2\", main=\"Inter-city distances\")\ntext(X2, labels = rownames(X2), cex = 0.85)\n\n\n\n\n\n\n\n\n\n\n\n8.4.2 Community Ecology Example\n\n\n\n\n\n\ndune Dataset\n\n\n\nThe dune and dune.env datasets from R package vegan originate from an ecological study conducted in the sand dunes of the Meijendel area near Wassenaar, Netherlands.\n\ndune: Abundance counts for 30 species of vascular plants on 20 sites.\ndune.env: Environmental and management variables measured at the 20 sites.The variables relevant to the hypotheses include\n\nA1: A numeric vector of thickness of soil A1 horizon.\nMoisture: An ordered factor with levels: 1 &lt; 2 &lt; 4 &lt; 5.\nManagement: A factor with levels: BF (Biological farming), HF (Hobby farming), NM (Nature Conservation Management), and SF (Standard Farming).\nManure: The amount of manure applied (a proxy for soil fertility).\n\n\nThe scientific interest in the study revolves around how human activities and natural environmental gradients influence plant communities.\n\nlibrary(vegan)\ndata(dune)\ndata(dune.env)\nhead(dune)\n\n\n  \n\n\nhead(dune.env)\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nResearch Questions\n\n\n\n\n\n\nQuestion 1: Will plant communities under different management regimes (e.g., fertilized v.s. unfertilized, heavy grazing v.s. light grazing) have significantly different species compositions?\nQuestion 2: Is variation in species composition driven by key environmental gradients like moisture and fertility?\nQuestion 3: Does our conclusion change if we prioritize species identity (Jaccard) over species abundance (Bray-Curtis)?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(4570) \nlibrary(vegan)\nlibrary(ggplot2)\nlibrary(tibble)\nfit1 = metaMDS(\n  comm = dune, \n  distance = \"bray\", \n  k = 2\n)\n\nRun 0 stress 0.1192678 \nRun 1 stress 0.1192678 \n... Procrustes: rmse 3.17397e-05  max resid 9.709634e-05 \n... Similar to previous best\nRun 2 stress 0.1183186 \n... New best solution\n... Procrustes: rmse 0.02026913  max resid 0.06495776 \nRun 3 stress 0.1192678 \nRun 4 stress 0.1939202 \nRun 5 stress 0.1183186 \n... New best solution\n... Procrustes: rmse 2.184957e-05  max resid 6.530542e-05 \n... Similar to previous best\nRun 6 stress 0.1192678 \nRun 7 stress 0.1192679 \nRun 8 stress 0.1808911 \nRun 9 stress 0.1192679 \nRun 10 stress 0.1808911 \nRun 11 stress 0.1192678 \nRun 12 stress 0.1982888 \nRun 13 stress 0.1192678 \nRun 14 stress 0.1192678 \nRun 15 stress 0.1192678 \nRun 16 stress 0.1192678 \nRun 17 stress 0.1192678 \nRun 18 stress 0.1183186 \n... New best solution\n... Procrustes: rmse 7.727579e-06  max resid 1.995558e-05 \n... Similar to previous best\nRun 19 stress 0.1192679 \nRun 20 stress 0.1192678 \n*** Best solution repeated 1 times\n\n\n\nprint(fit1)\n\n\nCall:\nmetaMDS(comm = dune, distance = \"bray\", k = 2) \n\nglobal Multidimensional Scaling using monoMDS\n\nData:     dune \nDistance: bray \n\nDimensions: 2 \nStress:     0.1183186 \nStress type 1, weak ties\nBest solution was repeated 1 time in 20 tries\nThe best solution was from try 18 (random start)\nScaling: centring, PC rotation, halfchange scaling \nSpecies: expanded scores based on 'dune' \n\n\n\n\n\n\n\n\nDistances in metaMDS\n\n\n\n\n\nThe R function metaMDS provides lots of distance specifications, which can be retrieved by running the code ?vegdist.\n\n\n\n\n# Extract NMDS site coordinates directly\nsite_coords = as.data.frame(\n  vegan::scores(fit1, display = \"sites\"))\nsite_coords = site_coords %&gt;% \n  tibble::rownames_to_column(var = \"Site\")\nmeta_data = dune.env %&gt;%\n  rownames_to_column(var = \"Site\") %&gt;%\n  select(Site, Management)\n\nsite_scores = left_join(site_coords, meta_data, by = \"Site\")\n\nhead(site_scores)\n\n\n  \n\n\n\n\n\nVisualize scores\nggplot(site_scores, \n       aes(x = NMDS1, y = NMDS2)) +\n    geom_point(\n      aes(color = Management, shape = Management), size = 4) +\n    labs(\n      title = \"NMDS Ordination\",\n      subtitle =\"Community Composition by Management Regime\") +\n    scale_color_discrete(name = \"Management Type\") +\n    scale_shape_discrete(name = \"Management Type\") +\n    theme_minimal(base_size = 14) +\n    coord_fixed() \n\n\n\n\n\nSites closer together have more similar plant species composition (Bray-Curtis)\n\n\n\n\n\nNote: To perform the hypothesis testing, we could not use MANOVA as the data are Bray-Curtis dissimilarity matrix. Instead, one should use the so-called PERMANOVA test that is designed for this kind of data. This test will not be discussed in this course. The plot above visually shows if the colored clusters are well-separated, the hypothesis is then supported, otherwise it won’t.\n\n\n\nCode\n# we only look at how the scores would change according to \n# moisture, A1, and management\nfit.env &lt;- envfit(\n  fit1 ~ Moisture + A1 + Management, \n  data = dune.env, \n  permutations = 999\n)\n\ndf = as.data.frame(\n  vegan::scores(fit.env, display=\"vectors\")) %&gt;%\n  rownames_to_column(var = \"Environmental_Factor\")\ndf1 = df %&gt;% \nmutate(\n    R2 = fit.env$vectors$r, \n    p_value = fit.env$vectors$`pvals`\n  ) %&gt;%\n  mutate(\n    NMDS1_Scaled = NMDS1 * R2,\n    NMDS2_Scaled = NMDS2 * R2\n  ) %&gt;%\n  filter(p_value &lt;= 0.05)\n\n\n\n\nCode\nggplot(site_scores, \n       aes(x = NMDS1, y = NMDS2)) +\n  geom_point(\n    aes(color = Management, shape = Management), \n    size = 4, \n    alpha = 0.8\n  ) +\n  geom_segment(\n    data = df1,\n    aes(x = 0, y = 0, xend = NMDS1, yend = NMDS2),\n    arrow = arrow(length = unit(0.3, \"cm\")), \n    color = \"darkred\",\n    linewidth = 1\n  ) +\n  geom_text(\n    data = df1, \n    aes(x = NMDS1, y = NMDS2, \n        label = paste0(Environmental_Factor, \"\\n(R2=\", round(R2, 2), \")\")), \n    color = \"darkred\",\n    size = 4, \n    nudge_x = 0.1, \n    nudge_y = -0.1,\n    fontface = \"bold\"\n  ) +\n    labs(title = \"NMDS Ordination with Significant Environmental Gradients\") +\n  theme_bw() +\n  coord_fixed() \n\n\n\n\n\n\n\n\n\n\nInterpretation\n\nClusters: The colors show that the Management regime clearly separates the sites, confirming the PERMANOVA result.\nGradients: The dark red arrows show the most important continuous gradients. If the ‘A1’ arrow is long and points, for example, toward the ‘NM’ cluster, it means the plant community differences are strongly linked to the sites under nature conservation management.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "ch8/08-MDS.html#goodness-of-fit-metrics",
    "href": "ch8/08-MDS.html#goodness-of-fit-metrics",
    "title": "8  Multidimensional Scaling",
    "section": "8.5 Goodness of Fit Metrics",
    "text": "8.5 Goodness of Fit Metrics\n\n\n\n\n\n\n\n\nFeature\nMetric MDS (mMDS / Classical MDS)\nNon-Metric MDS (NMDS / Kruskal’s)\n\n\n\n\nPrimary Goal\nPreserve the absolute numerical values (\\delta_{ij}).\nPreserve the rank order of the dissimilarities (\\text{rank}(\\delta_{ij})).\n\n\nCore Metric\nStrain (or Metric Stress) (Variance Explained)\nKruskal’s Stress 1 (S)\n\n\nWhat is Measured\nThe discrepancy between the original metric distances (d_{ij}) and the final map distances (\\delta_{ij}).\nThe discrepancy between the rank of map distances (\\delta_{ij}) and the rank of original distances ({d}_{ij}).\n\n\nInterpretation\nThe strain indicates the proportion of total variation in the original distance matrix captured by the chosen dimensions.\nThe S value indicates how well the map’s geometry reflects the ordering of the input dissimilarities.\n\n\nRule of Thumb\nA high strain (e.g., &gt; 0.70) is desirable.\nA low Stress value (e.g., &lt; 0.15) is required for a reliable map interpretation.\n\n\nVisual Check\nScree Plot (for variance explained by dimension).\nShepard Diagram (visualizes preservation of rank order).\n\n\n\n\n\n\n\n\n\nStrain\n\n\n\n\n\nTwo simple cumulative fit measures are: \\begin{equation*}\nP_m = \\frac{\\sum_{k=1}^m |\\lambda_k|}{\\sum_{k=1}^n |\\lambda_k|},\n\\qquad\nP_m = \\frac{\\sum_{k=1}^m \\lambda_k^2}{\\sum_{k=1}^n \\lambda_k^2}.\n\\end{equation*}\n\nValues of P_m near 0.8 or above suggest a reasonable fit.\nIf B has a considerable number of large negative eigenvalues, the metric MDS method should not be used, and non-metric scaling should be considered.\nStrain preserves distance/variance.\n\n\n# Recall the air distance example\nfit = cmdscale(d=D, k=2, eig=TRUE)\neig &lt;- fit$eig\npm1 &lt;- cumsum(abs(eig)) / sum(abs(eig))\npm2 &lt;- cumsum(eig^2) / sum(eig^2)\ndata.frame(m = seq_along(eig), PM1 = pm1, PM2 = pm2)[1:5, ]\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nStress\n\n\n\n\n\n\nFor non-metric MDS, we can obtain stress directly from the R output.\nStress preserves the rank order.\n\n\n# Recal the dune data example \nfit1$stress\n\n[1] 0.1183186\n\n\n\n\n\n\nstressplot(fit1, \n           main = \"Shepard Diagram\",\n           lwd = 2)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "ch8/08-MDS.html#exercises",
    "href": "ch8/08-MDS.html#exercises",
    "title": "8  Multidimensional Scaling",
    "section": "8.6 Exercises",
    "text": "8.6 Exercises\n\n8.6.1 House voting (dissimilarities)\nLet’s analyze the voting dissimilarities of 15 U.S. congressmen from New Jersey on 19 environmental bills using non-metric MDS. The dissimilarity matrix is based on the number of times they voted differently. Higher value means more dissimilarity. We’ll apply MASS::isoMDS, then draw a Shepard diagram to assess monotonic fit.\n\n\nR Code: House Voting Data\nrepnames &lt;- c(\"Hunt(R)\",\"Sandman(R)\",\"Howard(D)\",\n              \"Thompson(D)\",\"Freylinghuysen(R)\",\n              \"Forsythe(R)\",\"Widnall(R)\",\"Roe(D)\",\n              \"Heltoski(D)\",\"Rodino(D)\",\n              \"Minish(D)\",\"Rinaldo(R)\",\"Maraziti(R)\",\n              \"Daniels(D)\",\"Patten(D)\")\n\nrepvote &lt;- matrix(c(\n  0, 8,15,15,10, 9, 7,15,16,14,15,16, 7,11,13,\n  8, 0,17,12,13,13,12,16,17,15,16,17,13,12,16,\n 15,17, 0, 9,16,12,15, 5, 5, 6, 5, 4,11,10, 7,\n 15,12, 9, 0,14,12,13,10, 8, 8, 8, 6,15,10, 7,\n 10,13,16,14, 0, 8, 9,13,14,12,12,12,10,11,11,\n  9,13,12,12, 8, 0, 7,12,11,10, 9,10, 6, 6,10,\n  7,12,15,13, 9, 7, 0,17,16,15,14,15,10,11,13,\n 15,16, 5,10,13,12,17, 0, 4, 5, 5, 3,12, 7, 6,\n 16,17, 5, 8,14,11,16, 4, 0, 3, 2, 1,13, 7, 5,\n 14,15, 6, 8,12,10,15, 5, 3, 0, 1, 2,11, 4, 6,\n 15,16, 5, 8,12, 9,14, 5, 2, 1, 0, 1,12, 5, 5,\n 16,17, 4, 6,12,10,15, 3, 1, 2, 1, 0,12, 6, 4,\n  7,13,11,15,10, 6,10,12,13,11,12,12, 0, 9,13,\n 11,12,10,10,11, 6,11, 7, 7, 4, 5, 6, 9, 0, 9,\n 13,16, 7, 7,11,10,13, 6, 5, 6, 5, 4,13, 9, 0\n), nrow=15, byrow=TRUE, dimnames=list(repnames, repnames))\n\nD &lt;- as.dist(repvote)\n\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nR Code: Non-metric MDS\nlibrary(MASS)\nnm &lt;- isoMDS(D) # 2D by default\n\n\ninitial  value 15.268246 \niter   5 value 10.264075\nfinal  value 9.879047 \nconverged\n\n\nR Code: Non-metric MDS\nhead(nm$points)\n\n\n                       [,1]      [,2]\nHunt(R)           -8.435401  0.906338\nSandman(R)        -7.405025  7.877023\nHoward(D)          6.093016 -1.497199\nThompson(D)        3.518702  5.248689\nFreylinghuysen(R) -7.245742 -4.182170\nForsythe(R)       -3.278710 -2.568967\n\n\n\n\nR Code: Check Stress\nnm$stress\n\n\n[1] 9.879047\n\n\nInterpretation: The final stress value is above 8 indicates a moderately good representation, which is also supported by the Shepard diagram.\n\n\nR Code: Visualization\ndf.nm = as_tibble(nm$points, \n                  .name_repair =\"unique\")\ncolnames(df.nm) = c(\"Dim1\", \"Dim2\")\ndf.nm$id = rownames(nm$points)\n\n# Extract party from the congressman names\ndf.nm = df.nm %&gt;%\n  mutate(party = \n           str_extract(id, \"\\\\(R\\\\)|\\\\(D\\\\)\")) %&gt;%\n  mutate(party = str_remove_all(party, \"[()]\")\n         )\n\nggplot(df.nm, \n       aes(x = Dim1, y = Dim2, \n           label = id, color = party)) +\n  geom_point(size = 3) +\n  geom_text(nudge_y = 0.5) +\n  labs(\n    title = \"Non-metric MDS of Congressman Voting Records\",\n    subtitle = \"Separation by Party Affiliation\",\n    x = \"Dimension 1\",\n    y = \"Dimension 2\"\n  ) \n\n\n\n\n\n\n\n\n\nInterpretation: This plot reveals that the separation is mainly along party lines, with Democrats on the right side of the display. One Republican, Rinaldo, has a voting record similar to the Democrats on environmental issues. The Republicans exhibit more variation than the Democrats. The two congressmen with the most abstentions, Sandman (R) and Thompson (D), are both located in the upper portion of the display.\nShepard diagram: monotonic relationship between observed dissimilarities and fitted distances.\n\nPoints close to the step-function indicate a good monotone fit; large vertical spreads indicate strain (stress).\n\n\n\nR Code: Shepard diagram\nsh &lt;- MASS::Shepard(D, nm$points)\nplot(sh, pch=\".\", xlab=\"Dissimilarity\", ylab=\"Distance\")\nlines(sh$x, sh$yf, type=\"S\")",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multidimensional Scaling</span>"
    ]
  },
  {
    "objectID": "ch9/09-ClusterAnalysis.html",
    "href": "ch9/09-ClusterAnalysis.html",
    "title": "9  Cluster Analysis",
    "section": "",
    "text": "9.1 Learning Objectives\nBy the end of this lecture, you should be able to:",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "ch9/09-ClusterAnalysis.html#learning-objectives",
    "href": "ch9/09-ClusterAnalysis.html#learning-objectives",
    "title": "9  Cluster Analysis",
    "section": "",
    "text": "Explain what cluster analysis is and when it is used.\nDistinguish between:\n\nHierarchical clustering\nK-means clustering\n\nCompute and interpret distances between observations.\nRead and interpret a dendrogram.\nRun hierarchical and K-means clustering in R.\nUse simple criteria (Calinski–Harabasz index, Gap statistic) to help choose the number of clusters.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "ch9/09-ClusterAnalysis.html#what-is-cluster-analysis",
    "href": "ch9/09-ClusterAnalysis.html#what-is-cluster-analysis",
    "title": "9  Cluster Analysis",
    "section": "9.2 What is Cluster Analysis?",
    "text": "9.2 What is Cluster Analysis?\nGoal: Discover reasonable groupings (“clusters”) of subjects or units.\nExamples:\n\nGroup medical patients by how they use different types of services.\nGroup potential customers by product preferences.\nGroup environmental habitats by land cover types.\nGroup foods by nutrient content.\n\n\n\n\n\n\n\nUnsupervised learning\n\n\n\n\n\nCluster analysis is an unsupervised learning method:\n\nNo response variable / labels.\nWe only use similarity/dissimilarity between observations.\n\n\n\n\n\n9.2.1 Clustering Example: Old Faithful Geyser\n\nData: eruption durations and waiting times.\n\n\n\nCode\ndata(faithful)\nplot(faithful,\n     main = \"How many clusters can you see?\",\n     pch = 19)\n\n\n\n\n\nOld Faithful geyser data\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\n\n\n\nInterpretation:\n\nThere seems to be two groups in the dataset\nShort waiting, short eruption\nLong waiting, long eruption\n\n\n\n\n\n\n\n9.2.2 Data for Cluster Analysis\n\nBig picture:\n\nn individuals (cases)\np variables (or traits) measured on each case \n\\mathbf{x}_{1}=\\begin{pmatrix} x_{11} \\\\ x_{12} \\\\ \\vdots \\\\ x_{1p} \\end{pmatrix}, \\mathbf{x}_{2}=\\begin{pmatrix} x_{21} \\\\ x_{22} \\\\ \\vdots \\\\ x_{2p} \\end{pmatrix}, \\ldots, \\mathbf{x}_{n}=\\begin{pmatrix} x_{n1} \\\\ x_{n2} \\\\ \\vdots \\\\ x_{np} \\end{pmatrix}\n\n\nGoal: We want to group the \\mathbf{x}_i’s into clusters such that\n\nWithin a cluster: observations are similar.\nAcross clusters: observations are different.\n\n\nA crucial factor for generating useful clusters is the initial selection of variables.\n\n\n9.2.3 Two Key Ingredients\nMost clustering methods share two core ideas:\n\nDistance (dissimilarity) between cases\n\nEuclidean distance\nMahalanobis distance\nDistances based on correlations, absolute differences, etc\n\nLinkage or grouping procedure\n\nHow do we define the distance between clusters of observations?\nThis drives the hierarchy in hierarchical clustering.\n\n\n\n\n\n\n\n\nDistances\n\n\n\n\n\n\nEuclidean distance: d(\\mathbf{x}_{i},\\mathbf{x}_{j})=\\sqrt{(\\mathbf{x}_{i}-\\mathbf{x}_{j})^{\\prime}(\\mathbf{x}_{i}-\\mathbf{x}_{j})}\nStatistical distance (Mahalanobis): d(\\mathbf{x}_{i},\\mathbf{x}_{j})=\\sqrt{(\\mathbf{x}_{i}-\\mathbf{x}_{j})^{\\prime}\\Sigma^{-1}(\\mathbf{x}_{i}-\\mathbf{x}_{j})}\nStandardized Distance: d(\\mathbf{x}_{i},\\mathbf{x}_{j})=\\sqrt{(\\mathbf{x}_{i}-\\mathbf{x}_{j})^{\\prime}[\\text{diag}(S)]^{-1}(\\mathbf{x}_{i}-\\mathbf{x}_{j})}\nAbsolute Values (Manhattan/City-Block): d(\\mathbf{x}_{i},\\mathbf{x}_{j})=\\frac{1}{p}\\sum_{k=1}^{p}|{x}_{ik}-{x}_{jk}|\nCorrelation-based: d(\\text{item}_{i},\\text{item}_{j})=1-|r_{ij}| or d(\\text{item}_{i},\\text{item}_{j})=1-r_{ij}^{2}",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "ch9/09-ClusterAnalysis.html#hierarchical-clustering",
    "href": "ch9/09-ClusterAnalysis.html#hierarchical-clustering",
    "title": "9  Cluster Analysis",
    "section": "9.3 Hierarchical Clustering",
    "text": "9.3 Hierarchical Clustering\n\nHierarchical clustering does not assume the number of clusters.\nHierarchical clustering uses distances between observations to build a tree representation, called a dendrogram. This encodes a hierarchy of clusters.\nIt is based on an agglomerative (bottom-up) approach: Starts with each observation as its own cluster and merges the closest pairs/groups until all are in one cluster.\n\n\n9.3.1 Basic Idea\n\n\n\n\n\n\n\n\n\n\nA hierarchical clustering algorithm\n\n\n\n\nDefine a distance (or dissimilarity) between clusters\nInitialize: each cluster contains one data point\nRepeat:\n\nCompute distances between all clusters\nMerge two closest clusters\n\nSave both clustering and sequence of cluster operations\nThis gives us a Dendrogram.\n\n\n\n\n\n\n\n\n\n\n\nIllustration of a hierarchical clustering algorithm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDendrogram\n\n\n\n\nVertical axis: distance between clusters.\nHorizontal axis: observations (or variables).\nLeaves: individual observations.\nInternal nodes: clusters formed by merging.\nHeight of the join indicates dissimilarity.\n\nThe number of clusters can be selected according to a threshold on dissimilarity.\n\n\n\n\n\n\n\n\n\nDetermine the number of clusters\n\n\n\n\n\nTo determine the number of clusters, cut the dendrogram at a certain height and count the resulting branches.\n\n\n\n\n\n9.3.2 Linkage Methods\nLinkage determines the dissimilarity d(C_1, C_2) between two clusters, C_1 and C_2. The key R function for hierarchical clustering is hclust in the stats package.\n\n\n\n\n\n\n\n\nLinkage Method\nFormula\nDescription\n\n\n\n\nSingle Linkage (Nearest-Neighbor)\nd(C_{1},C_{2})=\\min_{i\\in C_{1},j\\in C_{2}}d_{ij}\nDistance between closest points in the clusters.\n\n\nComplete Linkage (Furthest-Neighbor)\nd(C_{1},C_{2})=\\max_{i\\in C_{1},j\\in C_{2}}d_{ij}\nDistance between farthest points in the clusters.\n\n\nAverage Linkage\nd(C_{1},C_{2})=\\frac{1}{|C_{1}|\\cdot|C_{2}|}\\sum_{i\\in C_{1},j\\in C_{2}}d_{ij}\nAverage of all pairwise distances.\n\n\nCentroid Method\nd(\\overline{\\mathbf{x}}_{A},\\overline{\\mathbf{x}}_{B})=\\bigl\\|\\overline{\\mathbf{x}}_{A}-\\overline{\\mathbf{x}}_{B}\\bigr\\|^2_2\nDistance between centroids (means) of two clusters.\n\n\nWard’s Method\n\\Delta_{SST} = \\frac{n_{A}n_{B}}{n_{A}+n_{B}}\\bigl\\|\\overline{\\mathbf{x}}_{A}-\\overline{\\mathbf{x}}_{B}\\bigr\\|^2_2\nMerges two clusters that result in the smallest increase in the total within-cluster sum of squared Euclidean distances (SST).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.3.3 Insect Community Data Example\n\n\n\n\n\n\nInsect Community Data\n\n\n\nThe data set (newpbi.csv) consist of insect counts collected on 30 Iowa prairies with 44 types of insects. Counts of these insects were taken periodically over one summer at the 30 sites. The first column (Site) identifies the sample location, and the second column (Type) is likely a categorical environmental or habitat variable. The remaining columns are abundance counts for different arthropod groups (e.g., Araneae, ACari).\n\n\nR Code: Load Data\ndf = read.csv(\"newpbi.csv\")\nhead(df)\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nResearch Questions\n\n\n\n\n\n\nCommunity Structure\n\nQuestion 1: Do distinct communities exist in the sampled areas, and if so, how many? This can discover if the sites fall into natural groupings based purely on the species/taxa they contain. This could reveal previously unrecognized ecological zones or biotypes\nQuestion 2: Are the natural community clusters consistent with the recorded habitat types (Type)?\n\nTaxonomic and Functional Groups\n\nQuestion 3: Which specific arthropod taxa (e.g., specific beetle or fly families) tend to co-occur across sites?\n\n\n\n\n\n\n\nR Code: Visualize the data\nabundance_matrix = as.matrix(df[,-c(1:2)])\n\nheatmap(abundance_matrix, col=cm.colors(256), \n               Rowv=NA, Colv=NA) \n\n\n\n\n\n\n\n\n\n\nA heatmap displays:\n\nObservations (or variables) on rows.\nVariables (or observations) on columns.\nColor intensity = value size.\n\n\n\n\n\n\n\n\nheatmap\n\n\n\n\n\nheatmap(x, Rowv = NULL, Colv = if(symm)\"Rowv\" else NULL,\n        distfun = dist, hclustfun = hclust,\n        reorderfun = function(d, w) reorder(d, w),\n        add.expr, symm = FALSE, revC = identical(Colv, \"Rowv\"),\n        scale = c(\"row\", \"column\", \"none\"), na.rm = TRUE,\n        margins = c(5, 5), ColSideColors, RowSideColors,\n        cexRow = 0.2 + 1/log10(nr), cexCol = 0.2 + 1/log10(nc),\n        labRow = NULL, labCol = NULL, main = NULL,\n        xlab = NULL, ylab = NULL,\n        keep.dendro = FALSE, verbose = getOption(\"verbose\"), ...)\n\nBy default, heatmap performs clustering for both rows and columns based on the clustering function hclust. Unless we know the default clustering uses an appropriate distance/dissimilarity metric, we can use the clustered heatmap for interpretation.\n\n\n\n\n\n\nR Code: Calculate Bray-Curtis dissimilarity via vegdist\nlibrary(vegan)\nD = vegdist(abundance_matrix, method = \"bray\")\n\n\n\n\nR Code: Hierarchical Clustering via hclust\nhca1 = hclust(D, method=\"single\")\nhca2 = hclust(D, method=\"complete\")\nhca3 = hclust(D, method=\"average\")\nhca4 = hclust(D, method=\"ward.D2\")\n\n\n\n\nR Code: Dendrograms\nlibrary(factoextra)\np1=fviz_dend(hca1)\np2=fviz_dend(hca2)\np3=fviz_dend(hca3)\np4=fviz_dend(hca4)\n\npatchwork::wrap_plots(p1 + ggtitle(\"single\"),\n                      p2 + ggtitle(\"complete\"),\n                      p3 + ggtitle(\"average\"),\n                      p4 + ggtitle(\"ward\"),ncol=2)\n\n\n\n\n\n\n\n\n\n\nInterpretation: The height at which two branches join represents the dissimilarity (Bray-Curtis distance) between them. Short branches that merge low down indicate highly similar communities.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "ch9/09-ClusterAnalysis.html#k-means-clustering",
    "href": "ch9/09-ClusterAnalysis.html#k-means-clustering",
    "title": "9  Cluster Analysis",
    "section": "9.4 K-Means Clustering",
    "text": "9.4 K-Means Clustering\n\nK-means clustering is a type non-hierarchical clustering algorithm\nNo tree structure exists\nOne often needs to pre-determine the number of clusters in the algorithm to find the clusters\n\n\n9.4.1 Basic Idea\n\n\n\n\n\n\n\n\nA K-means clustering algorithm\n\n\n\n\nInitialize: Pick K points randomly as cluster centers.\n\nRepeat:\n\nAssign points to closet cluster center\nUpdate cluster center location to the mean of the assigned points\n\nStop when no points change cluster assignment (convergence)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIllustration of K-means clustering\n\n\n\n\n\n\n\n\n\n\n\nIteration 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIteration 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIteration 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.4.2 A Toy Example\nThis small data set has only four cases (A, B, C, and D) and two variables are measured on each case. The data vectors are: A=\\begin{pmatrix}5\\\\ 3\\end{pmatrix} \\quad B=\\begin{pmatrix}-1\\\\ 1\\end{pmatrix} \\quad C=\\begin{pmatrix}1\\\\ -2\\end{pmatrix} \\quad D=\\begin{pmatrix}-3\\\\ -2\\end{pmatrix} \nStart with initial clusters \\mathbf{(A, B)} and \\mathbf{(C, D)}. The centroids for the initial clusters are C_{1}=\\begin{pmatrix}2\\\\ 2\\end{pmatrix} \\quad \\text{and} \\quad C_{2}=\\begin{pmatrix}-1\\\\ -2\\end{pmatrix}\n\n\n\n\n\n\n\nStep-by-step K-means iteration\n\n\n\n\n\nIn each iteration, we need to compute the distance of each case from each centroid. Here we use Euclidean distance. Then we update cluster centroids.\n\n\n\n\n\n\nIteration 1\n\n\n\n\n\n\ndistances:\n\nCase A: d(A,C_{1})=\\sqrt{(5-2)^{2}+(3-2)^{2}}=\\sqrt{10}, and d(A,C_{2})=\\sqrt{(5-(-1))^{2}+(3-(-2))^{2}}=\\sqrt{61}\nCase B: d(B,C_{1})=\\sqrt{(-1-2)^{2}+(1-2)^{2}}=\\sqrt{10}, and d(B,C_{2})=\\sqrt{(-1-(-1))^{2}+(1-(-2))^{2}}=\\sqrt{9}\nCase C: d(C,C_{1})=\\sqrt{(1-2)^{2}+(-2-2)^{2}}=\\sqrt{17}, and d(C,C_{2})=\\sqrt{(1-(-1))^{2}+(-2-(-2))^{2}}=\\sqrt{4}\nCase D: d(D,C_{1})=\\sqrt{(-3-2)^{2}+(-2-2)^{2}}=\\sqrt{41}, and d(D,C_{2})=\\sqrt{(-3-(-1))^{2}+(-2-(-2))^{2}}=\\sqrt{4}\n\nThe new clusters are \\mathbf{(A)} and \\mathbf{(B, C, D)}\n\nThe new centroids are: C_{1}=\\begin{pmatrix}5\\\\ 3\\end{pmatrix} \\quad \\text{and} \\quad C_{2}=\\begin{pmatrix}-1\\\\ -1\\end{pmatrix} \n\nCheck convergence: for example, specify a tolerance for the difference between two consecutive centroids.\n\n\n\n\n\n\n\n\n\n\nIteration 2\n\n\n\n\n\n\nDistances:\n\nCase A: d(A,C_{1})=\\sqrt{(5-5)^{2}+(3-3)^{2}}=\\sqrt{0} and d(A,C_{2})=\\sqrt{(5-(-1))^{2}+(3-(-1))^{2}}=\\sqrt{52}\nCase B: d(B,C_{1})=\\sqrt{(-1-5)^{2}+(1-3)^{2}}=\\sqrt{40} and d(B,C_{2})=\\sqrt{(-1-(-1))^{2}+(1-(-1))^{2}}=\\sqrt{4}\nCase C: d(C,C_{1})=\\sqrt{(1-5)^{2}+(-2-3)^{2}}=\\sqrt{41} and d(C,C_{2})=\\sqrt{(1-(-1))^{2}+(-2-(-1))^{2}}=\\sqrt{5}\nCase D: d(D,C_{1})=\\sqrt{(-3-5)^{2}+(-2-3)^{2}}=\\sqrt{89} and d(D,C_{2})=\\sqrt{(-3-(-1))^{2}+(-2-(-1))^{2}}=\\sqrt{5}\nThe new clusters are \\mathbf{(A)} and \\mathbf{(B, C, D)} and the clusters did not change: C_{1}=\\begin{pmatrix}5\\\\ 3\\end{pmatrix} \\quad \\text{and} \\quad C_{2}=\\begin{pmatrix}-1\\\\ -1\\end{pmatrix}\n\n\nStop: The K-means algorithm has converged to produce clusters \\mathbf{(A)} and \\mathbf{(B, C, D)}\n\n\n\n\n\n\n\n\n\n9.4.3 How to Choose the Number of Clusters?\n\n\n\n\n\n\nReview of notations\n\n\n\n\n\n\nn is the number of observations in the data.\np is the number of variables measured on each observation.\nC_k represents the k-th cluster at a particular step of the clustering algorithm and n_k is the number of observations in C_k.\nSS_{T} is the total sum of squared distances from the overall vector of means for all n observations in the data set: \n  SS_T = \\sum_{i=1}^n (\\mathbf{x}_i-\\bar{\\mathbf{x}})^{\\prime}  (\\mathbf{x}_i-\\bar{\\mathbf{x}})\n\n\n\n\n\n\n\n\n\n\n\nCluster validation metrics\n\n\n\n\n\nWe define the components used in cluster validation metrics:\n\n{G}: The number of groups or clusters. The goal of these methods is to find the optimal {G}.\n\\mathbf{w}_k: The sum of squared distances for all points within a single cluster, C_k. \n\\mathbf{w}_k:= \\sum_{i \\text{ in } C_k} (\\mathbf{x}_i-\\bar{\\mathbf{x}}_k)^{\\prime}  (\\mathbf{x}_i-\\bar{\\mathbf{x}}_k)\n\n\\mathbf{W}_G: The total sum of squared distances across all \\mathbf{G} clusters. This is the Within-Group Sum of Squares (WCSS): \n  \\mathbf{W}_G = \\sum_{k=1}^{G} \\mathbf{w_k}\n  \n\\mathbf{B}_G: The Between-Group Sum of Squares (BCSS), which measures the separation of the group centroids. It is defined based on the Total Sum of Squares ({SS}_T): \n  \\mathbf{B}_G = {SS}_T - \\mathbf{W}_G\n  \n\n\n\n\n\n\n\n\n\n\nComparative methods\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nCore Concept\nOptimal {G} Rule\nMathematical Metric\n\n\n\n\nElbow (WCSS)\nMeasures total compactness (\\mathbf{W}_G) as {G} increases.\nChoose the {G} where the decrease in \\mathbf{W}_G starts to slow significantly (the “bend”).\nPlot \\mathbf{W}_G vs. {G}\n\n\nCalinski-Harabasz Index\nMaximizes the ratio of separation (\\mathbf{B}_G) to compactness (\\mathbf{W}_G).\nChoose the {G} that maximizes the CH index (highest peak).\n\\text{CH}(G) = \\frac{\\mathbf{B}_G}{\\mathbf{W}_G} \\cdot \\frac{n-G}{G-1}\n\n\nGap Statistic\nCompares the observed compactness (\\mathbf{W}_G) to the expected compactness (\\mathbf{W}_{G}^*) under a random null distribution.\nChoose the smallest {G} that maximizes the Gap statistic.\n\\text{Gap}(G) = E_{n} \\{\\log(\\mathbf{W}_{G}^*)\\} - \\log({\\mathbf{W}_G})\n\n\n\nNote: \\mathbf{W}_G^* is defined slightly different from \\mathbf{W}_G.\n\nDefine D_k = \\sum_{i, i'\\in C_k} d_{ii'} to be the sum of pairwise distances for all points in cluster C_k.\nSet W_G^*=\\sum_{k=1}^G \\frac{1}{2n_k} D_k.\n\n\n\n\n\n\n9.4.4 Working Example\n\n\nUSArests data\nlibrary(factoextra)\nlibrary(cluster)\n#load data\ndf &lt;- USArrests\n\n#remove rows with missing values\ndf &lt;- na.omit(df)\n\ndf &lt;- scale(df)\n\nhead(df)\n\n\n               Murder   Assault   UrbanPop         Rape\nAlabama    1.24256408 0.7828393 -0.5209066 -0.003416473\nAlaska     0.50786248 1.1068225 -1.2117642  2.484202941\nArizona    0.07163341 1.4788032  0.9989801  1.042878388\nArkansas   0.23234938 0.2308680 -1.0735927 -0.184916602\nCalifornia 0.27826823 1.2628144  1.7589234  2.067820292\nColorado   0.02571456 0.3988593  0.8608085  1.864967207\n\n\n\n\nElbow method\nfviz_nbclust(df, kmeans, method = \"wss\")\n\n\n\n\n\n\n\n\n\n\n\nGap statistic\nfviz_nbclust(df, kmeans, method = \"gap_stat\")\n\n\n\n\n\n\n\n\n\n\n\nK-means clustering\nset.seed(4750)\n\n#perform k-means clustering with k = 4 clusters\nkm = kmeans(df, \n     centers = 4, \n     nstart = 25 # choose 25 initial centers\n     )\n\nprint(km)\n\n\nK-means clustering with 4 clusters of sizes 13, 13, 16, 8\n\nCluster means:\n      Murder    Assault   UrbanPop        Rape\n1  0.6950701  1.0394414  0.7226370  1.27693964\n2 -0.9615407 -1.1066010 -0.9301069 -0.96676331\n3 -0.4894375 -0.3826001  0.5758298 -0.26165379\n4  1.4118898  0.8743346 -0.8145211  0.01927104\n\nClustering vector:\n       Alabama         Alaska        Arizona       Arkansas     California \n             4              1              1              4              1 \n      Colorado    Connecticut       Delaware        Florida        Georgia \n             1              3              3              1              4 \n        Hawaii          Idaho       Illinois        Indiana           Iowa \n             3              2              1              3              2 \n        Kansas       Kentucky      Louisiana          Maine       Maryland \n             3              2              4              2              1 \n Massachusetts       Michigan      Minnesota    Mississippi       Missouri \n             3              1              2              4              1 \n       Montana       Nebraska         Nevada  New Hampshire     New Jersey \n             2              2              1              2              3 \n    New Mexico       New York North Carolina   North Dakota           Ohio \n             1              1              4              2              3 \n      Oklahoma         Oregon   Pennsylvania   Rhode Island South Carolina \n             3              3              3              3              4 \n  South Dakota      Tennessee          Texas           Utah        Vermont \n             2              4              1              3              2 \n      Virginia     Washington  West Virginia      Wisconsin        Wyoming \n             3              3              2              2              3 \n\nWithin cluster sum of squares by cluster:\n[1] 19.922437 11.952463 16.212213  8.316061\n (between_SS / total_SS =  71.2 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\n\nVisualiazing clusters\nfviz_cluster(km, data = df)\n\n\n\n\n\n\n\n\n\n\n\nCompute cluster means\naggregate(USArrests, by=list(cluster=km$cluster), mean)\n\n\n\n  \n\n\n\n\n\nCode\n#add cluster assigment to original data\nfinal_data = cbind(USArrests, cluster = km$cluster)\n\n#view final data\nhead(final_data)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "ch10/10-DecisionTrees.html",
    "href": "ch10/10-DecisionTrees.html",
    "title": "10  Decision Trees",
    "section": "",
    "text": "10.1 Learning goals\nBy the end of this section, you should be able to:",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "ch10/10-DecisionTrees.html#learning-goals",
    "href": "ch10/10-DecisionTrees.html#learning-goals",
    "title": "10  Decision Trees",
    "section": "",
    "text": "Explain the basic idea of decision trees\nUnderstand the idea of classification tree.\nDefine impurity measures (misclassification error, Gini index, cross-entropy).\nFit and interpret a classification tree using R package rpart.\nVisualize trees and decision boundaries.\nExplain overfitting and why pruning / cross-validation are needed.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "ch10/10-DecisionTrees.html#introduction",
    "href": "ch10/10-DecisionTrees.html#introduction",
    "title": "10  Decision Trees",
    "section": "10.2 Introduction",
    "text": "10.2 Introduction\n\n\n\n\n\n\nSupervised learning\n\n\n\n\n\n\nSupervised learning: We train the model using a dataset where we already know the correct answers (the “labels”).\nTraining data: A datset that is used to train (or fit) the model (in order to estimate unknown model parameters).\nTesting data: A dataset that is not used for training a model but used to evaluate the accuracy of the model.\n\n\n\n\n\n\n\n\n\n\nDecision trees\n\n\n\n\n\n\nDecision trees are essentially flowcharts of decisions that help us predict an outcome.\nDecision trees are supervised learning models used for:\n\nClassification (predicting categories)\nRegression (predicting numeric outcomes)\n\nThe term decision tree refers to the general structure\n\nA classification tree is a specific type of decision trees where the target variable is a categorical variable.\nA regression tree is another specific type of decision trees where the target variable is a numerical variable.\n\nWe often use classification and regression trees with the acronym CART to refer to the two types of statistical models/algorithms based on decision trees.\nIn this course, we only focus on classification trees.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "ch10/10-DecisionTrees.html#classification-tree",
    "href": "ch10/10-DecisionTrees.html#classification-tree",
    "title": "10  Decision Trees",
    "section": "10.3 Classification Tree",
    "text": "10.3 Classification Tree\n\n10.3.1 Basic Idea\n\n\n\n\n\n\n\n\nBasic idea of a CART algorithm\n\n\n\n\nInitialization: All observations are in one node at the top of the tree.\nRecursion for each node:\n\nPick a binary decision rule based on variables.\nSplit a node into two child nodes based on the selected decision rule\n\nRecursion stops when no further gain is achieved, or some pre-set stopping rules are met.\n\n\n\n\n\n\n\n\n\n\nTree terminologies\n\n\n\n\n\n\nRoot node: The starting point; represents the entire dataset.\nInternal node: A node that splits the data based on a feature test (a decision).\nBranch/edge: The outcome of the test (e.g., ‘Yes’ or ‘No’).\nLeaf node (terminal node): A node that has no descendants (child nodes).\n\n\n\n\n\n\n\n\nIllustration of a CART algorithm\n\n\n\n\n\n\n\n\n\nRecursive binary splitting\n\n\n\nRecursive binary splitting in decision trees is a top-down, greedy approach\n\nTop-down:\n\nstart at the top of tree with all the data and split the data into subgroups as we move down the tree\nIn contrast to bottom-up: each case starts as its own region and we merge regions together.\n\nGreedy:\n\nAt each step of tree-building, the best split at this step is made.\nIt’s possible that another split may result in an overall better tree\n\n\n\n\n\n\nWe need to define the decision rule as the splitting criterion to split the node. In this course, we focus on the concept Gini index that is used in the original CART algorithm.\nWe also need to determine when the tree stops splitting.\n\n\n\n10.3.2 Splitting Criterion\n\nThe CART algorithm determines the optimal split by searching for the cutoff (or split point) that yields the maximum reduction in impurity (or maximum information gain).\nThis is equivalent to finding the split that results in the lowest weighted average impurity of the resulting child nodes.\n\n\n\n\n\n\n\nDefinition of Gini index\n\n\n\n\n\nDefinition: The Gini index G for a given node m containing observations from K classes is defined as \nG_m:= \\sum_{i=1}^K \\sum_{j\\neq i} p_i p_j = 1 - \\sum_{i=1}^K p_i^2\n\n\nK is the total number of distinct classes (categories or labels) in the target variable.\np_i is the proportion of observations in node m that belong to class i with p_1 + p_2 + \\ldots + p_K = 1.\nThe maximum value of the Gini Index occurs at p_i = 1/K for all i=1,2, \\ldots, K.\nThe minimum value of the Gini Index is 0.\nG_m\\approx 0: The node is approximately perfectly pure. All observations in the node nearly belong to the same class.\nG_m &gt; .5: The node is viewed as highly impure (mixed). Observations are distributed roughly equally across multiple classes.\nGini index quantifies the purity (or homogeneity) of a node in a decision tree.\nThe Gini index, also called Gini impurity, is a measure in the construction of decision trees to evaluate how often a randomly selected element from a data set would be incorrectly labelled if it were labeled according to the distribution of the labels in the data set.\n\n\n\n\n\n\n\n\n\n\nDetermination of Split\n\n\n\n\nFor any potential split point c on a variable X, we divide the data into the left-child node and the right child node.\nThe split point c is chosen to minimize the weighted Gini index\n\n\n\n\nFor each continuous variable x\\in [c_0, c_d],\n\nwe divide the range of variable into d intervals of width w specified by c_0, c_0 + w, c_0 + 2w, \\ldots, c_d;\nwe then choose the cutoff (or split point) c_i that maximizes the impurity of the child nodes.\n\nFor ordinal variables, we examine each possible cutoff.\nFor nominal variables, we put some categories in the left child and the others in the right child.\n\n\n\n10.3.3 Working Example with the Titanic Data\n\n\n\n\n\n\nWorking Example with the iris Data\n\n\n\nThe Titanic dataset contains information about the passengers aboard the RMS Titanic, which sank after striking an iceberg on April 15, 1912, during its maiden voyage from Southampton (UK) to New York City. Out of 2,224 passengers and crew, more than 1,500 people died, making it one of the deadliest peacetime maritime disasters.\nWe will use the ptitanic dataset from the rpart.plot package with 1046 observations on 6 variables\n\npclass: passenger class, unordered factor: 1st 2nd 3rd\nsurvived: died or survived\nsex: male or female\nage: in years\nsibsp: number of siblings or spouses aboard\nparch: number of parents or children aboard\nGoal: We want to predict survived based on the other 5 variables.\n\n\nlibrary(rpart)\nlibrary(rpart.plot)\n\ndata(ptitanic)\ndf = ptitanic\nhead(df)\n\n\n  \n\n\n\n\ndf$age = as.numeric(ptitanic$age)\ndf$sibsp = as.integer(ptitanic$sibsp)\ndf$parch = as.integer(ptitanic$parch)\n\n\nlibrary(GGally)\nlibrary(dplyr)\ndf %&gt;% \n  select(-survived) %&gt;%\n  ggpairs()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain/Test data\n\n\n\n\n\n\nset.seed(4750)\nn = nrow(df)\nfrac = 0.7 # 70% for training \ntrain_idx = sample(seq_len(n), size = floor(frac * n))\n\ndf_train = df[train_idx, ]\ndf_test = df[-train_idx, ]\n\ndim(df_train)\n\n[1] 916   6\n\ndim(df_test)\n\n[1] 393   6\n\n\n\n\n\n\n\n\n\n\n\nFit a classification tree\n\n\n\n\n\n\nfit = rpart(survived ~ ., data = df_train)\nprint(fit)\n\nn= 916 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 916 352 died (0.61572052 0.38427948)  \n   2) sex=male 590 117 died (0.80169492 0.19830508)  \n     4) age&gt;=13.5 545  97 died (0.82201835 0.17798165) *\n     5) age&lt; 13.5 45  20 died (0.55555556 0.44444444)  \n      10) sibsp&gt;=2.5 21   1 died (0.95238095 0.04761905) *\n      11) sibsp&lt; 2.5 24   5 survived (0.20833333 0.79166667) *\n   3) sex=female 326  91 survived (0.27914110 0.72085890)  \n     6) pclass=3rd 149  70 died (0.53020134 0.46979866)  \n      12) sibsp&gt;=2.5 14   1 died (0.92857143 0.07142857) *\n      13) sibsp&lt; 2.5 135  66 survived (0.48888889 0.51111111)  \n        26) parch&gt;=2.5 9   1 died (0.88888889 0.11111111) *\n        27) parch&lt; 2.5 126  58 survived (0.46031746 0.53968254) *\n     7) pclass=1st,2nd 177  12 survived (0.06779661 0.93220339) *\n\n\n\n\n\n\n\n\n\n\n\nVisualizing the tree\n\n\n\n\n\n\ng = rpart.plot(fit,\n  main = \"Classification Tree (unpruned)\"\n)\n\n\n\n\nClassification tree.\n\n\n\n\n\ng = rpart.plot(\n  fit,\n  type = 2,        # split labels on branches\n  extra = 104,     # show class, prob, and percentage of obs.\n  under = TRUE,    # show text information under the box\n  main = \"Classification Tree (unpruned)\"\n)\n\n\n\n\nClassification tree\n\n\n\n\n\nHow to read a node?\n\nEach node shows the predicted class (died or survived), predicted probability of survival, and the percentage of observations in the node\nFor the root node, it shows the percentage of observations that corresponds to survived (here it is .38) in the first figure above. In the second figure, it shows two probabilities (0.62, 0.38), corresponding to two classes died and survived.\n\nIn the first figure (with default setting in rpart.plot), the probability shown in each node always refers to the survived observations.\n.82 .18 (lowest left node): estimated probabilities for each class: died or survived\n59% (lowest left node): percentage of training observations in that node\n\nAt each node, the algorithm searched over all predictors and possible cut points to find the split that maximally reduces node impurity (i.e., inhomogeneity). We will talk about this concept in what follows.\n\n\n\n\n\n\n\n\n\n\nInterpretation of splits\n\n\n\n\n\n\nRoot split (split at tree depth 0): sex = male\n\nleft child: all observations with sex = male\nright child: all observations with sex = female\n\nA split at tree depth 1 (on the left child): age &gt;= 14\n\nThis separates remaining observations further into two classes (died or survived) with the constrain that its left child corresponds to age &gt;= 14 and its right child corresponds to age &lt; 14.\n\n\n\n\n\n\n\n10.3.4 Pruning The Tree\n\nfit1 = rpart(survived ~ ., data = df_train, cp=0.00001)\nrpart.plot(fit1)\n\n\n\n\n\n\n\n\n\nAs we can see, the fitted classification tree has a deeper tree structure. The question then becomes whether this deeper tree is necessary (brings benefits).\nDeep trees (those with large number of tree nodes) can overfit. To address this issue, rpart uses cost-complexity pruning controlled by cp (complexity parameter).\n\n\n\n\n\n\n\noverfit vs underfit\n\n\n\n\n\n\nA statistical model or machine learning model overfits the data if the model’s error on training data is very low (often near zero) and simultaneously the model’s error on testing data is much higher than the training error. Overfitting is often implied by high training accuracy and low testing accuracy.\nOverfitting is often a characteristics of overly complex models. In decision trees, this means the tree with too many deep branches.\nUnderfitting is the opposite of overfitting.\nIn general, we want to build a statistical model/machine learning model that balance model complexity and testing accuracy.\nThis is achieved by cross-validation (CV):\n\nCV is used during training to estimate how well the model will generalize to unseen data (testing data).\nCV divides the data into training data and testing data.\n\n\n\n\n\nLet’s first look at the fitting outputs from fit1, which gives the complexity parameter (CP) and cross-validation error.\n\nprintcp(fit1)\n\n\nClassification tree:\nrpart(formula = survived ~ ., data = df_train, cp = 1e-05)\n\nVariables actually used in tree construction:\n[1] age    parch  pclass sex    sibsp \n\nRoot node error: 352/916 = 0.38428\n\nn= 916 \n\n          CP nsplit rel error  xerror     xstd\n1 0.40909091      0   1.00000 1.00000 0.041824\n2 0.02556818      1   0.59091 0.59091 0.036021\n3 0.01988636      2   0.56534 0.57670 0.035711\n4 0.01420455      4   0.52557 0.54830 0.035064\n5 0.00946970      6   0.49716 0.54830 0.035064\n6 0.00284091      9   0.46875 0.52557 0.034519\n7 0.00142045     15   0.45170 0.55682 0.035262\n8 0.00094697     17   0.44886 0.56818 0.035521\n9 0.00001000     20   0.44602 0.56818 0.035521\n\n\nThe table includes:\n\nnsplit: number of splits\nrel error: training error (relative to root node)\nxerror: cross-validated error\nxstd: standard error of xerror\n\n\nplotcp(fit1)\n\n\n\n\n\n\n\n\n\nNote: a horizontal line in the figure above is drawn 1 standard error above the minimum of the curve.\n\nTo select the best CP, we could use\n\nbest_cp = fit1$cptable[which.min(fit1$cptable[, \"xerror\"]), \"CP\"]\nbest_cp\n\n[1] 0.002840909\n\n\n\nfit1_pruned = prune(fit1, cp = best_cp)\n\nrpart.plot(\n  fit1_pruned,\n  main = \"Pruned Classification Tree\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to interpte the results?\n\n\n\n\n\n\nIn general, there is a clear class boundaries due to sex=female, Pclass&lt;3, and Age&lt;14.\nCompared to males, females have a higher survival rate. This makes sense because\n\nMany women and children were evacuated first.\n\n1st and 2nd class passengers had much better survival chances\n\nProximity to lifeboats\nBetter access to upper decks\n\nChildren had higher survival rates\n\nOften evacuated first with their families\n\n\n\n\n\n\n\n\n\n\n\nRandomness in cross-validation\n\n\n\n\nThe problem with reducing the xerror is that the cross-validation (CV) error is a random quantity. This is because CV randomly divides the data in rpart.\nThere is no guarantee that if we were to fit the sequence of trees again using a different random seed that the same tree would minimize the cross-validation error.\nA more robust alternative to minimum cross-validation error is to use the one standard deviation rule: choose the smallest tree whose cross-validation error is within one standard error of the minimum. Depending on how we define this there are two possible choices.\n\nThe first tree whose point estimate of the cross-validation error falls within the \\pm 1 xstd of the minimum.\nOn the other hand, the standard error lower limit of the tree of size three is within +1 xstd of the minimum.\n\n\n\n\n\n\n10.3.5 Model Evaluation\nFor classification problems, we could evaluate the performance the model by looking at classification errors, which are defined through a confusion matrix. A confusion matrix is a table that summarizes how well a classification model performs by comparing the model’s predicted classes with the actual (true) classes.\n\n\n\n\n\n\nConfusion Matrix\n\n\n\n\n\n\nA confusion matrix is a square table where:\n\nRows represent the true classes\nColumns represent the predicted classes\n\nEach cell shows the number of observations that fall into that true–predicted combination.\n\n\n\n\n\n\n\n\n\n\nExample of Binary Confusion Matrix\n\n\n\n\n\nSuppose a model predicts whether a passenger survived the Titanic disaster.\n\n\n\n\n\n\n\n\nTrue Predicted\nPredicted: Survived\nPredicted: Died\n\n\n\n\nTrue: Survived\nTP (True Positives)\nFN (False Negatives)\n\n\nTrue: Died\nFP (False Positives)\nTN (True Negatives)\n\n\n\n\nTrue Positive (TP): correctly predicted survived.\nTrue Negative (TN): correctly predicted died.\nFalse Positive (FP) (Type I error): predicted survived but actually died. (A “false alarm”)\nFalse Negative (FN) (Type II error): predicted died but actually survived. (A “miss”)\nThe table above does not require strict ordering in columns or rows. One could switch the column Predicted: Survived and the column Predicted: Died but the meaning/interpretation should also be defined appropriately.\n\n\n\n\n\nWe first show the prediction results and confusion matrix based on training data.\n\npred_class_train = predict(fit1_pruned, \n                           type=\"class\"  \n                           )\nconf_mat_train = table(Predicted=pred_class_train, True=df_train$survived)\n\nconf_mat_train\n\n          True\nPredicted  died survived\n  died      501      102\n  survived   63      250\n\n\nWe can also get mis-classification errors using the following R code\n\nprob.results = predict(fit1_pruned)\nhead(prob.results)\n\n           died  survived\n[1,] 0.06779661 0.9322034\n[2,] 0.06779661 0.9322034\n[3,] 0.82201835 0.1779817\n[4,] 0.82201835 0.1779817\n[5,] 0.40625000 0.5937500\n[6,] 0.40625000 0.5937500\n\n\nThe confusion matrix can also be represented in terms of probabilities:\n\nprop.table(table(Predicted=pred_class_train,\n                 True=df_train$survived))\n\n          True\nPredicted        died   survived\n  died     0.54694323 0.11135371\n  survived 0.06877729 0.27292576\n\n\n\npred_class = predict(fit1_pruned, newdat=df_test, type=\"class\")\nconf_mat = table(Predicted=pred_class, True=df_test$survived)\n\nconf_mat\n\n          True\nPredicted  died survived\n  died      215       42\n  survived   30      106\n\n\nTo show probabilities in confusion matrix, one can use the following code\n\nprop.table(conf_mat)\n\n          True\nPredicted        died   survived\n  died     0.54707379 0.10687023\n  survived 0.07633588 0.26972010\n\n\n\n\n\n\n\n\nCommon Metrics Derived from the Confusion Matrix\n\n\n\n\n\n\nAccuracy: The proportion of total predictions that were correct. \n\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n\nSensitivity / Recall (True Positive Rate): Out of all actual positives, how many did the model correctly identify? (Focuses on avoiding Misses) \n\\text{Recall} = \\frac{TP}{TP + FN}\n\nSpecificity (True Negative Rate): Out of all actual negatives, how many did the model correctly identify? (The opposite of Recall, focusing on the negative class) \n\\text{Specificity} = \\frac{TN}{TN + FP}\n\nPrecision: Out of all predicted positives, how many were actually correct? (Focuses on avoiding False Alarms) \n\\text{Precision} = \\frac{TP}{TP + FP}\n\n\n\n\n\n\naccuracy &lt;- mean(pred_class == df_test$survived)\naccuracy\n\n[1] 0.8167939\n\n\n\nTP = conf_mat[1,1]\nFP = conf_mat[2,1]\nFN = conf_mat[1,2]\nTN = conf_mat[2,2]\n\nAccuracy = (TP + TN) / (TP + TN + FP + FN)\nRecall = TP / (TP + FN)\nSpecificity = TN / (TN + FP)\nPrecision = TP / (TP + FP)\n\ncat(\" Accuracy: \", Accuracy, \"\\n\", \n    \"Recall: \", Recall, \"\\n\", \n    \"Specificity: \", Specificity, \"\\n\",\n    \"Precision: \", Precision)\n\n Accuracy:  0.8167939 \n Recall:  0.8365759 \n Specificity:  0.7794118 \n Precision:  0.877551",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "ch10/10-DecisionTrees.html#exercise",
    "href": "ch10/10-DecisionTrees.html#exercise",
    "title": "10  Decision Trees",
    "section": "10.4 Exercise",
    "text": "10.4 Exercise\n\n\n\n\n\n\nBronchopulmonary Dysplasia (BPD) Study\n\n\n\n\n\nThis example is from Biostatistics Casebook (pp. 104-119)\nTraining samples consist of all infants at the Stanford Medical Center between 1962 and 1973 who were diagnosed with respiratory distress syndrome (RDS) and received ventilatory assistance for at least 24 hours (except one infant with incomplete records). Most of these babies were born prematurely and had underdeveloped lungs. Some breathing assistance involving elevated levels of oxygen was needed to keep the babies alive. Bronchiopulmonary dysplasia (BPD) is deterioration of lung tissue (scarring) in infants exposed to a high level of oxygen. A panel of physicians reviewed each case to determine if BPD was present.\nInfants who did not survive for at least 72 hours were excluded from the analysis because there was not enough time for BPD to develop. One additional infant was excluded due to incomplete records. This reduced the sample from 299 to 248 babies, including 78 with BPD and 170 without BPD. The dataset is available in the file bpd.csv.\nGeneral background variables:\n\nSex (0=female, 1=male)\nYOB: year of birth (coded from 62 to 73)\nAPGAR: one minute APGAR score (0 to 10 with 10 as the most healthy)\nGEST: gestational age (weeks × 10)\nBWT: birth weight (grams)\nAGSYM: age at onset of RDS (hours × 10)\nRDS: severity of initial X-ray for RDS (0 to 5=most severe)\nAGVEN: Age at onset of ventilation (hours)\nVENTL: total hours on the ventilator\nLOWO2: hours of exposure to (21-39%) levels of oxygen\nMEDO2: hours of exposure to (40-79%) levels of oxygen\nHIO2: hours of exposure to (80-100%) levels of oxygen\nINTUB: hours of endotracheal intubation\nResponse variable: BPD (coded 1=yes, 2=no)\n\nThe goal is to perform classification using the classification tree method.\n\n\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\nFirst convert categorical variables like sex and rds to factors. There is no need to convert categorical variables into zero-one variables, because it would not change the tree that is produced. Read the data into a data frame and create factors.\n\n\nView Solution\nbpdr = read.csv(\"bpd.csv\", header=T)\n\nbpdr$sex = as.factor(bpdr$sex)\nbpdr$rds = as.ordered(bpdr$rds)\n \nhead(bpdr)\n\n\n\n  \n\n\n\nCreate a factor to distinguish the two populations with labels “BPD” and “No BPD”.\n\n\nView Solution\nbpdr$y[bpdr$bpd==1] = 'BPD'\nbpdr$y[bpdr$bpd==2] = 'No BPD'\nbpdr$y = as.factor(bpdr$y)\n\n\nSet a seed for starting a random number generator to generate random numbers used in crossvalidations to prune trees and estimate misclassification probabilities.\n\n\nView Solution\nset.seed(4750)\n \nbpd.rp = rpart(y ~ sex+yob+gest+bwt+agsym+agven+intub+\n                 ventl+lowo2+medo2+hio2+rds, \n               data=bpdr, cp=0.0001)\n\nsummary(bpd.rp)\n\n\nCall:\nrpart(formula = y ~ sex + yob + gest + bwt + agsym + agven + \n    intub + ventl + lowo2 + medo2 + hio2 + rds, data = bpdr, \n    cp = 1e-04)\n  n= 248 \n\n           CP nsplit rel error    xerror       xstd\n1 0.538461538      0 1.0000000 1.0000000 0.09374569\n2 0.038461538      1 0.4615385 0.6666667 0.08218816\n3 0.025641026      3 0.3846154 0.5256410 0.07499920\n4 0.003205128      4 0.3589744 0.5128205 0.07425765\n5 0.000100000      8 0.3461538 0.5128205 0.07425765\n\nVariable importance\nmedo2 intub ventl lowo2  hio2 agven   yob \n   31    19    19    13    10     6     1 \n\nNode number 1: 248 observations,    complexity param=0.5384615\n  predicted class=No BPD  expected loss=0.3145161  P(node) =1\n    class counts:    78   170\n   probabilities: 0.315 0.685 \n  left son=2 (62 obs) right son=3 (186 obs)\n  Primary splits:\n      medo2 &lt; 183    to the right, improve=45.43011, (0 missing)\n      ventl &lt; 220.5  to the right, improve=43.28307, (1 missing)\n      intub &lt; 271    to the right, improve=39.85744, (0 missing)\n      lowo2 &lt; 437.5  to the right, improve=29.52134, (0 missing)\n      hio2  &lt; 153    to the right, improve=17.09879, (0 missing)\n  Surrogate splits:\n      intub &lt; 286.5  to the right, agree=0.871, adj=0.484, (0 split)\n      ventl &lt; 301.5  to the right, agree=0.863, adj=0.452, (0 split)\n      lowo2 &lt; 429    to the right, agree=0.831, adj=0.323, (0 split)\n      agven &lt; 64.5   to the right, agree=0.786, adj=0.145, (0 split)\n      hio2  &lt; 305    to the right, agree=0.774, adj=0.097, (0 split)\n\nNode number 2: 62 observations,    complexity param=0.02564103\n  predicted class=BPD     expected loss=0.1612903  P(node) =0.25\n    class counts:    52    10\n   probabilities: 0.839 0.161 \n  left son=4 (48 obs) right son=5 (14 obs)\n  Primary splits:\n      ventl &lt; 220.5  to the right, improve=6.034381, (1 missing)\n      intub &lt; 221.5  to the right, improve=4.680002, (0 missing)\n      hio2  &lt; 45.5   to the right, improve=4.466501, (0 missing)\n      medo2 &lt; 443.5  to the right, improve=4.181601, (0 missing)\n      lowo2 &lt; 434    to the right, improve=1.743506, (0 missing)\n  Surrogate splits:\n      intub &lt; 221.5  to the right, agree=0.984, adj=0.929, (1 split)\n      medo2 &lt; 243    to the right, agree=0.820, adj=0.214, (0 split)\n      agven &lt; 129    to the left,  agree=0.803, adj=0.143, (0 split)\n      lowo2 &lt; 1928   to the left,  agree=0.787, adj=0.071, (0 split)\n      hio2  &lt; 0.5    to the right, agree=0.787, adj=0.071, (0 split)\n\nNode number 3: 186 observations,    complexity param=0.03846154\n  predicted class=No BPD  expected loss=0.1397849  P(node) =0.75\n    class counts:    26   160\n   probabilities: 0.140 0.860 \n  left son=6 (23 obs) right son=7 (163 obs)\n  Primary splits:\n      hio2  &lt; 159.5  to the right, improve=9.500455, (0 missing)\n      ventl &lt; 223.5  to the right, improve=5.863258, (0 missing)\n      intub &lt; 229    to the right, improve=4.550538, (0 missing)\n      lowo2 &lt; 321    to the right, improve=3.948499, (0 missing)\n      yob   &lt; 68.5   to the left,  improve=2.688830, (0 missing)\n\nNode number 4: 48 observations\n  predicted class=BPD     expected loss=0.04166667  P(node) =0.1935484\n    class counts:    46     2\n   probabilities: 0.958 0.042 \n\nNode number 5: 14 observations\n  predicted class=No BPD  expected loss=0.4285714  P(node) =0.05645161\n    class counts:     6     8\n   probabilities: 0.429 0.571 \n\nNode number 6: 23 observations,    complexity param=0.03846154\n  predicted class=BPD     expected loss=0.4347826  P(node) =0.09274194\n    class counts:    13    10\n   probabilities: 0.565 0.435 \n  left son=12 (10 obs) right son=13 (13 obs)\n  Primary splits:\n      medo2 &lt; 45     to the right, improve=1.950502, (0 missing)\n      rds   splits as  LLLLRRR,    improve=1.950502, (0 missing)\n      agven &lt; 32.5   to the right, improve=1.715062, (0 missing)\n      lowo2 &lt; 4.5    to the right, improve=1.715062, (0 missing)\n      yob   &lt; 68.5   to the left,  improve=1.590062, (0 missing)\n  Surrogate splits:\n      agven &lt; 37.5   to the right, agree=0.826, adj=0.6, (0 split)\n      lowo2 &lt; 17.5   to the right, agree=0.783, adj=0.5, (0 split)\n      intub &lt; 172    to the left,  agree=0.739, adj=0.4, (0 split)\n      ventl &lt; 169.5  to the left,  agree=0.739, adj=0.4, (0 split)\n      yob   &lt; 65.5   to the left,  agree=0.696, adj=0.3, (0 split)\n\nNode number 7: 163 observations,    complexity param=0.003205128\n  predicted class=No BPD  expected loss=0.0797546  P(node) =0.6572581\n    class counts:    13   150\n   probabilities: 0.080 0.920 \n  left son=14 (13 obs) right son=15 (150 obs)\n  Primary splits:\n      lowo2 &lt; 527.5  to the right, improve=4.1181750, (0 missing)\n      ventl &lt; 250.5  to the right, improve=1.9012610, (0 missing)\n      intub &lt; 433.5  to the right, improve=1.7798600, (0 missing)\n      medo2 &lt; 82.5   to the right, improve=1.6898960, (0 missing)\n      gest  &lt; 352.5  to the left,  improve=0.5030691, (1 missing)\n  Surrogate splits:\n      intub &lt; 390    to the right, agree=0.945, adj=0.308, (0 split)\n      ventl &lt; 288.5  to the right, agree=0.939, adj=0.231, (0 split)\n\nNode number 12: 10 observations\n  predicted class=BPD     expected loss=0.2  P(node) =0.04032258\n    class counts:     8     2\n   probabilities: 0.800 0.200 \n\nNode number 13: 13 observations\n  predicted class=No BPD  expected loss=0.3846154  P(node) =0.05241935\n    class counts:     5     8\n   probabilities: 0.385 0.615 \n\nNode number 14: 13 observations\n  predicted class=No BPD  expected loss=0.4615385  P(node) =0.05241935\n    class counts:     6     7\n   probabilities: 0.462 0.538 \n\nNode number 15: 150 observations,    complexity param=0.003205128\n  predicted class=No BPD  expected loss=0.04666667  P(node) =0.6048387\n    class counts:     7   143\n   probabilities: 0.047 0.953 \n  left son=30 (41 obs) right son=31 (109 obs)\n  Primary splits:\n      ventl &lt; 146    to the right, improve=1.7369110, (0 missing)\n      intub &lt; 148.5  to the right, improve=1.4771010, (0 missing)\n      medo2 &lt; 158    to the right, improve=1.2132460, (0 missing)\n      agven &lt; 4.5    to the left,  improve=0.4336516, (0 missing)\n      rds   splits as  RRRRRLL,    improve=0.4148485, (0 missing)\n  Surrogate splits:\n      intub &lt; 146    to the right, agree=0.953, adj=0.829, (0 split)\n      lowo2 &lt; 206    to the right, agree=0.753, adj=0.098, (0 split)\n      agven &lt; 5.5    to the left,  agree=0.747, adj=0.073, (0 split)\n      medo2 &lt; 133    to the right, agree=0.747, adj=0.073, (0 split)\n      hio2  &lt; 144.5  to the right, agree=0.733, adj=0.024, (0 split)\n\nNode number 30: 41 observations,    complexity param=0.003205128\n  predicted class=No BPD  expected loss=0.1707317  P(node) =0.1653226\n    class counts:     7    34\n   probabilities: 0.171 0.829 \n  left son=60 (27 obs) right son=61 (14 obs)\n  Primary splits:\n      lowo2 &lt; 4      to the right, improve=1.2393860, (0 missing)\n      medo2 &lt; 81.5   to the right, improve=1.2393860, (0 missing)\n      agven &lt; 31     to the right, improve=1.1223610, (0 missing)\n      ventl &lt; 155    to the left,  improve=1.1223610, (0 missing)\n      yob   &lt; 69.5   to the left,  improve=0.8294531, (0 missing)\n  Surrogate splits:\n      hio2  &lt; 82.5   to the left,  agree=0.805, adj=0.429, (0 split)\n      bwt   &lt; 1078   to the right, agree=0.707, adj=0.143, (0 split)\n      agven &lt; 12.5   to the right, agree=0.707, adj=0.143, (0 split)\n      yob   &lt; 67.5   to the right, agree=0.683, adj=0.071, (0 split)\n      gest  &lt; 275    to the right, agree=0.683, adj=0.071, (0 split)\n\nNode number 31: 109 observations\n  predicted class=No BPD  expected loss=0  P(node) =0.4395161\n    class counts:     0   109\n   probabilities: 0.000 1.000 \n\nNode number 60: 27 observations,    complexity param=0.003205128\n  predicted class=No BPD  expected loss=0.2592593  P(node) =0.108871\n    class counts:     7    20\n   probabilities: 0.259 0.741 \n  left son=120 (7 obs) right son=121 (20 obs)\n  Primary splits:\n      hio2  &lt; 68.5   to the right, improve=1.8417990, (0 missing)\n      medo2 &lt; 76     to the right, improve=1.5282650, (0 missing)\n      lowo2 &lt; 99     to the left,  improve=1.4158250, (0 missing)\n      ventl &lt; 161.5  to the left,  improve=0.9259259, (0 missing)\n      intub &lt; 195    to the left,  improve=0.7879528, (0 missing)\n  Surrogate splits:\n      yob   &lt; 68.5   to the left,  agree=0.815, adj=0.286, (0 split)\n      gest  &lt; 390    to the right, agree=0.815, adj=0.286, (0 split)\n      bwt   &lt; 2976.5 to the right, agree=0.815, adj=0.286, (0 split)\n      ventl &lt; 150    to the left,  agree=0.815, adj=0.286, (0 split)\n      medo2 &lt; 170.5  to the right, agree=0.815, adj=0.286, (0 split)\n\nNode number 61: 14 observations\n  predicted class=No BPD  expected loss=0  P(node) =0.05645161\n    class counts:     0    14\n   probabilities: 0.000 1.000 \n\nNode number 120: 7 observations\n  predicted class=BPD     expected loss=0.4285714  P(node) =0.02822581\n    class counts:     4     3\n   probabilities: 0.571 0.429 \n\nNode number 121: 20 observations\n  predicted class=No BPD  expected loss=0.15  P(node) =0.08064516\n    class counts:     3    17\n   probabilities: 0.150 0.850 \n\n\nDisplay the tree.\n\n\nView Solution\nrpart.plot(bpd.rp)\n\n\n\n\n\n\n\n\n\nDisplay the classification training errors by this tree. Print a brief description of what happens at each node\n\n\nView Solution\nprint(bpd.rp, digits=3)\n\n\nn= 248 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 248 78 No BPD (0.3145 0.6855)  \n    2) medo2&gt;=183 62 10 BPD (0.8387 0.1613)  \n      4) ventl&gt;=220 48  2 BPD (0.9583 0.0417) *\n      5) ventl&lt; 220 14  6 No BPD (0.4286 0.5714) *\n    3) medo2&lt; 183 186 26 No BPD (0.1398 0.8602)  \n      6) hio2&gt;=160 23 10 BPD (0.5652 0.4348)  \n       12) medo2&gt;=45 10  2 BPD (0.8000 0.2000) *\n       13) medo2&lt; 45 13  5 No BPD (0.3846 0.6154) *\n      7) hio2&lt; 160 163 13 No BPD (0.0798 0.9202)  \n       14) lowo2&gt;=528 13  6 No BPD (0.4615 0.5385) *\n       15) lowo2&lt; 528 150  7 No BPD (0.0467 0.9533)  \n         30) ventl&gt;=146 41  7 No BPD (0.1707 0.8293)  \n           60) lowo2&gt;=4 27  7 No BPD (0.2593 0.7407)  \n            120) hio2&gt;=68.5 7  3 BPD (0.5714 0.4286) *\n            121) hio2&lt; 68.5 20  3 No BPD (0.1500 0.8500) *\n           61) lowo2&lt; 4 14  0 No BPD (0.0000 1.0000) *\n         31) ventl&lt; 146 109  0 No BPD (0.0000 1.0000) *\n\n\nCompute estimates of the misclassification probabilities\n\n\nView Solution\nbpd.prob = predict(bpd.rp)\nhead(bpd.prob)\n\n\n        BPD     No BPD\n1 0.0000000 1.00000000\n2 0.0000000 1.00000000\n3 0.4285714 0.57142857\n4 0.9583333 0.04166667\n5 0.0000000 1.00000000\n6 0.0000000 1.00000000\n\n\nMake a table of classification results based on confusion matrix.\n\n\nView Solution\nconf.matrix &lt;- table(bpdr$y, predict(bpd.rp, type=\"class\"))\n\nconf.matrix\n\n\n        \n         BPD No BPD\n  BPD     58     20\n  No BPD   7    163\n\n\nView Solution\n# one can also show probabilities\nprop.table(conf.matrix)\n\n\n        \n                BPD     No BPD\n  BPD    0.23387097 0.08064516\n  No BPD 0.02822581 0.65725806\n\n\n\n\nCode\nconf.matrix &lt;- table(bpdr$y, predict(bpd.rp, type=\"class\"))\n\nconf.matrix\n\n\n        \n         BPD No BPD\n  BPD     58     20\n  No BPD   7    163",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  }
]