[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Multivariate Data Analysis",
    "section": "",
    "text": "Preface\nThis book contains the course notes for STAT 4750/5750 (Introduction to Multivariate Data Analysis) at Iowa State University. This course is designed for undergraduate students in statistics and data science and graduate students from the applied sciences with majors outside statistics. The prerequisite for this course includes STAT 3010 or STAT 3260 (for undergraduates) or STAT 5101 for graduate students. Knowledge of matrix algebra is recommended but not required to understand the topics covered in this book.\nThe course STAT 3010 (Intermediate Statistical Concepts and Methods) covers statistical concepts and methods used in the analysis of observational data. Topics include analysis of single sample, two sample and paired sample data; simple and multiple linear regression; model building and analysis of residuals; one-way ANOVA, tests of independence for contingency tables, and logistic regression.\nThe course STAT 3260 (Introduction to Business Statistics II) covers multiple regression, regression diagnostics, model building, applications in analysis of variance and time series, random variables, conditional probability, and data visualization.\nThe course STAT 5101 (Statistical Methods for Research Workers) was renamed from the previous course STAT 5870 starting Fall 2025. STAT 5101 is a first course in statistics for graduate students from the applied sciences, and covers topics including basic experimental designs and analysis of variance, analysis of categorical data, logistic and log-linear regression, likelihood-based inference, and the use of simulation.\nStudents who have the needed backgrounds shall find the statistical concepts and methods in this book easy to follow and understand. All the methods are illustrated with various examples with step-by-step solutions and extensive R code. Exercises are also given at each chapter to help students better understand the statistical methods and apply these methods for real data analysis by adapting the corresponding R code in the book.\nThe materials in this book are largely influenced by previous course notes taught by several instructors including Yumou Qiu and Kenneth Koehler in the Department of Statistics at Iowa State University."
  },
  {
    "objectID": "ch1/01-intro.html#course-outline",
    "href": "ch1/01-intro.html#course-outline",
    "title": "1  Introduction to Multivariate Data",
    "section": "1.1 Course Outline",
    "text": "1.1 Course Outline\n\nIntroduction to Multivariate Data\nNumerical Summaries and Visualization of Multivariate Data\nMultivariate Normal Distribution\nComparing Centers of Distributions\n(Hotelling T^2, MANOVA, Repeated Measures)\nPrincipal Component Analysis\n(Summarizing data in lower dimensions)\nExploratory Factor Analysis\n(What to do when the variables of interest cannot be directly observed.)\nMulti-Dimensional Scaling\nCluster Analysis\nDiscriminant Analysis and Classification\n(Including linear discriminants, logistic regression, classification trees, and random forests)"
  },
  {
    "objectID": "ch1/01-intro.html#objectives-of-multivariate-analysis",
    "href": "ch1/01-intro.html#objectives-of-multivariate-analysis",
    "title": "1  Introduction to Multivariate Data",
    "section": "1.2 Objectives of Multivariate Analysis",
    "text": "1.2 Objectives of Multivariate Analysis\n\nUnderstand dependencies among variables: What is the nature of associations among variables?\nPrediction: If variables are associated, then we might be able to predict the value of some of them given information on the others. (Statistical inference)\nHypothesis testing: Are differences in sets of response means for two or more groups large enough to be distinguished from sampling variation? (Statistical inference)\nDimensionality reduction: Can we reduce the dimensionality of the problem by considering a small number of (linear) combinations of a large number of measurements without losing important information?\n\nPrincipal Components\nFactor Analysis\nMultidimensional Scaling\n\nGrouping (Cluster Analysis): Identify groups of “similar” units using a common set of measured traits.\nClassification: Classify units into previously defined groups using a common set of measured traits."
  },
  {
    "objectID": "ch1/01-intro.html#introduction-to-r-and-rstudio",
    "href": "ch1/01-intro.html#introduction-to-r-and-rstudio",
    "title": "1  Introduction to Multivariate Data",
    "section": "1.3 Introduction to R and RStudio",
    "text": "1.3 Introduction to R and RStudio\n\n1.3.1 Getting Started\nTo download R, please choose your preferred CRAN mirror. Here I recommend the mirror 0-Cloud available at https://cloud.r-project.org/.\nInstall R on macOS\nFor macOS users, below are the steps you need to install R.\n\nInstall Xcode (e.g., via the App Store). Skip this step if your Mac already has Xcode installed.\nInstall XQuartz.\nGo to https://cloud.r-project.org/ and click “Download R for macOS.”\nClick on the download link for the latest R version for macOS and follow the instruction.\n\nInstall R on Windows:\nFor Windows users, the following instructions guide you to install R.\n\nGo to https://cloud.r-project.org/ and click “Download R for Windows.”\nClick “install R for the first time.”\nChoose a download mirror (any CRAN mirror will work).\nClick on the download link for the latest R version for Windows and follow the instruction\n\nInstall RStudio\nRStudio has multiple panes in the window, open by default: one for writing and editing code, another for executing it, another for plots produced or help, and another that lists the R data objects available. RStudio can be download for free at https://posit.co/download/rstudio-desktop/.\n\n\n1.3.2 Introduction to R Programming\n\n\n\n\n\n\nSome Tips: R Working Environment\n\n\n\n\n\nWhenever you work with R for data analysis, it is recommended to load all the packages used in the data analysis pipeline before the function sessionInfo(). In addition, if random numbers are generated, it is also recommended to set random seed to ensure reproducibility using set.seed().\n\n\nR Working Environment\n# load packages \nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(lubridate)\n\nsessionInfo()\n\n\nR version 4.4.3 (2025-02-28)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.7.6\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] lubridate_1.9.4 readr_2.1.5     tidyr_1.3.1     dplyr_1.1.4    \n[5] ggplot2_3.5.2  \n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.5       cli_3.6.3         knitr_1.49        rlang_1.1.5      \n [5] xfun_0.50         purrr_1.0.2       generics_0.1.3    jsonlite_1.8.9   \n [9] glue_1.8.0        colorspace_2.1-1  htmltools_0.5.8.1 hms_1.1.3        \n[13] scales_1.3.0      rmarkdown_2.29    grid_4.4.3        evaluate_1.0.3   \n[17] munsell_0.5.1     tibble_3.2.1      tzdb_0.4.0        fastmap_1.2.0    \n[21] yaml_2.3.10       lifecycle_1.0.4   compiler_4.4.3    timechange_0.3.0 \n[25] htmlwidgets_1.6.4 pkgconfig_2.0.3   rstudioapi_0.17.1 digest_0.6.37    \n[29] R6_2.5.1          tidyselect_1.2.1  pillar_1.10.1     magrittr_2.0.3   \n[33] withr_3.0.2       tools_4.4.3       gtable_0.3.6     \n\n\n\n\n\n\n\n\n\n\n\nR is a Calculator\n\n\n\n\n\n\nCode\n# Numeric, logical, character types\n3.14159             # numeric\n\n[1] 3.14159\n\nCode\nT                   # logical, can also be TRUE\n\n[1] TRUE\n\nCode\nF                   # logical, can also be FALSE \n\n[1] FALSE\n\nCode\n\"Stat 4750/5750\"    # character\n\n[1] “Stat 4750/5750”\n\nCode\n# Basic arithmetic operators\n1 + 2\n\n[1] 3\n\nCode\n20 - 3\n\n[1] 17\n\nCode\n2 * 6\n\n[1] 12\n\nCode\n4 / 3\n\n[1] 1.333333\n\nCode\n2 ^ 4\n\n[1] 16\n\nCode\n-3.14\n\n[1] -3.14\n\nCode\n(1 + 2) * (3 / 4)  # use parenthesis\n\n[1] 2.25\n\nCode\n2 == 1\n\n[1] FALSE\n\nCode\n# Vector\nc()  # empty vector\n\nNULL\n\nCode\nc(1, 2, 3, 4)\n\n[1] 1 2 3 4\n\nCode\n1:4\n\n[1] 1 2 3 4\n\nCode\n# Look up the function help, 3 ways:\n# Following 3 ways work for most functions\n          # 1. search in help pane\n?sum      # 2. use ?\nsum(1:4)  # 3. move your cursor to the function, then press F1, the easiest way\n\n[1] 10\n\nCode\n# The 3rd way doesn't work for functions having some symbols, like +, ==, %*%\n          # 1. search in help pane\n?`+`      # 2. use ? but wrap the symbol with ``\n\n\n\n\n\n\n\n\n\n\nR Objects\n\n\n\n\n\n\n\nCode\n# Assignment operator &lt;-\n\n# Numeric class\nmynumbers &lt;- 5:12\nmynumbers\n\n\n[1]  5  6  7  8  9 10 11 12\n\n\nCode\nmynumbers + 2\n\n\n[1]  7  8  9 10 11 12 13 14\n\n\nCode\nmynumbers * 10\n\n\n[1]  50  60  70  80  90 100 110 120\n\n\nCode\ntypeof(mynumbers)      # type of an object\n\n\n[1] \"integer\"\n\n\nCode\n# check if is numeric (both integer and double type are numeric)\nis.numeric(mynumbers)  \n\n\n[1] TRUE\n\n\nCode\nis.vector(mynumbers)   # check if is vector\n\n\n[1] TRUE\n\n\nCode\nlength(mynumbers)      # length of an object\n\n\n[1] 8\n\n\nCode\n# Character class\nmytext &lt;- c(\"hello\", \"class\", \"four\") \nmytext\n\n\n[1] \"hello\" \"class\" \"four\" \n\n\nCode\ntypeof(mytext)\n\n\n[1] \"character\"\n\n\nCode\nis.numeric(mytext)\n\n\n[1] FALSE\n\n\nCode\nis.character(mytext)\n\n\n[1] TRUE\n\n\nCode\nis.character(mynumbers)\n\n\n[1] FALSE\n\n\nCode\nis.vector(mytext)\n\n\n[1] TRUE\n\n\nCode\nlength(mytext)\n\n\n[1] 3\n\n\nCode\n# Logical class\nmylogic &lt;- c(TRUE, FALSE, TRUE, TRUE)\ntypeof(mylogic)\n\n\n[1] \"logical\"\n\n\nCode\n# Factor class\ngender &lt;- c(\"male\", \"female\", \"female\", \"female\", \"male\") \ngender\n\n\n[1] \"male\"   \"female\" \"female\" \"female\" \"male\"  \n\n\nCode\nis.vector(gender)\n\n\n[1] TRUE\n\n\nCode\nis.character(gender)\n\n\n[1] TRUE\n\n\nCode\nis.vector(gender)\n\n\n[1] TRUE\n\n\nCode\nis.factor(gender)\n\n\n[1] FALSE\n\n\nCode\ngenderf &lt;- factor(gender)\ngenderf\n\n\n[1] male   female female female male  \nLevels: female male\n\n\nCode\nis.character(genderf)\n\n\n[1] FALSE\n\n\nCode\nis.factor(genderf)\n\n\n[1] TRUE\n\n\nCode\nsummary(mynumbers)  # numeric: 5-number summary\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   5.00    6.75    8.50    8.50   10.25   12.00 \n\n\nCode\nsummary(mylogic)    # logical: count how many T and F\n\n\n   Mode   FALSE    TRUE \nlogical       1       3 \n\n\nCode\nsummary(mytext)     # character:\n\n\n   Length     Class      Mode \n        3 character character \n\n\nCode\nsummary(genderf)    # factor: count the frequency\n\n\nfemale   male \n     3      2 \n\n\nCode\n# List class\nmylist &lt;- list(1, \"ABC\", FALSE)\nmylist\n\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] \"ABC\"\n\n[[3]]\n[1] FALSE\n\n\nCode\nis.vector(mylist)\n\n\n[1] TRUE\n\n\nCode\nis.list(mylist)\n\n\n[1] TRUE\n\n\nCode\nlength(mylist)\n\n\n[1] 3\n\n\n\n\n\n\n\n\n\n\n\nWorking with Matrices\n\n\n\n\n\n\n\nCode\n# All elements should be the same type\nmydata &lt;- matrix(c(140, 120, 160, 145, 125, 65, 60, 63, 66, 61), ncol = 2, byrow = FALSE) \nmydata\n\n\n     [,1] [,2]\n[1,]  140   65\n[2,]  120   60\n[3,]  160   63\n[4,]  145   66\n[5,]  125   61\n\n\nCode\ndim(mydata)  # dimensions\n\n\n[1] 5 2\n\n\nCode\ncolnames(mydata) &lt;- c(\"Weight.lbs\", \"Height.in\")\nrownames(mydata) &lt;- c(\"a\", \"b\", \"c\", \"d\", \"e\")\nmydata\n\n\n  Weight.lbs Height.in\na        140        65\nb        120        60\nc        160        63\nd        145        66\ne        125        61\n\n\nCode\nsummary(mydata)  # summary of each column\n\n\n   Weight.lbs    Height.in \n Min.   :120   Min.   :60  \n 1st Qu.:125   1st Qu.:61  \n Median :140   Median :63  \n Mean   :138   Mean   :63  \n 3rd Qu.:145   3rd Qu.:65  \n Max.   :160   Max.   :66  \n\n\n\n\n\n\n\n\n\n\n\nWorking with Data Frames\n\n\n\n\n\n\n\nData Frames\n# Different columns can have different types\ndf &lt;- data.frame(Weight.lbs = c(140, 120, 160, 145, 125, 180, 165),\n                 Height.in = c(65, 60, 63, 66, 61, 70, 68),\n                 Gender = c(rep(\"Female\", 5), rep(\"Male\", 2)), stringsAsFactors = T)\ndf\n\n\n  Weight.lbs Height.in Gender\n1        140        65 Female\n2        120        60 Female\n3        160        63 Female\n4        145        66 Female\n5        125        61 Female\n6        180        70   Male\n7        165        68   Male\n\n\nData Frames\ndim(df)\n\n\n[1] 7 3\n\n\nData Frames\nhead(df)     # the first 6 rows\n\n\n  Weight.lbs Height.in Gender\n1        140        65 Female\n2        120        60 Female\n3        160        63 Female\n4        145        66 Female\n5        125        61 Female\n6        180        70   Male\n\n\nData Frames\nsummary(df)  # summary of each column\n\n\n   Weight.lbs      Height.in        Gender \n Min.   :120.0   Min.   :60.00   Female:5  \n 1st Qu.:132.5   1st Qu.:62.00   Male  :2  \n Median :145.0   Median :65.00             \n Mean   :147.9   Mean   :64.71             \n 3rd Qu.:162.5   3rd Qu.:67.00             \n Max.   :180.0   Max.   :70.00             \n\n\nData Frames\nstr(df)      # structure of the data frame\n\n\n'data.frame':   7 obs. of  3 variables:\n $ Weight.lbs: num  140 120 160 145 125 180 165\n $ Height.in : num  65 60 63 66 61 70 68\n $ Gender    : Factor w/ 2 levels \"Female\",\"Male\": 1 1 1 1 1 2 2\n\n\na tibble is a modern implementation of a data frame, designed to be more user-friendly and efficient, especially when working with large datasets.\n\n\ntibble\n# install.packages(\"tibble\")\nlibrary(tibble)\n# create a tibble from an existing object\nas_tibble(df) \n\n\n# A tibble: 7 × 3\n  Weight.lbs Height.in Gender\n       &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt; \n1        140        65 Female\n2        120        60 Female\n3        160        63 Female\n4        145        66 Female\n5        125        61 Female\n6        180        70 Male  \n7        165        68 Male  \n\n\ntibble\n# create a new tibble \ntibble(x=1:5, \n       y=1.0,\n       z=x^2+y)\n\n\n# A tibble: 5 × 3\n      x     y     z\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     1     2\n2     2     1     5\n3     3     1    10\n4     4     1    17\n5     5     1    26\n\n\ntibble\n# define a row-by-row tibble\ntribble(\n  ~x, ~y, ~z,\n  \"a\", 2, 1,\n  \"b\", 3, 4\n)\n\n\n# A tibble: 2 × 3\n  x         y     z\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 a         2     1\n2 b         3     4\n\n\n\n\n\n\n\n\n\n\n\nOperations on Matrix and Data Frame\n\n\n\n\n\n\n\nCode\nmydata[, 1]                     # extract the first column\n\n\n  a   b   c   d   e \n140 120 160 145 125 \n\n\nCode\nmydata[1, ]                     # extract the first row\n\n\nWeight.lbs  Height.in \n       140         65 \n\n\nCode\nmydata[1:2, ]                   # extract the first two rows\n\n\n  Weight.lbs Height.in\na        140        65\nb        120        60\n\n\nCode\nmydata[3, 2]                    # extract the element in the third row and the second column\n\n\n[1] 63\n\n\nCode\ndf[seq(1, 7, 2), ]              # extract every second row\n\n\n  Weight.lbs Height.in Gender\n1        140        65 Female\n3        160        63 Female\n5        125        61 Female\n7        165        68   Male\n\n\nCode\nseq(1, 7, 2)\n\n\n[1] 1 3 5 7\n\n\nCode\nsubset(df, Gender == \"Male\")    # select based on value\n\n\n  Weight.lbs Height.in Gender\n6        180        70   Male\n7        165        68   Male\n\n\nCode\nt(mydata)                       # transpose of matrix\n\n\n             a   b   c   d   e\nWeight.lbs 140 120 160 145 125\nHeight.in   65  60  63  66  61\n\n\nCode\nmydata[, 1] * mydata[, 2]       # element-wise multiplication\n\n\n    a     b     c     d     e \n 9100  7200 10080  9570  7625 \n\n\nCode\nmydata %*% t(mydata)            # matrix multiplication\n\n\n      a     b     c     d     e\na 23825 20700 26495 24590 21465\nb 20700 18000 22980 21360 18660\nc 26495 22980 29569 27358 23843\nd 24590 21360 27358 25381 22151\ne 21465 18660 23843 22151 19346\n\n\n\n\n\n\n\n\n\n\n\nPlotting with the ggplot2 Package\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\ndata(\"USArrests\")\ndat = USArrests %&gt;% as_tibble()\ndat %&gt;% \n  ggplot(aes(x=UrbanPop, y=Murder)) + \n  geom_point()\n\n\n\n\n\n\n\nCode\ndat_long = dat %&gt;%\n  tidyr::pivot_longer(1:4,names_to = \"Variable\", values_to = \"Value\") \nhead(dat_long)\n\n\n# A tibble: 6 × 2\n  Variable Value\n  &lt;chr&gt;    &lt;dbl&gt;\n1 Murder    13.2\n2 Assault  236  \n3 UrbanPop  58  \n4 Rape      21.2\n5 Murder    10  \n6 Assault  263  \n\n\nCode\n# faceted histogram\nggplot(dat_long, aes(Value)) + \n  geom_histogram(bins=20, fill=\"grey80\", color=\"white\") + \n  facet_wrap(~Variable, scales=\"free\") + \n  labs(title = \"Distributions by Variable\")"
  },
  {
    "objectID": "ch1/01-intro.html#organization-of-data-and-notation",
    "href": "ch1/01-intro.html#organization-of-data-and-notation",
    "title": "1  Introduction to Multivariate Data",
    "section": "1.4 Organization of Data and Notation",
    "text": "1.4 Organization of Data and Notation\n\nn = the number of observations or units\n\np = the number of variables measured on each unit\n\nIf p = 1, then we are back in the usual univariate setting\n\nx_{ik} = the i-th observation of the k-th variable\n\n\n\\text{Observations} \\quad \\overset{\\text{Variables}}{\n\\begin{bmatrix}\nx_{11} & x_{12} & \\cdots & x_{1p} \\\\\nx_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{bmatrix}\n}\n\n\nData matrix:  X_{n \\times p}=\\left[ \\begin{array}{cccc} x_{11} & x_{12} & \\cdots & x_{1p} \\\\ x_{21} & x_{22} & \\cdots & x_{2p} \\\\ \\vdots & \\vdots & & \\vdots  \\\\ x_{n1} & x_{n2} & \\cdots & x_{np}\\end{array} \\right ] \nThis can be written as n rows or as p columns \nX_{n \\times p} =\n\\begin{bmatrix}\n\\mathbf{x}_1^{\\prime} \\\\\n\\mathbf{x}_2^{\\prime} \\\\\n\\vdots \\\\\n\\mathbf{x}_n^{\\prime}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf{x}_1^{\\top} \\\\\n\\mathbf{x}_2^{\\top} \\\\\n\\vdots \\\\\n\\mathbf{x}_n^{\\top}\n\\end{bmatrix}\n=\n\\left[ \\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_p \\right]\n where both \\prime and \\top represent matrix transpose.\n\n\n\nR Code: Data Organization\nX = as.matrix(USArrests)\ndim(X)\n\n\n[1] 50  4\n\n\n\n\nR Code: Matrix Transpose\nt(X[1:5,])\n\n\n         Alabama Alaska Arizona Arkansas California\nMurder      13.2   10.0     8.1      8.8        9.0\nAssault    236.0  263.0   294.0    190.0      276.0\nUrbanPop    58.0   48.0    80.0     50.0       91.0\nRape        21.2   44.5    31.0     19.5       40.6"
  },
  {
    "objectID": "ch1/01-intro.html#descriptive-statistics",
    "href": "ch1/01-intro.html#descriptive-statistics",
    "title": "1  Introduction to Multivariate Data",
    "section": "1.5 Descriptive Statistics",
    "text": "1.5 Descriptive Statistics\n\n1.5.1 Sample Mean\n\nThe sample mean of the kth variable (k = 1,...,p) is \n\\bar{x}_k = \\frac{1}{n} \\sum_{i = 1}^n x_{ik}\n\n\n\n\n1.5.2 Sample Variance and Sample Standard Deviation\n\nThe sample variance of the kth variable is \ns^2_k = \\frac{1}{n-1} \\sum_{i=1}^n (x_{ik} - \\bar{x}_k)^2  \n\nThe sample standard deviation is given by \ns_k = \\sqrt{s^2_k}\n\nWe often use s_{kk} to denote the sample variance for the k-th variable. Thus, \ns^2_k = s_{kk}\n\n\n\n\n1.5.3 Sample Covariance and Sample Correlation\n\nThe sample covariance between variable k and variable j is computed as \ns_{jk} = \\frac{1}{n-1} \\sum_{i=1}^n (x_{ij} - \\bar{x}_j) (x_{ik} - \\bar{x}_k)\n\nIf variables k and j are independent, the population covariance will be exactly zero, but the sample covariance will vary about zero.\nThe sample correlation between variables k and j is defined as \nr_{jk} = \\frac{s_{jk}}{\\sqrt{s_{jj}} \\sqrt{s_{kk}}}\n\nr_{jk} is between -1 and 1.\nr_{jk} = r_{kj}\nThe sample correlation is equal to the sample covariance if measurements are standardized.\nThe sample correlation (r_{ij}) will vary about the value of the population correlation (\\rho_{ij})\n\n\n\n\n\n\n\nR Code: Descriptive Statistics\n\n\n\n\n\n\n# load built-in US crime rates data\ndata(\"USArrests\") \ndat = USArrests \nhead(dat)\n\n           Murder Assault UrbanPop Rape\nAlabama      13.2     236       58 21.2\nAlaska       10.0     263       48 44.5\nArizona       8.1     294       80 31.0\nArkansas      8.8     190       50 19.5\nCalifornia    9.0     276       91 40.6\nColorado      7.9     204       78 38.7\n\n\n\n\nSample mean\nmu = colMeans(dat)\nmu \n\n\n  Murder  Assault UrbanPop     Rape \n   7.788  170.760   65.540   21.232 \n\n\nSample mean\nlibrary(dplyr)\ndat %&gt;% \n  summarise(across(where(is.numeric), mean))\n\n\n  Murder Assault UrbanPop   Rape\n1  7.788  170.76    65.54 21.232\n\n\n\n\nSample variance\ns2 = apply(dat, 2, var)\ns2 \n\n\n    Murder    Assault   UrbanPop       Rape \n  18.97047 6945.16571  209.51878   87.72916 \n\n\nSample variance\n# using dplyr\ndat %&gt;% \n  summarise(across(where(is.numeric), var))\n\n\n    Murder  Assault UrbanPop     Rape\n1 18.97047 6945.166 209.5188 87.72916\n\n\n\n\nSample standard deviation\n# compute standard deviation from data \ns = apply(dat, 2, sd)  \n# or from sample variance \ns = sqrt(s2)\ns \n\n\n   Murder   Assault  UrbanPop      Rape \n 4.355510 83.337661 14.474763  9.366385 \n\n\n\n\nSample covariance\nn = nrow(dat)\ns_jk = 1/(n-1) * sum((dat[,1] - mu[1]) * (dat[,2] - mu[2]))\ns_jk \n\n\n[1] 291.0624\n\n\n\n\nSample correlation\nr_jk = s_jk / sqrt(s2[1] * s2[2])\n\n\n\n\n\n\n\n1.5.4 Covariance and Correlation Measures\n\nCovariance and correlation measure linear association.\nOther non-linear (or curved) relationships may exist among variables even if r_{jk} = 0.\nA population correlation of zero means no linear association,\nbut it does not necessarily imply independence.\n\n\n\n\n\n\nCorrelation Measures Linear Association\n\n\n\n\n\n\n\n\n\nNonlinear Dependence with (Near) Zero Correlation"
  },
  {
    "objectID": "ch1/01-intro.html#matrix-organization-of-descriptive-statistics",
    "href": "ch1/01-intro.html#matrix-organization-of-descriptive-statistics",
    "title": "1  Introduction to Multivariate Data",
    "section": "1.6 Matrix Organization of Descriptive Statistics",
    "text": "1.6 Matrix Organization of Descriptive Statistics\n\n1.6.1 Sample Mean\n\nSample mean: \\bar{\\mathbf{x}} is the p \\times 1 vector of sample means:\n\n\n\\bar{\\mathbf{x}} =\n\\begin{bmatrix}\n\\bar{x}_1 \\\\\n\\bar{x}_2 \\\\\n\\vdots \\\\\n\\bar{x}_p\n\\end{bmatrix}\n\n\n\\bar{\\mathbf{x}} is an estimate of the vector of population means:\n\n\n\\boldsymbol{\\mu} =\n\\begin{bmatrix}\n\\mu_1 \\\\\n\\mu_2 \\\\\n\\vdots \\\\\n\\mu_p\n\\end{bmatrix}\n\n\n\nR Code: Sample Mean\nX = as.matrix(USArrests)\nxbar = colMeans(X)\nxbar \n\n\n  Murder  Assault UrbanPop     Rape \n   7.788  170.760   65.540   21.232 \n\n\n\nCentered Data: X_c = X - \\mathbf{1}_n \\bar{\\mathbf{x}}^\\top\n\n\n\nR Code: Centered Data\nn = nrow(X) \nones_n = matrix(1, n, ncol=1)\nXc = X - ones_n %*% t(xbar)\n\n\n\n\n1.6.2 Sample Covariance\n\nS is the p \\times p symmetric matrix of sample variances (on the diagonal) and sample covariances (the off-diagonal elements):\n\n\nS =\n\\begin{bmatrix}\ns_{11} & s_{12} & s_{13} & \\cdots & s_{1p} \\\\\ns_{21} & s_{22} & s_{23} & \\cdots & s_{2p} \\\\\n\\vdots & \\vdots & \\vdots &        & \\vdots \\\\\ns_{p1} & s_{p2} & s_{p3} & \\cdots & s_{pp}\n\\end{bmatrix}\n= \\frac{1}{n-1} \\sum_{i=1}^n (\\mathbf{x}_i - \\bar{\\mathbf{x}})(\\mathbf{x}_i - \\bar{\\mathbf{x}})^{\\top} = \\frac{1}{n-1} X_c^\\top X_c\n\n\nS is an estimate of the population covariance matrix:\n\n\n\\Sigma =\n\\begin{bmatrix}\n\\sigma_{11} & \\sigma_{12} & \\sigma_{13} & \\cdots & \\sigma_{1p} \\\\\n\\sigma_{21} & \\sigma_{22} & \\sigma_{23} & \\cdots & \\sigma_{2p} \\\\\n\\vdots      & \\vdots      & \\vdots      &        & \\vdots      \\\\\n\\sigma_{p1} & \\sigma_{p2} & \\sigma_{p3} & \\cdots & \\sigma_{pp}\n\\end{bmatrix}\n\n\n\nR Code: Sample Covariance\nS = cov(X) \n# or using matrix algebra\nS_mat = t(Xc)%*%Xc/(n-1)\nsignif(cbind(S, S_mat),4)\n\n\n          Murder Assault UrbanPop   Rape  Murder Assault UrbanPop   Rape\nMurder    18.970   291.1    4.386  22.99  18.970   291.1    4.386  22.99\nAssault  291.100  6945.0  312.300 519.30 291.100  6945.0  312.300 519.30\nUrbanPop   4.386   312.3  209.500  55.77   4.386   312.3  209.500  55.77\nRape      22.990   519.3   55.770  87.73  22.990   519.3   55.770  87.73\n\n\n\n\n1.6.3 Sample Correlation\n\nThe p \\times p matrix of sample correlations is also symmetric:\n\n\nR =\n\\begin{bmatrix}\n1       & r_{12} & r_{13} & \\cdots & r_{1p} \\\\\nr_{21}  & 1      & r_{23} & \\cdots & r_{2p} \\\\\n\\vdots  & \\vdots & \\vdots &        & \\vdots \\\\\nr_{p1}  & r_{p2} & r_{p3} & \\cdots & 1\n\\end{bmatrix}\n= D^{-1/2} \\, S \\, D^{-1/2}\n\n\nD^{-1/2} is a diagonal matrix with (j,j) entry 1/\\sqrt{s_{jj}} = 1/s_j, i.e.,\n\n\nD^{-1/2} =\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{s_{11}}} & 0                       & 0                       & \\cdots & 0 \\\\\n0                       & \\frac{1}{\\sqrt{s_{22}}} & 0                       & \\cdots & 0 \\\\\n\\vdots                  & \\vdots                  & \\vdots                  &        & \\vdots \\\\\n0                       & 0                       & 0                       & \\cdots & \\frac{1}{\\sqrt{s_{pp}}}\n\\end{bmatrix}\n\n\nR is an estimate of the population correlation matrix \nP =\n\\begin{bmatrix}\n1          & \\rho_{12} & \\rho_{13} & \\cdots & \\rho_{1p} \\\\\n\\rho_{21}  & 1         & \\rho_{23} & \\cdots & \\rho_{2p} \\\\\n\\vdots     & \\vdots    & \\vdots    &        & \\vdots   \\\\\n\\rho_{p1}  & \\rho_{p2} & \\rho_{p3} & \\cdots & 1\n\\end{bmatrix}\n\n\n\n\nR Code: Sample Correlation\nR = cor(X)\nR\n\n\n             Murder   Assault   UrbanPop      Rape\nMurder   1.00000000 0.8018733 0.06957262 0.5635788\nAssault  0.80187331 1.0000000 0.25887170 0.6652412\nUrbanPop 0.06957262 0.2588717 1.00000000 0.4113412\nRape     0.56357883 0.6652412 0.41134124 1.0000000\n\n\n\n\nR Code: Pairwise Scatterplots and Correlations\nGGally::ggpairs(dat)"
  },
  {
    "objectID": "ch1/01-intro.html#standardization",
    "href": "ch1/01-intro.html#standardization",
    "title": "1  Introduction to Multivariate Data",
    "section": "1.7 Standardization",
    "text": "1.7 Standardization\n\n1.7.1 Standardized Data (or z-Scores)\n\nSuppose x_{ij} is the measurement on the j-th outcome variable for the i-th subject in the data set.\nThe standardized value is\n\nz_{ij} = \\frac{x_{ij} - \\bar{x}_{j}}{s_j}\n\nThe entire set of p standardized responses for the i-th subject can be computed as\n\n\\mathbf{z}_i =\n\\begin{bmatrix}\nz_{i1} \\\\\nz_{i2} \\\\\n\\vdots \\\\\nz_{ip}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\dfrac{x_{i1} - \\bar{x}_{1}}{s_1} \\\\\n\\dfrac{x_{i2} - \\bar{x}_{2}}{s_2} \\\\\n\\vdots \\\\\n\\dfrac{x_{ip} - \\bar{x}_{p}}{s_p}\n\\end{bmatrix}\n= D^{-1/2} (\\mathbf{x}_i - \\bar{\\mathbf{x}})\n\n\n\n\nR Code: Standardized Data\ns_j = sqrt(diag(S))\n\nDsqrt = diag(1/s_j)\n\n# using matrix algebra\nZ =  (X -  matrix(1, nrow(X), 1) %*% matrix(xbar, 1)) %*% Dsqrt\n\n# using scale function \nX_scaled = as.matrix(scale(X, center=TRUE, scale=TRUE))\n\n\n# using mutate function\ndat_scaled &lt;- dat %&gt;%\n  mutate(across(everything(), ~ (.x - mean(.x)) / sd(.x)))\n\n\n\n\nR Code: Plotting the Data\np1 = ggplot(dat, aes(Murder, Assault)) + \n  geom_point(alpha=0.8) + \n  labs(title=\"Unscaled\")\n\np2 = ggplot(dat_scaled, aes(Murder, Assault)) + \n  geom_point(alpha=.8) + \n  labs(title=\"Standardized (z-scores)\")\n\npatchwork::wrap_plots(p1, p2, ncol=1)\n\n\n\n\n\n\n\n1.7.2 Standardized Population Mean\n\nThe vector of true means for a set of standardized responses is\na vector of zeros:\n\n\nE(\\mathbf{z}_i) =\nE\\begin{bmatrix}\nz_{i1} \\\\\nz_{i2} \\\\\n\\vdots \\\\\nz_{ip}\n\\end{bmatrix}\n=\nE\\begin{bmatrix}\n\\dfrac{x_{i1} - \\bar{x}_{1}}{s_1} \\\\\n\\dfrac{x_{i2} - \\bar{x}_{2}}{s_2} \\\\\n\\vdots \\\\\n\\dfrac{x_{ip} - \\bar{x}_{p}}{s_p}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{bmatrix}\n\n\nThe vector of estimated means for a set of standardized responses\nis also a vector of zeros.\n\n\n\n1.7.3 Standardized Population Covariance\n\nThe true covariance matrix for a set of standardized responses is the population correlation matrix:\n\n\n\\text{Var}(\\mathbf{z}_i) = P =\n\\begin{bmatrix}\n1 & \\rho_{12} & \\cdots & \\rho_{1p} \\\\\n\\rho_{21} & 1 & \\cdots & \\rho_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\rho_{p1} & \\rho_{p2} & \\cdots & 1\n\\end{bmatrix}\n\n\nAn estimate of the true covariance matrix for a set of standardized responses is the sample correlation matrix:\n\n\n\\widehat{\\text{Var}(\\mathbf{z}_i)} = R =\n\\begin{bmatrix}\n1 & r_{12} & \\cdots & r_{1p} \\\\\nr_{21} & 1 & \\cdots & r_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nr_{p1} & r_{p2} & \\cdots & 1\n\\end{bmatrix}\n= D^{-1/2} S D^{-1/2}"
  },
  {
    "objectID": "ch1/01-intro.html#exercises",
    "href": "ch1/01-intro.html#exercises",
    "title": "1  Introduction to Multivariate Data",
    "section": "1.8 Exercises",
    "text": "1.8 Exercises\n\n1.8.1 Exercise 1: Data Types\nCommon types of objects for data analysis are numeric, character, logical, factors, and dates. Character data do not have numerical values, such as names of people or words in a book, Logical data takes values of true or false and can be used to make decisions.\n\nFor each of the following commands, either explain why they should be errors, or explain the non-erroneous result.\n\n    x &lt;- c(\"1\",\"2\",\"3\")\n    max(x)\n    sort(x)\n    sum(x)\n\nFor the next two commands, either explain their results, or why they should produce errors.\n\n    y &lt;- c(\"1\",3,4)\n    y[2] + y[3]\n\nFor the next two commands, either explain their results, or why they should produce errors.\n\n    z &lt;- data.frame(z1=\"1\", z2=3, z3=5)\n    z[1,2] + z[1,3]\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\nx is bound to a vector of characters instead of numerical values. The functions max, sort, sum should take numerical values as input, so the values in x are first converted from characters to numerical values implicitly and then the functions apply to these incorrect numerical values\ny is a vector of characters, which cannot be used for addition\nz is a data frame, but the variable z1 is a character while z2 and z3 are numerical. Adding z2 and z3 will produce the correct results.\n\n\n\n\n\n\n1.8.2 Exercise 2: Working with Matrix and Data Frames\n\nA matrix is a 2D array, with rows and columns, much like you would have seen in linear algebra. Primarily we think of these as numeric objects. Arrays can have more than two dimensions. In multivariate analysis we are typically thinking of data in the form of a matrix, with samples/cases in the rows and variables represented as columns. Data frames are 2D arrays that could have multiple types of data in different columns. Lists are collections of possibly different length and different types of objects.\n\n\nCreate a matrix and print it out.\n\n\n\nView Solution\ndat = matrix(c(140, 120, 160, 145, 125, \n               65, 60, 63, 66, 61), \n             ncol = 2, byrow = FALSE) \n\n\n\n\nView Solution\ndat # or print(dat) \n\n\n     [,1] [,2]\n[1,]  140   65\n[2,]  120   60\n[3,]  160   63\n[4,]  145   66\n[5,]  125   61\n\n\n\nRename the column names of a matrix.\n\n\n\nView Solution\ncolnames(dat) = c(\"weight.lbs\", \"Height.in\")\ndat \n\n\n     weight.lbs Height.in\n[1,]        140        65\n[2,]        120        60\n[3,]        160        63\n[4,]        145        66\n[5,]        125        61\n\n\n\nData frames can store both columns of numeric data and columns of character data, columns of integers, and factors.\n\n\nCreate a data frame with Male and Female observations, and print out the data frame.\n\n\n\nView Solution\ndf = data.frame(\n  Weight.lbs = c(140, 120, 160, 145, 125,\n       180, 165), \n  Height.in = c(65, 60, 63, 66, 61, 70, 68),\n  Gender = c(rep(\"Female\", 5), rep(\"Male\", 2))\n  ) \n\ndf\n\n\n  Weight.lbs Height.in Gender\n1        140        65 Female\n2        120        60 Female\n3        160        63 Female\n4        145        66 Female\n5        125        61 Female\n6        180        70   Male\n7        165        68   Male\n\n\n\nGet the rows and columns of a data frame.\n\n\n\nView Solution\ndim(df)\n\n\n[1] 7 3\n\n\nView Solution\nnrow(df)\n\n\n[1] 7\n\n\nView Solution\nncol(df)\n\n\n[1] 3\n\n\n\nGet summary statistics of a data frame.\n\n\n\nView Solution\nsummary(df)\n\n\n   Weight.lbs      Height.in        Gender         \n Min.   :120.0   Min.   :60.00   Length:7          \n 1st Qu.:132.5   1st Qu.:62.00   Class :character  \n Median :145.0   Median :65.00   Mode  :character  \n Mean   :147.9   Mean   :64.71                     \n 3rd Qu.:162.5   3rd Qu.:67.00                     \n Max.   :180.0   Max.   :70.00                     \n\n\n\nOperations with Matrix and Data Frame\n\n\nExtract the first row and second column of a matrix.\n\n\n\nView Solution\ndat[1,]\n\n\nweight.lbs  Height.in \n       140         65 \n\n\nView Solution\ndat[,2]\n\n\n[1] 65 60 63 66 61\n\n\n\nSubset the first two rows of a matrix\n\n\n\nView Solution\ndat[1:2,]\n\n\n     weight.lbs Height.in\n[1,]        140        65\n[2,]        120        60\n\n\n\nSubset rows 1,3,5 of a matrix.\n\n\n\nView Solution\ndat[c(1,3,5), ]\n\n\n     weight.lbs Height.in\n[1,]        140        65\n[2,]        160        63\n[3,]        125        61\n\n\n\nSelect all the Male observations from a data frame.\n\n\n\nView Solution\nsubset(df, Gender==\"Male\")\n\n\n  Weight.lbs Height.in Gender\n6        180        70   Male\n7        165        68   Male\n\n\n\nTranspose of a matrix\n\n\n\nView Solution\nA = matrix(c(3,1,2,4), ncol=2)\nA\n\n\n     [,1] [,2]\n[1,]    3    2\n[2,]    1    4\n\n\n\nMatrix multiplication\n\n\n\nView Solution\nB = matrix(c(1,2,3,4), ncol=2)\nA %*% B \n\n\n     [,1] [,2]\n[1,]    7   17\n[2,]    9   19\n\n\nView Solution\nB %*% A \n\n\n     [,1] [,2]\n[1,]    6   14\n[2,]   10   20\n\n\n\nMatrix inversion\n\n\n\nView Solution\nsolve(A)\n\n\n     [,1] [,2]\n[1,]  0.4 -0.2\n[2,] -0.1  0.3\n\n\n\n\n1.8.3 Exercise 3: Linear Algebra\nConsider the linear system A X = b, where A is an n\\times n positive definite matrix and b is a n-dimensional vector, the unique solution is X = A^{-1}b. Please answer the following questions:\n\nWrite an R function called my_solver() such that given inputs A and b, the function my_solver() returns the solution of the linear system, i.e., X &lt;- my_solver(A, b).\nRun the following code to get A and b.\n\nset.seed(123)\nA = matrix(c(5,1,1,6), ncol=2)\nn = nrow(A)\nb = rnorm(n,1)\nThen use your function my_solver() to produce the answer and verify your solution. (hint: AX should be equal to b)\n\n\nView Solution\nmy_solver &lt;- function(A, b){\n  x = solve(A, b)\n  return(x)\n}\n\nA = matrix(c(5,1,1,6), ncol=2)\nn = nrow(A)\nb = rnorm(n,1)\nx1 = my_solver(A, b)\nsum((A%*%x1-b)^2)\n\n\n[1] 1.232595e-32\n\n\n\n\n1.8.4 Exercise 4: Working with ggplot2 Package\nEPA monitors Air Quality data across the entire U.S. The file AQSdata.csv contains daily PM 2.5 concentrations and other information. Please answer the following questions using the ggplot() function for plotting. In addition make sure that all the x-axis and y-axis labels have 14 font size.\n\nRead the data file AQSdata.csv into R.\nGenerate density plots of PM2.5 concentrations grouped by County in one single panel, where each density should have its own color. What do you find from the figure?\nPlot histograms of PM2.5 concentrations across different counties with one panel for one histogram.\nGenerate boxplots of PM2.5 concentrations by County. What would you say about the distributions?\nReorder the boxplots above by the median value of PM2.5 concentrations.\nConverting the Site ID to a factor and plot the histogram grouped by Site ID.\nGenerate the time series plot for the monitoring Site ID 450190048.\nPlot time series of PM2.5 concentrations for all monitoring sites in one panel, where each site has its own color\nPlot time series of PM2.5 concentrations across all monitoring sites in multiple panels, where one panel only has one site, and each row only has two panels.\nIn the time series plot, there seems to be not enough space to hold the x-axis labels. One way to avoid this is to rotate the axis labels. Please rotate all the time labels 45 degree.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\nlibrary(readr)\nlibrary(lubridate)\n\ndf = read_csv(\"AQSdata.csv\")\nhead(df)\n\n\n# A tibble: 6 × 20\n  Date       Source `Site ID`   POC Daily Mean PM2.5 Con…¹ UNITS DAILY_AQI_VALUE\n  &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;                  &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n1 11/09/2021 AQS    450190020     1                   15.5 ug/m…              58\n2 11/10/2021 AQS    450190020     1                   13.6 ug/m…              54\n3 11/11/2021 AQS    450190020     1                    8.1 ug/m…              34\n4 11/12/2021 AQS    450190020     1                    7.1 ug/m…              30\n5 11/13/2021 AQS    450190020     1                   10.7 ug/m…              45\n6 11/14/2021 AQS    450190020     1                    7.5 ug/m…              31\n# ℹ abbreviated name: ¹​`Daily Mean PM2.5 Concentration`\n# ℹ 13 more variables: `Site Name` &lt;chr&gt;, DAILY_OBS_COUNT &lt;dbl&gt;,\n#   PERCENT_COMPLETE &lt;dbl&gt;, AQS_PARAMETER_CODE &lt;dbl&gt;, AQS_PARAMETER_DESC &lt;chr&gt;,\n#   CBSA_CODE &lt;dbl&gt;, CBSA_NAME &lt;chr&gt;, STATE_CODE &lt;dbl&gt;, STATE &lt;chr&gt;,\n#   COUNTY_CODE &lt;chr&gt;, COUNTY &lt;chr&gt;, SITE_LATITUDE &lt;dbl&gt;, SITE_LONGITUDE &lt;dbl&gt;\n\n\nCode\ntheme_mat = theme(axis.text = element_text(size = 14),\n                  axis.title = element_text(size = 14, face = \"bold\"))\ndf = rename(df, PM2.5 = `Daily Mean PM2.5 Concentration`)\nggplot(df) +\n  geom_freqpoly(aes(\n    x = PM2.5,\n    y = after_stat(density),\n    color = COUNTY\n  ), binwidth = 2) +\n  xlab(\"Daily PM2.5 concentration (ug/m3 LC)\") +\n  theme_mat\n\n\n\n\n\nCode\nggplot(df) +\n  geom_histogram(aes(x = PM2.5), binwidth = .8) +\n  facet_wrap(~ COUNTY, ncol = 3) +\n  xlab(\"Daily PM2.5 concentration (ug/m3 LC)\") +\n  theme_mat\n\n\n\n\n\nCode\nggplot(df) +\n  geom_boxplot(aes(x = COUNTY, y = PM2.5)) +\n  xlab(\"County\") +\n  ylab(\"Daily PM2.5 concentration (ug/m3 LC)\") +\n  theme_mat\n\n\n\n\n\nCode\nggplot(df) +\n  geom_boxplot(aes(x = reorder(COUNTY, PM2.5, FUN = median), \n                   y = PM2.5)) +\n  xlab(\"County\") +\n  ylab(\"Daily PM2.5 concentration (ug/m3 LC)\") +\n  theme_mat\n\n\n\n\n\nCode\ndf1 = df\ndf1$`Site ID` =  as.factor(df$`Site ID`)\nggplot(df1) +\n  geom_freqpoly(aes(x = PM2.5, color = `Site ID`), binwidth = 2) +\n  xlab(\"Daily PM2.5 concentration (ug/m3 LC)\") +\n  theme_mat\n\n\n\n\n\nCode\ndf1 %&gt;%\n  filter(`Site ID` == 450190048) %&gt;%\n  ggplot() +\n  geom_line(aes(x = mdy(Date), y = PM2.5)) +\n  labs(x = \"Time\", y = \"Daily PM2.5 concentration (ug/m3 LC)\") + \n  theme_mat\n\n\n\n\n\nCode\ndf1 %&gt;%\n  ggplot() +\n  geom_line(aes(x = mdy(Date), y = PM2.5, color = `Site ID`)) +\n  labs(x = \"Time\", y = \"Daily PM2.5 concentration (ug/m3 LC)\") + \n  theme_mat\n\n\n\n\n\nCode\ng &lt;- df1 %&gt;%\n  ggplot() +\n  geom_line(aes(x = mdy(Date), y = PM2.5)) +\n  facet_wrap(~ `Site ID`, ncol = 2) +\n  labs(x = \"Time\", y = \"Daily PM2.5 concentration (ug/m3 LC)\") + \n  theme_mat\nprint(g)\n\n\n\n\n\nCode\ng + theme(axis.text.x = element_text(angle = 45))\n\n\n\n\n\n\n\n\n\n\n1.8.5 Exercise 5: Working with dplyr Package\nContinuing working with the above PM 2.5 data.\n\nFilter all the observations in the county Greenville. How many observations are there?\nFilter all the observations in Greenville in August 2021\nFilter all the observations in Greenville in August 2021 and select the variables PM2.5 concentrations, Date, latitude and longitude of sites\nGenerate scatterplots of PM2.5 against latitude and longitude in two different panels\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\nlibrary(dplyr)\n\ndf %&gt;%\n  filter(COUNTY == \"Greenville\")\n\n\n# A tibble: 937 × 20\n   Date       Source `Site ID`   POC PM2.5 UNITS    DAILY_AQI_VALUE `Site Name` \n   &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       \n 1 01/01/2021 AQS    450450015     1   6.3 ug/m3 LC              26 Greenville …\n 2 01/02/2021 AQS    450450015     1   6.6 ug/m3 LC              28 Greenville …\n 3 01/03/2021 AQS    450450015     1   5.4 ug/m3 LC              23 Greenville …\n 4 01/05/2021 AQS    450450015     1   7.4 ug/m3 LC              31 Greenville …\n 5 01/06/2021 AQS    450450015     1   7.7 ug/m3 LC              32 Greenville …\n 6 01/07/2021 AQS    450450015     1   9.5 ug/m3 LC              40 Greenville …\n 7 01/08/2021 AQS    450450015     1   7.5 ug/m3 LC              31 Greenville …\n 8 01/09/2021 AQS    450450015     1   5.3 ug/m3 LC              22 Greenville …\n 9 01/10/2021 AQS    450450015     1  12.1 ug/m3 LC              51 Greenville …\n10 01/11/2021 AQS    450450015     1  16.7 ug/m3 LC              61 Greenville …\n# ℹ 927 more rows\n# ℹ 12 more variables: DAILY_OBS_COUNT &lt;dbl&gt;, PERCENT_COMPLETE &lt;dbl&gt;,\n#   AQS_PARAMETER_CODE &lt;dbl&gt;, AQS_PARAMETER_DESC &lt;chr&gt;, CBSA_CODE &lt;dbl&gt;,\n#   CBSA_NAME &lt;chr&gt;, STATE_CODE &lt;dbl&gt;, STATE &lt;chr&gt;, COUNTY_CODE &lt;chr&gt;,\n#   COUNTY &lt;chr&gt;, SITE_LATITUDE &lt;dbl&gt;, SITE_LONGITUDE &lt;dbl&gt;\n\n\nCode\ndf %&gt;%\n  mutate(Date = mdy(Date), YM = format_ISO8601(Date, precision = \"ym\")) %&gt;%\n  filter(COUNTY == \"Greenville\", YM == \"2021-08\")\n\n\n# A tibble: 82 × 21\n   Date       Source `Site ID`   POC PM2.5 UNITS    DAILY_AQI_VALUE `Site Name` \n   &lt;date&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       \n 1 2021-08-01 AQS    450450015     1  13.8 ug/m3 LC              55 Greenville …\n 2 2021-08-02 AQS    450450015     1  19   ug/m3 LC              66 Greenville …\n 3 2021-08-03 AQS    450450015     1  16.9 ug/m3 LC              61 Greenville …\n 4 2021-08-04 AQS    450450015     1  15.6 ug/m3 LC              58 Greenville …\n 5 2021-08-05 AQS    450450015     1  11   ug/m3 LC              46 Greenville …\n 6 2021-08-06 AQS    450450015     1  10.3 ug/m3 LC              43 Greenville …\n 7 2021-08-07 AQS    450450015     1   9.7 ug/m3 LC              40 Greenville …\n 8 2021-08-08 AQS    450450015     1  10   ug/m3 LC              42 Greenville …\n 9 2021-08-09 AQS    450450015     1  12   ug/m3 LC              50 Greenville …\n10 2021-08-10 AQS    450450015     1  12.5 ug/m3 LC              52 Greenville …\n# ℹ 72 more rows\n# ℹ 13 more variables: DAILY_OBS_COUNT &lt;dbl&gt;, PERCENT_COMPLETE &lt;dbl&gt;,\n#   AQS_PARAMETER_CODE &lt;dbl&gt;, AQS_PARAMETER_DESC &lt;chr&gt;, CBSA_CODE &lt;dbl&gt;,\n#   CBSA_NAME &lt;chr&gt;, STATE_CODE &lt;dbl&gt;, STATE &lt;chr&gt;, COUNTY_CODE &lt;chr&gt;,\n#   COUNTY &lt;chr&gt;, SITE_LATITUDE &lt;dbl&gt;, SITE_LONGITUDE &lt;dbl&gt;, YM &lt;chr&gt;\n\n\nCode\ndf %&gt;%\n  mutate(Date = mdy(Date), YM = format_ISO8601(Date, precision = \"ym\")) %&gt;%\n  filter(COUNTY == \"Greenville\", YM == \"2021-08\") %&gt;%\n  dplyr::select(PM2.5, Date, SITE_LATITUDE, SITE_LONGITUDE)\n\n\n# A tibble: 82 × 4\n   PM2.5 Date       SITE_LATITUDE SITE_LONGITUDE\n   &lt;dbl&gt; &lt;date&gt;             &lt;dbl&gt;          &lt;dbl&gt;\n 1  13.8 2021-08-01          34.8          -82.4\n 2  19   2021-08-02          34.8          -82.4\n 3  16.9 2021-08-03          34.8          -82.4\n 4  15.6 2021-08-04          34.8          -82.4\n 5  11   2021-08-05          34.8          -82.4\n 6  10.3 2021-08-06          34.8          -82.4\n 7   9.7 2021-08-07          34.8          -82.4\n 8  10   2021-08-08          34.8          -82.4\n 9  12   2021-08-09          34.8          -82.4\n10  12.5 2021-08-10          34.8          -82.4\n# ℹ 72 more rows\n\n\nCode\ndf %&gt;%\n  dplyr::select(PM2.5, SITE_LATITUDE, SITE_LONGITUDE) %&gt;%\n  pivot_longer(\n    cols = c(\"SITE_LATITUDE\", \"SITE_LONGITUDE\"),\n    names_to = \"variable\",\n    values_to = \"value\"\n  ) %&gt;%\n  ggplot(aes(x = value, y = PM2.5)) +\n  geom_point() +\n  facet_wrap( ~ variable, scale = \"free\") +\n  xlab(\"\") +\n  ylab(\"Daily PM2.5 concentration (ug/m3 LC)\") +\n  theme_mat"
  },
  {
    "objectID": "ch2/02-graphs.html#introduction",
    "href": "ch2/02-graphs.html#introduction",
    "title": "2  Graphs and Data Visualization",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\n\nGraphs reveal information about the center, shape, and spread of distributions.\nThey can also show typical and extreme outcomes, associations, and differences between groups.\nDifferent data types require different graph types."
  },
  {
    "objectID": "ch2/02-graphs.html#categorical-data",
    "href": "ch2/02-graphs.html#categorical-data",
    "title": "2  Graphs and Data Visualization",
    "section": "2.2 Categorical Data",
    "text": "2.2 Categorical Data\nCategorical data types:\n\nNominal: no natural order (e.g., eye color, blood type).\nOrdinal: ordered, but spacing not meaningful (e.g., therapy response).\nInterval: ordered and evenly spaced (e.g., fruit count).\n\n\n2.2.1 Bar Chart Example\nBar chart is used for displaying categorical variables.\n\n\nR Code: Bart Chart\ncolors &lt;- c('Red', 'Pink', 'White')\ncounts &lt;- c(80, 60, 40)\nbar_df &lt;- data.frame(Color = colors, Count = counts)\nggplot(bar_df, aes(x = Color, y = Count)) +\n  geom_bar(stat = 'identity', fill = 'steelblue') +\n  theme_minimal() +\n  labs(title = 'Poinsettia Colors')\n\n\n\n\n\n\nQuestion: What does the bar plot tell you about the color distribution for this poinsettia variety?"
  },
  {
    "objectID": "ch2/02-graphs.html#quantitative-data",
    "href": "ch2/02-graphs.html#quantitative-data",
    "title": "2  Graphs and Data Visualization",
    "section": "2.3 Quantitative Data",
    "text": "2.3 Quantitative Data\n\nMeasured on a numeric scale: e.g., weights, heights, cholesterol levels.\nUse histograms, boxplots, dotplots, density plots.\n\n\n2.3.1 Histogram Example (Tip Amounts)\nHistograms classify values into bins of equal width\n\nHeights of bars represent relative frequencies\n\n\n\nR Code: Histogram\ndata(\"tips\", package=\"reshape2\")\nggplot(tips, aes(x = tip)) +\n  geom_histogram(binwidth = 1, fill = 'orange', color = 'black') +\n  theme_minimal() +\n  labs(title = 'Histogram of Tip Amounts', x = 'Tip ($)', y = 'Count')\n\n\n\n\n\n\nQuestion: What does this histogram tell you about tips at this restaurant?\n\n\n\n\n\n\n\nTip\n\n\n\n\nCenter\nShape\nSpread\n\n\n\n\n\n2.3.2 Try Different Bin Widths\n\n\nR Code: Histograms with Different Bin Widths\nbw_list &lt;- c(0.25, 0.5, 1.0)\nplots &lt;- lapply(bw_list, function(bw) {\n  ggplot(tips, aes(x = tip)) +\n    geom_histogram(binwidth = bw, fill = 'lightblue', color = 'black') +\n    ggtitle(paste('Bin Width =', bw)) +\n    theme_minimal()\n})\nlibrary(gridExtra)\ndo.call(grid.arrange, c(plots, ncol = 1))\n\n\n\n\n\n\n\n2.3.3 Boxplot\nBoxplot is graphical display of the five number data summary (minimum, Q1, median, Q3, maximum). It is good for comparing samples from different populations.\n\n\nR Code: Boxplot\nggplot(tips, aes(y = tip)) +\n  geom_boxplot(fill = 'lightgreen') +\n  theme_minimal() +\n  labs(title = 'Boxplot of Tip Amounts', y = 'Tip ($)')"
  },
  {
    "objectID": "ch2/02-graphs.html#one-quantitative-one-categorical",
    "href": "ch2/02-graphs.html#one-quantitative-one-categorical",
    "title": "2  Graphs and Data Visualization",
    "section": "2.4 One Quantitative & One Categorical",
    "text": "2.4 One Quantitative & One Categorical\nSide by side box plots and dot plots can be used to compare distributions of a quantitative response variable for different levels of a categorical variable.\n\n\nR Code: Boxplots with Grouping\nggplot(tips, aes(x = day, y = tip)) +\n  geom_boxplot(fill = 'skyblue') +\n  theme_minimal() +\n  labs(title = 'Tip Amount by Day of Week')"
  },
  {
    "objectID": "ch2/02-graphs.html#two-quantitative-variables",
    "href": "ch2/02-graphs.html#two-quantitative-variables",
    "title": "2  Graphs and Data Visualization",
    "section": "2.5 Two Quantitative Variables",
    "text": "2.5 Two Quantitative Variables\nScatterplots convey information about associations between quantitative variables and also about unusual observations.\n\n\nR Code: Scatterplot\ntips$bill &lt;- tips$total_bill\nggplot(tips, aes(x = bill, y = tip)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se = FALSE, col = 'red') +\n  theme_minimal() +\n  labs(title = 'Tip vs Bill Amount', x = 'Bill ($)', y = 'Tip ($)')\n\n\n\n\n\n\nHow is the tip related to the bill?\nWhich variable is the response? Which is the explanatory variable?\nWhat type of relationship would you expect?"
  },
  {
    "objectID": "ch2/02-graphs.html#practice-problems",
    "href": "ch2/02-graphs.html#practice-problems",
    "title": "2  Graphs and Data Visualization",
    "section": "2.6 Practice Problems",
    "text": "2.6 Practice Problems\n\n2.6.1 Exercise 1: Bar Chart from Survey\nCreate a bar chart for the following survey results:\n\n\n\nPet Type\nCount\n\n\n\n\nDog\n15\n\n\nCat\n10\n\n\nFish\n3\n\n\nBird\n2\n\n\n\nUse ggplot2 package to visualize the data and describe what the chart tells you about pet preferences.\n\n\nView Solution\nlibrary(ggplot2)\npet_df &lt;- data.frame(\n  Pet = c(\"Dog\", \"Cat\", \"Fish\", \"Bird\"),\n  Count = c(15, 10, 3, 2)\n)\nggplot(pet_df, aes(x = Pet, y = Count, fill = Pet)) +\n  geom_bar(stat = \"identity\") +\n  theme_minimal() +\n  labs(title = \"Pet Types\", x = \"Pet Type\", y = \"Count\") +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n2.6.2 Exercise 2: Simulated Histogram\nSimulate 300 observations from a normal distribution with mean 70 and standard deviation 10.\n\nPlot a histogram using binwidth = 5 and 10.\nOverlay a density curve using geom_density().\nDescribe the center, spread, and shape of the distribution.\n\n\n\nView Solution\nset.seed(4750)\nx &lt;- rnorm(300, mean = 70, sd = 10)\n# Histogram with binwidth 5\np1 &lt;- ggplot(data.frame(x), aes(x = x)) +\n  geom_histogram(\n    binwidth = 5,\n    fill = \"skyblue\",\n    color = \"white\",\n    alpha = 0.7\n  ) +\n  geom_density(aes(y = after_stat(count) * 5),\n               color = \"red\",\n               linewidth = 1.2) +\n  labs(title = \"Histogram (binwidth = 5) with Density\", x = \"Value\", y = \"Count\") +\n  theme_minimal()\n\n# Histogram with binwidth 10\np2 &lt;- ggplot(data.frame(x), aes(x = x)) +\n  geom_histogram(\n    binwidth = 10,\n    fill = \"orange\",\n    color = \"white\",\n    alpha = 0.7\n  ) +\n  geom_density(aes(y =  after_stat(count) * 10),\n               color = \"red\",\n               linewidth = 1.2) +\n  labs(title = \"Histogram (binwidth = 10) with Density\", x = \"Value\", y = \"Count\") +\n  theme_minimal()\n\nlibrary(patchwork)\np1 / p2\n\n\n\n\n\n\n\n2.6.3 Exercise 3: Boxplots by Group\nSimulate exam scores for two groups of students (Group A and Group B), each with 50 observations.\n\nGenerate scores from rnorm(50, mean=80, sd=5) for Group A and rnorm(50, mean=75, sd=7) for Group B.\nCreate a combined data.frame and make a boxplot comparing the two groups.\nInterpret differences in central tendency and variability.\n\n\n\nView Solution\nset.seed(4750)\ngroup_a &lt;- rnorm(50, mean = 80, sd = 5)\ngroup_b &lt;- rnorm(50, mean = 75, sd = 7)\n\nscores &lt;- data.frame(\n  Score = c(group_a, group_b),\n  Group = rep(c(\"A\", \"B\"), each = 50)\n)\n\nlibrary(ggplot2)\nggplot(scores, aes(x = Group, y = Score, fill = Group)) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(title = \"Exam Scores by Group\", y = \"Score\")\n\n\n\n\n\nInterpretation:\n\nCentral tendency: Group A has a higher median and mean score than Group B.\nVariability: Group B shows a wider spread (greater interquartile range and more outliers) compared to Group A, which aligns with the larger standard deviation used in its simulation.\n\n\n\n2.6.4 Exercise 4: Scatterplot with Regression\nSimulate a dataset of 100 observations where x ~ runif(100, 0, 100) and y = 0.5 * x + rnorm(100, 0, 5).\n\nCreate a scatterplot of y vs x.\nAdd a regression line.\nDescribe the relationship and interpret the slope.\n\n\n\nView Solution\nset.seed(4750)\nx &lt;- runif(100, 0, 100)\ny &lt;- 0.5 * x + rnorm(100, 0, 5)\ndata &lt;- data.frame(x = x, y = y)\n\nggplot(data, aes(x = x, y = y)) +\n  geom_point(color = \"steelblue\", alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(title = \"Scatterplot of y vs x with Regression Line\",\n       x = \"x\",\n       y = \"y\") +\n  theme_minimal()\n\n\n\n\n\nInterpretation:\n\nThere is a strong positive linear relationship between x and y, as expected from the model.\nThe slope of the regression line is close to 0.5, indicating that, on average, each 1-unit increase in x results in a 0.5-unit increase in y.\nThe scatter around the regression line reflects the normal error term with standard deviation 5."
  },
  {
    "objectID": "ch2/02-graphs.html#two-quantitative-one-categorical",
    "href": "ch2/02-graphs.html#two-quantitative-one-categorical",
    "title": "2  Graphs and Data Visualization",
    "section": "2.7 Two Quantitative & One Categorical",
    "text": "2.7 Two Quantitative & One Categorical\n\nGrouping: a graph consisting of a single panel with multiple variables differentiated using different visual characteristics such as color, shape, and size.\nFaceting: a graph consisting of several separate panels, with one for each level of the faceted variable, or combination of two faceted variables.\n\n\n\nR Code: Grouping\nggplot(tips, aes(x = bill, y = tip, color = sex)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = 'lm', se = FALSE) +\n  labs(title = 'Tip vs Bill by Gender')\n\n\n\n\n\n\n\nR Code: Faceting\nggplot(tips, aes(x=bill, y=tip)) + \n  geom_point(alpha=0.6) + \n  geom_smooth(method = 'lm', se = FALSE) + \n  facet_wrap(~ sex, ncol=2) + \n  labs(title = 'Tip vs Bill by Gender')\n\n\n\n\n\n\nDoes the relationship differ for men and women?"
  },
  {
    "objectID": "ch2/02-graphs.html#two-quantitative-two-categorical-variables",
    "href": "ch2/02-graphs.html#two-quantitative-two-categorical-variables",
    "title": "2  Graphs and Data Visualization",
    "section": "2.8 Two Quantitative & Two Categorical Variables",
    "text": "2.8 Two Quantitative & Two Categorical Variables\n\n\nR Code: Grouping + Faceting\nggplot(tips, aes(x = bill, y = tip, color = sex)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_grid(smoker ~ .) +\n  labs(title = \"Tip vs Bill by Sex and Smoking Status\",\n       x = \"Bill ($)\", y = \"Tip ($)\")\n\n\n\n\n\n\n\nR Code: Faceting Only\nggplot(tips, aes(x = bill, y = tip, color = sex)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_grid(smoker ~ sex) +\n  labs(title = \"Tip vs Bill by Sex and Smoking Status\",\n       x = \"Bill ($)\", y = \"Tip ($)\")"
  },
  {
    "objectID": "ch2/02-graphs.html#three-variables",
    "href": "ch2/02-graphs.html#three-variables",
    "title": "2  Graphs and Data Visualization",
    "section": "2.9 Three Variables",
    "text": "2.9 Three Variables\n\n2.9.1 3D Scatterplot\n\n\nCode\nlibrary(plotly)\ndata(mtcars)\n# Example with mtcars\nplot_ly(\n  data = mtcars,\n  x = ~mpg, y = ~hp, z = ~wt,\n  type = 'scatter3d',\n  mode = 'markers',\n  color = ~as.factor(cyl)\n)\n\n\n\n\n\n\n\n\n2.9.2 Bubble Chart\n\n\nCode\nggplot(mtcars, \n       aes(x=mpg, y=hp, size=wt)) + \n  geom_point(alpha=.5, \n             fill=\"red\", \n             color=\"black\",\n             shape=21) + \n  scale_size_continuous(range = c(1, 5))"
  },
  {
    "objectID": "ch2/02-graphs.html#scatterplot-matrix",
    "href": "ch2/02-graphs.html#scatterplot-matrix",
    "title": "2  Graphs and Data Visualization",
    "section": "2.10 Scatterplot Matrix",
    "text": "2.10 Scatterplot Matrix\nA scatterplot matrix (sometimes called a “pairs plot”) is used to visualize the pairwise relationships between several quantitative variables in a dataset, all at once.\n\nEach row and column represents one variable.\nThe off-diagonal panels show scatterplots for each pair of variables (e.g., x vs y, x vs z, y vs z).\nThe diagonal often shows the distribution of each variable (as a histogram, density, or boxplot).\nYou can optionally color points by a group or class variable.\n\n\n\nR Code: Scatterplot Matrix\ndata(mtcars)\nlibrary(GGally)\nGGally::ggpairs(\n  mtcars,\n  columns = c(\"mpg\", \"hp\", \"wt\")\n)\n\n\n\n\n\n\n\nScatterplot Matrix with Base R\npairs(mtcars[,c(\"mpg\", \"hp\", \"wt\")])\n\n\n\n\n\n\n\nR Code: Sample Correlation\ncor(mtcars[,c(\"mpg\", \"hp\", \"wt\")])\n\n\n           mpg         hp         wt\nmpg  1.0000000 -0.7761684 -0.8676594\nhp  -0.7761684  1.0000000  0.6587479\nwt  -0.8676594  0.6587479  1.0000000"
  },
  {
    "objectID": "ch2/02-graphs.html#parallel-coordinates",
    "href": "ch2/02-graphs.html#parallel-coordinates",
    "title": "2  Graphs and Data Visualization",
    "section": "2.11 Parallel Coordinates",
    "text": "2.11 Parallel Coordinates\nA parallel coordinates plot is a powerful visualization tool used for exploring and comparing multivariate data (data with several quantitative variables). It’s especially valuable when you want to see how patterns, clusters, or outliers appear across many variables at once.\n\n\nR Code: Parallel Coordinates Plot\n# Convert cyl to factor for coloring\nmtcars$cyl &lt;- as.factor(mtcars$cyl)\n\nGGally::ggparcoord(\n  mtcars,\n  columns = c(1,4,6), # mpg, hp, wt\n  groupColumn = \"cyl\",\n  scale = \"uniminmax\",    # Normalize to [0,1] for fair comparison\n  showPoints = TRUE,\n  alphaLines = 0.6\n) +\n  scale_color_brewer(palette = \"Dark2\", name = \"Cylinders\") +\n  labs(title = \"Parallel Coordinates Plot: mtcars (by cylinders)\")\n\n\n\n\n\nInterpretation:\n\nEach line is a car.\nThe color indicates the number of cylinders.\nCars with more cylinders generally have lower mpg and higher hp and weight."
  },
  {
    "objectID": "ch2/02-graphs.html#exercises",
    "href": "ch2/02-graphs.html#exercises",
    "title": "2  Graphs and Data Visualization",
    "section": "2.12 Exercises",
    "text": "2.12 Exercises\n\n2.12.1 Exercise 1: Ames Housing Data\nLoad the Ames Housing data into R and answer the following questions:\n\nAmes = read.csv(\"Ameshousing.csv\")\nhead(Ames)\n\n  SalePrice Bedrooms LotArea LivingArea GarageArea Neighborhood\n1    270000        4   11792       2283        632      Gilbert\n2    377500        3   14892       1746        758      Gilbert\n3    337500        3   12456       1718        786      NridgHt\n4    462000        4   14257       2772        754      NridgHt\n5    489900        2   14803       2084       1220      NridgHt\n6    555000        2   15431       2402        672      NridgHt\n\n\n\nSummary Statistics\n\n\nHow many homes were included in this study?\n\nWhat are the names of the variables for which information was collected?\nCompute the median living area, the mean living area and the standard error for the mean living area for all of the houses in the data set.\nCompute the median living area, the mean living area and the standard error for the mean living area for the houses in each neighborhood.\nCompute the number of houses, mean sale price and the standard error of the mean sale price for houses with living areas less than 1800 square feet.\nSummarize in a paragraph the information in the correlation matrix about associations between sales price, lot size, living area, garage area, and number of rooms from the sample correlation matrix.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\ndim(Ames)[1]\n\n\n[1] 168\n\n\n\n\nCode\nnames(Ames)\n\n\n[1] \"SalePrice\"    \"Bedrooms\"     \"LotArea\"      \"LivingArea\"   \"GarageArea\"  \n[6] \"Neighborhood\"\n\n\n\n\nCode\nmean_LivingArea &lt;- mean(Ames$LivingArea)\nmean_LivingArea\n\n\n[1] 1700.107\n\n\nCode\nmedian_LivingArea &lt;- median(Ames$LivingArea)\nsderr_LivingArea &lt;- sd(Ames$LivingArea)/ sqrt(length(Ames$LivingArea))\nsderr_LivingArea\n\n\n[1] 32.32775\n\n\n\n\nCode\nmean_LivingArea &lt;- tapply(Ames$LivingArea, Ames$Neighborhood, mean)\nmean_LivingArea\n\n\n CollgCr  Gilbert  NridgHt \n1565.921 1658.966 1880.921 \n\n\nCode\nmedian_LivingArea &lt;- tapply(Ames$LivingArea, Ames$Neighborhood, median)\nmedian_LivingArea\n\n\nCollgCr Gilbert NridgHt \n 1591.5  1560.0  1743.0 \n\n\nCode\nsd_LivingArea &lt;- tapply(Ames$LivingArea, Ames$Neighborhood, sd)\nsderr_LivingArea &lt;- sd_LivingArea / sqrt(tapply(Ames$LivingArea, Ames$Neighborhood, length))\nsderr_LivingArea\n\n\n CollgCr  Gilbert  NridgHt \n42.72812 56.21652 57.40376 \n\n\n\n\nCode\nAmes2 &lt;- Ames[ Ames$LivingArea&lt;1800, ]\nn &lt;- dim(Ames2)[1]\ncat(\"number of houses with living area &lt; 1800: \", n)\n\n\nnumber of houses with living area &lt; 1800:  116\n\n\n\n\nCode\nmean_Saleprice &lt;- mean(Ames2$SalePrice)\ncat(\"Mean Sale Price for Houses with Living Area &lt; 1800: \", mean_Saleprice)\n\n\nMean Sale Price for Houses with Living Area &lt; 1800:  215756.7\n\n\n\n\nCode\nsderr_SalePrice &lt;- sd(Ames2$SalePrice)/ sqrt(length(Ames2$SalePrice))\ncat(\"Stderror for Mean Sale Price: \", sderr_SalePrice)\n\n\nStderror for Mean Sale Price:  4693.031\n\n\n\n\n\n\nGraphical Summaries\n\n\nCreate a scatterplot with a smooth curve passed through the points.\nFit a least squares regression line to the data in the plot.\nWhat does the regression line indicate about sales prices of houses increase with living area?\nWhat information is provided by the plot with smooth curve?\nAdd a new variable to the data frame that contains information on sales price divided by the living area in the house, i.e., the sales price per square foot of living area.\nCreate a histogram for the price per square foot of living space.\nWhat does this histogram reveal about the distribution of costs per square foot of living space for houses sold in the Ames area? The description should be based on the shape, center and spread of the distribution.\nCreate separate plots of sales prices versus living space categorized for each neighborhood\nConstruct side-by-side box plots to compare prices per square foot of living space across neighborhoods.\nDescribe the relationship between sales price and total living area of house changes across the three neighborhoods.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nAmes %&gt;% \n  ggplot(aes(x=LivingArea, y=SalePrice)) + \n    geom_point(alpha=.7) + \n    geom_smooth() + \n    labs(\n      x=\"Living Area (sq ft)\",\n      y=\"Sale Price (dollars)\",\n      title=\"Sale Price vs Total Living Area\"\n    )\n\n\n\n\n\n\n\nCode\nlm(SalePrice ~ LivingArea, data = Ames)\n\n\n\nCall:\nlm(formula = SalePrice ~ LivingArea, data = Ames)\n\nCoefficients:\n(Intercept)   LivingArea  \n   -21769.1        159.5  \n\n\nCode\nplot( Ames$LivingArea, Ames$SalePrice, \n      xlab=\"Living Area (sq ft)\", \n      ylab=\"Sale Price (dollars)\",\n      main = \"Sale Price vs Total Living Area\")\nabline(lm(SalePrice~LivingArea, data=Ames), lty=1)\n\n\n\n\n\n\nThe slope of the regression suggests that the mean sale price of homes in the Ames area goes up by about 159.50 dollars for each additional square foot of living space.\nThe smooth curve indicates that there is an approximate straight line relationship between the mean home price and the living area of homes for homes with between 800 and 2000 square feet of living area, but the relationship curves up for larger home sizes. Also the variation in sale prices tends to increase with the amount of living area in the house.\n\n\n\nCode\nAmes$pricesqft  &lt;-  Ames$SalePrice/Ames$LivingArea \nhead(Ames)\n\n\n  SalePrice Bedrooms LotArea LivingArea GarageArea Neighborhood pricesqft\n1    270000        4   11792       2283        632      Gilbert  118.2654\n2    377500        3   14892       1746        758      Gilbert  216.2085\n3    337500        3   12456       1718        786      NridgHt  196.4494\n4    462000        4   14257       2772        754      NridgHt  166.6667\n5    489900        2   14803       2084       1220      NridgHt  235.0768\n6    555000        2   15431       2402        672      NridgHt  231.0575\n\n\n\n\nCode\nAmes %&gt;% ggplot() + \n  geom_histogram(aes(pricesqft), binwidth =20) \n\n\n\n\n\n\nThis histogram indicates that the distribution of costs per square foot of living space for houses sold in the Ames area is centered around $145 per square foot. The distribution is skewed to the right and it appears to be bimodal with one mode around $125 per square foot and another near $165 pre sqaure foot. Most houses cost between $100 and $200 per square foot of living space.\n\n\n\nCode\nAmes %&gt;% \n  ggplot(aes(x=LivingArea, y=SalePrice)) + \n  geom_point() + \n  geom_smooth() + \n  facet_grid( . ~ Neighborhood) + \n  labs(\n    x=\"Living Area (sq ft)\",\n    y=\"Sale Price (dollars)\",\n    title=\"Sales Price vs Total Living Area\"\n  )\n\n\n\n\n\nCode\nAmes %&gt;% \n  ggplot(aes(x=LivingArea, y=SalePrice)) + \n  geom_point() + \n  geom_smooth() + \n  facet_grid(Neighborhood ~ .) + \n  labs(\n    x=\"Living Area (sq ft)\",\n    y=\"Sale Price (dollars)\",\n    title=\"Sales Price vs Total Living Area\"\n  )\n\n\n\n\n\n\n\nCode\nAmes %&gt;% \n  ggplot(aes(x=Neighborhood, y=pricesqft)) + \n  geom_boxplot() + \n  labs(y=\"Sales Price per Square Foot\")\n\n\n\n\n\n\nIn all three neighborhoods sales prices tend to be larger for house with more living area. In the College Circle (CollgCr) and Northridge Heights (NridgHt) neighborhoods, there are strong linear relationships between these two variables. Because the slope is larger for houses in the Northridge Heights neighborhood, the price per square foot of living space is higher. There are more expensive houses in the Northridge Heights neighborhood. In Gilbert, the trend in sales prices is not so close to a straight line. There is little trend in sales prices for houses with less than 1,750 square feet of living space. There is also little trend in the sales prices for houses with living space above 1,900 square feet, but those houses are more expensive than house with less then 17,50 square feet. There is one relatively expensive house with about 1,750 square feet of living space that appears to be an outlier. Perhaps this house has an extremely large lot size or some additional buildings on the property. This should be investigated.\n\n\n\n\n\n\n2.12.2 Exercise 2: Music Clips Data\nThe music clips data is posted in music-plusnew-sub.csv. The data file has five quantitative variables containing audio information from 62 songs. The first two columns (Artist, Type) describe the artist and type of music. The raw data come from a time series for the sound produced by each music clip (track). For each time series the variance of amplitude, average amplitude, maximum amplitude, and two additional variables calculated from the spectral decomposition of the time series are calculated. The Type variable classifies the tracks as either Rock, Classical or New Wave, and there are 5 tracks that are not identified.\n\nRead the data into a data frame, indicating that the row names are in column 1 of the data file and that column is not a variable.\nObtain information on the dimensions of the data frame. Also list the column names. List the first six columns odf data.\nFirst select a subset of the data that contains only classical and rock music.\nFor classical and rock music make histograms for the avergae amplitude variable (LAve) faceted by Type. Set the binwidth to units of 10. How do the distributions of average amplitude values differ between classical and rock music?\nMake a scatterplot of LVar vs LAve, with points colored by the type of music. Describe differences between the patterns of the points on the plot corresponding to Rock and Classical music.\nSelect three music types. The other songs have missing values for the music type.\nMake a parallel coordinate plot\nReorder how the variables appear on the plot.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\ndat = read.csv(\"music-plusnew-sub.csv\", row.names=1)  \n\nhead(dat) \n\n\n              Artist Type     LVar      LAve  LMax   LFEner     LFreq\nDancing Queen   Abba Rock 17600756 -90.00687 29921 105.9210  59.57379\nKnowing Me      Abba Rock  9543021 -75.76672 27626 102.8362  58.48031\nTake a Chance   Abba Rock  9049482 -98.06292 26372 102.3249 124.59397\nMamma Mia       Abba Rock  7557437 -90.47106 28898 101.6165  48.76513\nLay All You     Abba Rock  6282286 -88.95263 27940 100.3008  74.02039\nSuper Trouper   Abba Rock  4665867 -69.02084 25531 100.2485  81.40140\n\n\nCode\nstr(dat)\n\n\n'data.frame':   62 obs. of  7 variables:\n $ Artist: chr  \"Abba\" \"Abba\" \"Abba\" \"Abba\" ...\n $ Type  : chr  \"Rock\" \"Rock\" \"Rock\" \"Rock\" ...\n $ LVar  : num  17600756 9543021 9049482 7557437 6282286 ...\n $ LAve  : num  -90 -75.8 -98.1 -90.5 -89 ...\n $ LMax  : int  29921 27626 26372 28898 27940 25531 14699 8928 22962 15517 ...\n $ LFEner: num  106 103 102 102 100 ...\n $ LFreq : num  59.6 58.5 124.6 48.8 74 ...\n\n\nCode\nsummary(dat)\n\n\n    Artist              Type                LVar                LAve        \n Length:62          Length:62          Min.   :   293608   Min.   :-98.063  \n Class :character   Class :character   1st Qu.:  2844213   1st Qu.: -6.253  \n Mode  :character   Mode  :character   Median :  8210359   Median : -5.662  \n                                       Mean   : 19951792   Mean   : -7.807  \n                                       3rd Qu.: 24547475   3rd Qu.:  1.962  \n                                       Max.   :129472199   Max.   :216.232  \n      LMax           LFEner           LFreq       \n Min.   : 2985   Min.   : 83.88   Min.   : 41.41  \n 1st Qu.:16200   1st Qu.:101.69   1st Qu.: 99.18  \n Median :24431   Median :104.35   Median :175.29  \n Mean   :22486   Mean   :104.03   Mean   :231.39  \n 3rd Qu.:29918   3rd Qu.:108.15   3rd Qu.:315.12  \n Max.   :32766   Max.   :114.00   Max.   :877.77  \n\n\nCode\ntable(dat$Type)\n\n\n\nClassical  New wave      Rock \n       24         3        30 \n\n\nCode\ndf.sub = dat %&gt;% \n  dplyr::filter(Type==\"Rock\" | Type==\"Classical\")\n\nggplot(df.sub, aes(LAve)) + \n  geom_histogram(binwidth=10) + \n  facet_wrap( ~ Type, ncol=1)\n\n\n\n\n\nCode\nggplot(df.sub, aes(x=LVar, y=LAve, color=Type)) + \n  geom_point()\n\n\n\n\n\nCode\ndat2 = dat %&gt;% \n  filter(Type==\"Rock\" | Type==\"Classical\" | Type==\"New wave\")\nhead(dat2)\n\n\n              Artist Type     LVar      LAve  LMax   LFEner     LFreq\nDancing Queen   Abba Rock 17600756 -90.00687 29921 105.9210  59.57379\nKnowing Me      Abba Rock  9543021 -75.76672 27626 102.8362  58.48031\nTake a Chance   Abba Rock  9049482 -98.06292 26372 102.3249 124.59397\nMamma Mia       Abba Rock  7557437 -90.47106 28898 101.6165  48.76513\nLay All You     Abba Rock  6282286 -88.95263 27940 100.3008  74.02039\nSuper Trouper   Abba Rock  4665867 -69.02084 25531 100.2485  81.40140\n\n\nCode\nGGally::ggparcoord(dat2, columns=3:7,\n                   groupColumn = \"Type\",\n                   title=\"Parallel Coordinate Plot: Music Types\")\n\n\n\n\n\nCode\nGGally::ggparcoord(dat2, columns=c(4,3,5,6,7),\n                   groupColumn = \"Type\",\n                   title=\"Parallel Coordinate Plot: Music Types\")"
  },
  {
    "objectID": "ch3/03-MVN.html",
    "href": "ch3/03-MVN.html",
    "title": "3  Multivariate Normal Distribution",
    "section": "",
    "text": "3.1 Why Multivariate Normal?\nThe univariate normal distribution is fundamental in statistics. But real data usually involve multiple variables measured on the same subjects, and these are often correlated.\nRecall that if a random variable X has a normal distribution with mean \\mu and variance \\sigma^2, its density function has the form \nf(x) = \\frac{1}{ \\sqrt{2 \\pi } \\sigma} \\exp \\left\\{-\\frac{1}{2\\sigma^2} (x - \\mu)^2 \\right\\}, \\;\\;\\;\\; -\\infty &lt; x &lt; \\infty.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate Normal Distribution</span>"
    ]
  },
  {
    "objectID": "ch3/03-MVN.html#why-multivariate-normal",
    "href": "ch3/03-MVN.html#why-multivariate-normal",
    "title": "3  Multivariate Normal Distribution",
    "section": "3.1 Why Multivariate Normal?",
    "text": "3.1 Why Multivariate Normal?\nThe univariate normal distribution is fundamental in statistics. But real data usually involve multiple variables measured on the same subjects, and these are often correlated.\n\nHeight and weight of students, or exam scores in different subjects.\n\nRecall that if a random variable X has a normal distribution with mean \\mu and variance \\sigma^2, its density function has the form \nf(x) = \\frac{1}{ \\sqrt{2 \\pi } \\sigma} \\exp \\left\\{-\\frac{1}{2\\sigma^2} (x - \\mu)^2 \\right\\}, \\;\\;\\;\\; -\\infty &lt; x &lt; \\infty."
  },
  {
    "objectID": "ch3/03-MVN.html#sec-matrix-background",
    "href": "ch3/03-MVN.html#sec-matrix-background",
    "title": "3  Multivariate Normal Distribution",
    "section": "3.2 Matrix Background",
    "text": "3.2 Matrix Background\n\nThe inverse of a p\\times p matrix A is denoted by A^{-1}.\nif A^{-1} exists, A^{-1} is the unique matrix such that      AA^{-1} = A^{-1}A = I  =  \\left[ \\begin{array}{ccccc}  1 & 0 & \\cdots & 0 & 0 \\\\\n                                                              0 & 1 & \\cdots & 0 & 0  \\\\   \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n                                                              0 & 0 & \\cdots & 1 & 0  \\\\ 0 & 0 & \\cdots & 0 & 1  \\end{array} \\right]\n\n\nA is said to be symmetric if A is the same as its transpose, i.e., A=A^\\top or A = A'.\nA is said to be positive definite if \n\\left[ \\begin{array}{cccc} x_1 & x_2 & \\cdots & x_p \\end{array} \\right]\nA\n\\left[ \\begin{array}{c} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_p \\end{array} \\right]\n&gt; 0 \\quad \\text{for any} \\quad\n\\left[ \\begin{array}{cccc} x_1 & x_2 & \\cdots & x_p \\end{array} \\right] \\neq \\mathbf{0}\n\nIf A is positive definite, then A^{-1} exists.\n\n\n\n\n\n\n\nExercise: Matrix Algebra\n\n\n\n\n\n\nA = matrix(c(2,1,1,4), ncol=2, byrow=TRUE)\nA\n\n     [,1] [,2]\n[1,]    2    1\n[2,]    1    4\n\n\n\n\nR Code: Scalar-matrix multiplication\na = 2 \na * A\n\n\n     [,1] [,2]\n[1,]    4    2\n[2,]    2    8\n\n\n\n\nR Code: Matrix-matrix addition\nB = matrix(c(1,2,2,9), ncol=2, byrow=TRUE)\nA + B \n\n\n     [,1] [,2]\n[1,]    3    3\n[2,]    3   13\n\n\n\n\nR Code: Matrix-vector multiplication\nx = c(1,2)\nA %*% x \n\n\n     [,1]\n[1,]    4\n[2,]    9\n\n\n\n\nR Code: Matrix-matrix multiplication\nA %*% B \n\n\n     [,1] [,2]\n[1,]    4   13\n[2,]    9   38\n\n\n\n\nR Code: Matrix inversion\n# matrix inversion\nsolve(A)\n\n\n           [,1]       [,2]\n[1,]  0.5714286 -0.1428571\n[2,] -0.1428571  0.2857143\n\n\nR Code: Matrix inversion\n# check result\nA %*% solve(A)\n\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n\n\n\n\n\n3.2.1 Eigenvalues and Eigenvectors\n\n\n\n\n\n\nDefinition\n\n\n\nGiven a p \\times p square matrix A, an eigenvector \\mathbf{v} and its corresponding eigenvalue \\lambda satisfy: A \\mathbf{v} = \\lambda \\mathbf{v}\n\n\nInterpretation: Multiplying A by its eigenvector \\mathbf{v} simply stretches or shrinks \\mathbf{v} by a factor \\lambda—the direction of \\mathbf{v} does not change.\n\nWhy Do They Matter?\n\nEigenvectors show the principal directions in which A stretches or compresses the space.\nIn statistics, the eigenvectors of a covariance matrix gives the directions of maximum variance.\nThe corresponding eigenvalues tell you how much variance there is in each direction as explained below.\n\nSome Properties\n\nIf (\\lambda_j, \\mathbf{e}_j) is an eigenvalue-eigenvector pair for a p \\times p matrix A, by definition A \\mathbf{e}_j = \\lambda_j \\mathbf{e}_j.\nEigenvectors are usually scaled to have length one, i.e., 1=\\mathbf{e}_j^{\\top} \\mathbf{e}_j = e_{j1}^2+e_{j2}^2 + \\cdots + e_{jp}^2.\nEigenvectors are mutually orthogonal, i.e., \\mathbf{e}_j^{\\top} \\mathbf{e}_k = 0 for any j \\ne k.\n(\\lambda_j^{-1}, \\mathbf{e}_j) is an eigenvalue-eigenvector pair of A^{-1} when A is positive definite and A^{-1} exists.\n\n\n\n\n3.2.2 Spectral Decomposition\n\n\n\n\n\n\nSpectral decomposition\n\n\n\nFor any p\\times p symmetric matrix A, its spectral decomposition is  A = \\lambda_1 \\mathbf{e}_1 \\mathbf{e}_1^{\\top} +  \\lambda_2 \\mathbf{e}_2 \\mathbf{e}_2^{\\top} +  \\cdots +  \\lambda_p \\mathbf{e}_p \\mathbf{e}_p^{\\top}\n\n\n\n\nIf A^{-1} exists, then A^{-1} = \\lambda_1^{-1} \\mathbf{e}_1 \\mathbf{e}_1^{\\top} + \\lambda_2^{-1} \\mathbf{e}_2 \\mathbf{e}_2^{\\top} + \\cdots + \\lambda_p^{-1} \\mathbf{e}_p \\mathbf{e}_p^{\\top}.\nA^{1/2} = \\lambda_1^{1/2} \\mathbf{e}_1 \\mathbf{e}_1^{\\top} + \\lambda_2^{1/2} \\mathbf{e}_2 \\mathbf{e}_2^{\\top} + \\cdots + \\lambda_p^{1/2} \\mathbf{e}_p \\mathbf{e}_p^{\\top}\nA^{-1/2} = \\lambda_1^{-1/2} \\mathbf{e}_1 \\mathbf{e}_1^{\\top} + \\lambda_2^{-1/2} \\mathbf{e}_2 \\mathbf{e}_2^{\\top} + \\cdots + \\lambda_p^{-1/2} \\mathbf{e}_p \\mathbf{e}_p^{\\top}\nA^{1/2} A^{1/2} = A and A^{-1/2}A^{-1/2}=A^{-1}.\n\n\n\nR Code: Create a Matrix\nA = matrix(c(4, 2, 1, 3), nrow = 2)\n\n\n\n\nR Code: Spectral Decomposition\neig = eigen(A)\nprint(eig)\n\n\neigen() decomposition\n$values\n[1] 5 2\n\n$vectors\n          [,1]       [,2]\n[1,] 0.7071068 -0.4472136\n[2,] 0.7071068  0.8944272\n\n\n\n\n3.2.3 Determinant of a p\\times p matrix\n\nThe determinant of the matrix A is denoted by |A| or \\text{det}(A).\nThe determinant of a p \\times p matrix is the product of its eigenvalues, i.e., |A| = \\text{det}(A) = \\lambda_1 \\lambda_2 \\cdots \\lambda_p.\nAll of the eigenvalues of a positive definite matrix are positive.\nThe determinant of a positive definite matrix must be larger than zero.\nIf at least one eigenvalue is zero and the inverse of the matrix does not exist, then the determinant of the matrix is zero.\n\n\n\nMatrix Determinant: det for Small Matrices\ndet(A) \n\n\n[1] 10\n\n\n\n\nMatrix Determinant: Eigen Decomposition\nE = eigen(A)\nprod(E$values)\n\n\n[1] 10"
  },
  {
    "objectID": "ch3/03-MVN.html#probability-density-function-of-mvn",
    "href": "ch3/03-MVN.html#probability-density-function-of-mvn",
    "title": "3  Multivariate Normal Distribution",
    "section": "3.3 Probability Density Function of MVN",
    "text": "3.3 Probability Density Function of MVN\nThe multivariate normal distribution (MVN) generalizes the univariate normal distribution to multiple variables.\nA random vector \\mathbf{x} = (x_1, x_2, \\ldots, x_p)^\\top follows a multivariate normal distribution if every linear combination of its components is normal. We use the notation: \n\\mathbf{x} \\sim N_p(\\boldsymbol{\\mu}, {\\Sigma}) \\quad\n\\text{ or }\\quad \\mathbf{x} \\sim \\text{MVN}(\\boldsymbol\\mu, \\Sigma)\n where\n\n\\boldsymbol{\\mu}:=(\\mu_1, \\mu_2, \\ldots, \\mu_p)^\\top is the p-dimensional mean vector\nand \\Sigma = \\left[ \\begin{array}{cccc} \\sigma_{11} & \\sigma_{12} & \\cdots & \\sigma_{1p} \\\\  \\sigma_{21} & \\sigma_{22} & \\cdots \\  & \\sigma_{2p} \\\\  \\vdots & \\vdots & \\ddots & \\vdots \\\\  \\sigma_{p1} & \\sigma_{p2} & \\cdots & \\sigma_{pp} \\end{array} \\right]  is the p \\times p covariance matrix of \\mathbf{x}.\n\n\n\n\n\n\n\nProbability Density Function (PDF)\n\n\n\nThe probability density function for \\mathbf{x} \\sim N_p(\\boldsymbol \\mu, \\Sigma) is \nf(\\mathbf{x}) = (2\\pi)^{-p/2} |\\Sigma|^{-1/2} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol\\mu)^\\top \\Sigma^{-1}(\\mathbf{x}-\\boldsymbol\\mu)\\right)\n\nGeometric Interpretation:\n\n\\boldsymbol{\\mu}: The center of the point cloud.\n{\\Sigma}: Controls the spread and orientation.\n\nDiagonal = variances\nOff-diagonal = covariances (correlation/shape)\n\n\n\n\n\n\n\n\n\n\nMahalanobis Distance\n\n\n\nThe quadratic form (\\mathbf{x}-\\boldsymbol\\mu)^\\top \\Sigma^{-1}(\\mathbf{x}-\\boldsymbol\\mu) in the density formula is the squared statistical distance of \\mathbf{x} from \\boldsymbol\\mu. It is often referred to as the square of the Mahalanobis distance.\n\n\n\n3.3.1 Some Properties\nIf \\mathbf{x} \\sim N_p(\\boldsymbol\\mu, \\Sigma), then the following statements are true:\n\nMarginal distributions: Each variable is normally distributed, i.e., x_i \\sim N(\\mu_i, \\sigma_{ii}).\nAny linear combinations is normal: Let a_1, \\ldots, a_p \\in \\mathbb{R}. Then \\mathbf{a}^\\top \\mathbf{x} = \\sum_{i=1}^p a_i x_i \\sim N(\\mathbf{a}^\\top \\boldsymbol \\mu, \\mathbf{a}^\\top \\Sigma \\mathbf{a})\nIf \\mathbf{x} \\sim N_p(\\boldsymbol \\mu, \\Sigma), then (\\mathbf{x}-\\boldsymbol\\mu)^\\top \\Sigma^{-1}(\\mathbf{x}-\\boldsymbol\\mu) \\sim \\chi_p^2\nConditional distributions: Any subset of variables, conditional on others, is normal.\nElliptical contours: The joint density forms ellipses.\n\n\n\n3.3.2 R Exercise: MVN Properties\n\nmu = c(0,0)\nSigma = matrix(c(2,1,1,4),ncol=2, byrow=TRUE)\na = c(1,1)\n\n# simulate from MVN\nset.seed(4750)\nx = MASS::mvrnorm(1, mu=mu, Sigma=Sigma)\n\n\n\nR Code: MVN properties\n# linear combination  \nt(a) %*% x \n\n\n           [,1]\n[1,] -0.4976936\n\n\nR Code: MVN properties\n# quadratic form \nt(x-mu)%*%solve(Sigma)%*%(x-mu)\n\n\n           [,1]\n[1,] 0.07250492"
  },
  {
    "objectID": "ch3/03-MVN.html#visualizing-the-mvn-in-r",
    "href": "ch3/03-MVN.html#visualizing-the-mvn-in-r",
    "title": "3  Multivariate Normal Distribution",
    "section": "3.4 Visualizing the MVN in R",
    "text": "3.4 Visualizing the MVN in R\n\n3.4.1 Visualizing Marginal Distributions\n\n\nR Code: Visualize MVN\nlibrary(MASS) \nset.seed(4750) \nmu &lt;- c(0, 0) \nSigma_none &lt;- matrix(c(1, 0, 0, 1), nrow=2) \nSigma_pos &lt;- matrix(c(1, 0.8, 0.8, 1), nrow=2) \nSigma_neg &lt;- matrix(c(1, -0.8, -0.8, 1), nrow=2)\n\nX_none &lt;- mvrnorm(500, mu, Sigma_none) \nX_pos &lt;- mvrnorm(500, mu, Sigma_pos) \nX_neg &lt;- mvrnorm(500, mu, Sigma_neg)\n\npar(mfrow = c(1,3)) \nplot(X_none, main=\"No Correlation\", xlab=\"X1\", ylab=\"X2\", col=rgb(0,0,1,0.3), pch=16) \nplot(X_pos, main=\"Positive Correlation\", xlab=\"X1\", ylab=\"X2\", col=rgb(0,0.5,0,0.3), pch=16) \nplot(X_neg, main=\"Negative Correlation\", xlab=\"X1\", ylab=\"X2\", col=rgb(1,0,0,0.3), pch=16) \n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nTry changing the covariance matrix \\Sigma in the code above.\n\nWhat happens if you set the off-diagonal entries to 0?\nWhat about +0.9 or -0.9?\n\n\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\nLet’s visualize how changing the off-diagonal entries (correlation) in \\Sigma affects the bivariate normal:\n\n\nCode\nlibrary(MASS)\nset.seed(4750)\nmu &lt;- c(0, 0)\n\n# Case 1: No correlation (off-diagonal = 0)\nSigma_none &lt;- matrix(c(1, 0, 0, 1), nrow=2)\nX_none &lt;- mvrnorm(500, mu, Sigma_none)\n\n# Case 2: Strong positive correlation (off-diagonal = +0.9)\nSigma_pos &lt;- matrix(c(1, 0.9, 0.9, 1), nrow=2)\nX_pos &lt;- mvrnorm(500, mu, Sigma_pos)\n\n# Case 3: Strong negative correlation (off-diagonal = -0.9)\nSigma_neg &lt;- matrix(c(1, -0.9, -0.9, 1), nrow=2)\nX_neg &lt;- mvrnorm(500, mu, Sigma_neg)\n\npar(mfrow = c(1,3))\nplot(X_none, main = \"No correlation\", xlab = \"X1\", \n     ylab = \"X2\", col = rgb(0,0,1,0.4), pch = 16)\nplot(X_pos, main = \"Positive correlation\", xlab = \"X1\",\n     ylab = \"X2\", col = rgb(0,0.5,0,0.4), pch = 16)\nplot(X_neg, main = \"Negative correlation\", xlab = \"X1\",\n     ylab = \"X2\", col = rgb(1,0,0,0.4), pch = 16)\n\n\n\n\n\n\n\n\n\n\n3.4.2 Density Contour\n\nA set of all vectors \\mathbf{x} that correspond to a constant height of the density function forms an ellipsoid centered at \\boldsymbol\\mu.\nThe MVN density is constant for all \\mathbf{x}’s that are the same statistical distance from the population mean vector, i.e., all \\mathbf{x}’s that satisfy \n\\sqrt{(\\mathbf{x}-\\boldsymbol\\mu)^\\top \\Sigma^{-1}(\\mathbf{x}-\\boldsymbol\\mu)} = \\text{constant}.\n\nThe axes of the ellipses are in the directions of the eigenvectors of \\Sigma and the length of the j-th longest axis is proportional to \\sqrt{\\lambda_{j}}, where \\lambda_{j} is the eigenvalue associated with the j-th eigenvector of \\Sigma.\n\n\n\n\n\n\nInterpretation:\n\nThe ellipse shows a constant-density contour for the MVN.\nThe red and purple arrows indicate eigenvectors of \\Sigma.\nThe lengths of these arrows are proportional to \\sqrt{\\lambda_j}.\n\n\n\nR Code: Bivariate Normal Density Contour\nlibrary(mvtnorm)\nlibrary(ggplot2)\n\nmu = c(0, 0)\nSigma = matrix(c(1, 0.7, 0.7, 1), 2)\n\n# Create grid\nx = seq(-3, 3, length=100)\ny = seq(-3, 3, length=100)\ngrid = expand.grid(x = x, y = y)\n\n# Compute density at each grid point\ngrid$z = mvtnorm::dmvnorm(as.matrix(grid), \n                           mean = mu, sigma = Sigma)\n\n# Plot contours\nggplot(grid, aes(x = x, y = y, z = z)) +\n  geom_contour_filled(bins = 15) +\n  coord_fixed() +\n  labs(title = \"Bivariate Normal Density (Contours)\",\n       x = \"X1\", y = \"X2\", fill = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n3.4.3 3D Surface Plot\n\n\nR Code: 3D Surface Plot\nlibrary(plotly)\n\nz_matrix &lt;- matrix(grid$z, nrow = length(x), byrow = FALSE)\nplot_ly(x = x, y = y, z = z_matrix) %&gt;%\n  add_surface(\n    contours = list(\n      z = list(show = TRUE, \n               usecolormap = TRUE, \n               highlightcolor = \"#ff0000\", \n               project = list(z = TRUE)))) %&gt;%\n  layout(title = \"Bivariate Normal Density (Surface)\",\n         scene = list(xaxis = list(title = \"X1\"),\n                      yaxis = list(title = \"X2\"),\n                      zaxis = list(title = \"Density\")))"
  },
  {
    "objectID": "ch3/03-MVN.html#central-1-alphatimes-100-region-of-mvn",
    "href": "ch3/03-MVN.html#central-1-alphatimes-100-region-of-mvn",
    "title": "3  Multivariate Normal Distribution",
    "section": "3.5 Central (1-\\alpha)\\times 100\\% Region of MVN",
    "text": "3.5 Central (1-\\alpha)\\times 100\\% Region of MVN\n\n3.5.1 Properties of Density Contours\n\n\n\n\n\n\nDefinition\n\n\n\n\nThe probability is 1 - \\alpha that the value of a random vector will be inside the ellipsoid defined by \n(\\mathbf{x}-\\boldsymbol\\mu)^\\top \\Sigma^{-1}(\\mathbf{x}-\\boldsymbol\\mu) \\leq \\chi^2_{(p), 1-\\alpha}\n where \\chi^2_{(p),1-\\alpha} is the upper (100 \\times \\alpha) percentile of a chi-square distribution with p degrees of freedom.\nThis is smallest region that has probability (1-\\alpha) of containing a vector of observations randomly selected from the population.\nThe jth axis of this ellipsoid is determined by the eigenvector associated with the j-th largest eigenvalue of \\Sigma, for j = 1,...,p.\nThe distance along the j-th axis from the center to the boundary of the ellipsoid is \\sqrt{\\lambda_j}  \\left( \\frac{2}{p \\Gamma(p/2)} \\right)^{1/p} \\sqrt{ \\chi^2_{(p), 1-\\alpha}}\n\n\n\n\n\n3.5.2 Geometric Properties\n\nThe ratio of the lengths of the major and minor axes is  \\frac{\\text{Length of major axis}}{\\text{Length of minor axis}}=\\frac{\\sqrt{\\lambda_1}}{\\sqrt{\\lambda_2}} \nThe area of the ellipse containing the central (1-\\alpha) \\times 100 \\% of a bivariate normal population is \narea = \\pi \\chi^2_{(2), 1-\\alpha} \\sqrt{\\lambda_1} \\sqrt{\\lambda_2} =\\pi \\chi^2_{(2),1-\\alpha} |\\Sigma|^{1/2} \nFor p-dimensional normal distributions the hypervolume of the p-dimensional ellipsoid is \\frac{2\\pi^{p/2}}{p \\Gamma(p/2)} \\left[\\chi^2_{(p),1-\\alpha}\\right]^{p/2} |\\Sigma|^{1/2} \n\n\n\n\n\n\n\nGamma function\n\n\n\n\n\n\n\\Gamma(1) is defined to be 1.0,\n \\Gamma\\left(\\frac{p}{2}\\right)=\\left(\\frac{p}{2}-1\\right)\\left(\\frac{p}{2}-2\\right) \\cdots (2)(1) when p is an even integer,\nand  \\Gamma(\\frac{p}{2})=\\frac{(p-2)(p-4) \\cdots(3)(1)}{ 2^{(p-1)/2}} \\sqrt{\\pi}  when p is an odd integer\n\n\n\n\n\n\n3.5.3 50% and 90% Contours for Two Bivariate Normals\n\n\nR Code: Bivariate Normal Contours\nlibrary(MASS)\nlibrary(ggplot2)\nlibrary(mvtnorm)\n\n# Two bivariate normals: different means, same covariance\nmu1 &lt;- c(0, 0)\nmu2 &lt;- c(2, 2)\nSigma &lt;- matrix(c(2, 1.2, 1.2, 2), 2)\n\n# Grid for density evaluation\nx &lt;- seq(-3, 6, length=120)\ny &lt;- seq(-3, 6, length=120)\ngrid &lt;- expand.grid(x=x, y=y)\nxy = as.matrix(grid)\nz1 &lt;- dmvnorm(xy, mean=mu1, sigma=Sigma)\nz2 &lt;- dmvnorm(xy, mean=mu2, sigma=Sigma)\ngrid$z1 = z1 \ngrid$z2 = z2\n\n# Find contour levels for 50% and 90%\nget_density_level &lt;- function(mu, Sigma, p) {\n  d2 &lt;- qchisq(p, df=2)\n  detS &lt;- det(Sigma)\n  norm_const &lt;- 1/(2*pi*sqrt(detS))\n  exp_part &lt;- exp(-0.5 * d2)\n  norm_const * exp_part\n}\nlevel_50 &lt;- get_density_level(mu1, Sigma, 0.5)\nlevel_90 &lt;- get_density_level(mu1, Sigma, 0.9)\n\n# Plot\nggplot() +\n  geom_contour(data=grid, aes(x=x, y=y, z=z1), \n               breaks=c(level_50, level_90),\n               color=\"blue\", size=1.2, linetype=\"solid\") +\n  geom_contour(data=grid, aes(x=x, y=y, z=z2), \n               breaks=c(level_50, level_90),\n               color=\"red\", size=1.2, linetype=\"dashed\") +\n  geom_point(aes(x=mu1[1], y=mu1[2]), color=\"blue\", size=3) +\n  geom_point(aes(x=mu2[1], y=mu2[2]), color=\"red\", size=3) +\n  annotate(\"text\", x=mu1[1], y=mu1[2]-0.5, \n           label=\"mu1\", color=\"blue\") +\n  annotate(\"text\", x=mu2[1], y=mu2[2]+0.5, \n           label=\"mu2\", color=\"red\") +\n  coord_fixed() +\n  labs(\n    title=\"50% (inner) and 90% (outer) Contours of Two Bivariate Normals\",\n    x=\"X1\", y=\"X2\", \n    caption=\"Density peaks at the mean for each distribution.\") +\n  theme_minimal()"
  },
  {
    "objectID": "ch3/03-MVN.html#overall-measures-of-variability",
    "href": "ch3/03-MVN.html#overall-measures-of-variability",
    "title": "3  Multivariate Normal Distribution",
    "section": "3.6 Overall Measures of Variability",
    "text": "3.6 Overall Measures of Variability\n\n\n\n\n\n\nMeasures of variability\n\n\n\n\nGeneralized variance and generalized standard deviation tell us about the overall “multivariate” spread or “joint variability”—not just variable by variable, but how “big” the whole data cloud is, including dependencies.\nTotal variance tells us the aggregate variance but is “blind” to how variables overlap (i.e., to correlations).\n\n\n\n\n3.6.1 Generalized Variance\n\nDefinition: |\\Sigma|=\\lambda_1\\lambda_2\\cdots\\lambda_p \nThe generalized variance measures the overall spread of the data in all directions at once.\nIf any variable has no variation, or if there’s perfect linear dependence (collinearity), the generalized variance drops to zero (the volume “flattens”).\n\n\n\n3.6.2 Generalized Standard Deviation\n\nDefinition: |\\Sigma|^{1/2}=\\sqrt{\\lambda_1\\lambda_2\\cdots\\lambda_p} \nThe generalized standard deviation is proportional to the volume of the ellipsoid.\n\n\n\n3.6.3 Total Variance\n\nDefinition \n\\begin{aligned}\ntrace(\\Sigma)\\equiv tr(\\Sigma)&:= \\sigma_{11} + \\sigma_{22} + \\cdots + \\sigma_{pp} \\\\\n               &= \\lambda_1+\\lambda_2+\\cdots+\\lambda_p\n\\end{aligned}\n\nThis is the total marginal variability in all directions, but does not account for correlation.\nIn the ellipsoid analogy, it’s the sum of the squared axis lengths (not the “volume”).\n\n\n\n3.6.4 Example: iris Dataset\nThe iris dataset is a classic and widely used dataset in R, commonly employed for statistical analysis, machine learning, and data visualization. It is built into R and can be loaded directly. Let’s compute the overall measures of the dataset.\n\n# Use iris measurements\nX &lt;- iris[, 1:4]\nhead(X)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1          5.1         3.5          1.4         0.2\n2          4.9         3.0          1.4         0.2\n3          4.7         3.2          1.3         0.2\n4          4.6         3.1          1.5         0.2\n5          5.0         3.6          1.4         0.2\n6          5.4         3.9          1.7         0.4\n\n\n\n\nR Code: Covariance\n# compute covariance \nS &lt;- cov(X)\nprint(S)\n\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    0.6856935  -0.0424340    1.2743154   0.5162707\nSepal.Width    -0.0424340   0.1899794   -0.3296564  -0.1216394\nPetal.Length    1.2743154  -0.3296564    3.1162779   1.2956094\nPetal.Width     0.5162707  -0.1216394    1.2956094   0.5810063\n\n\n\n\nR Code: Spectral Decomposition\neig &lt;- eigen(S)\nlambda &lt;- eig$values\n\n\n\n\nR Code: Overall Measures\n# Generalized variance: product of eigenvalues\ngen_var &lt;- prod(lambda)\n# Generalized standard deviation: sqrt of product\ngen_sd &lt;- sqrt(prod(lambda))\n# Total variance: sum of eigenvalues\ntotal_var &lt;- sum(lambda)\nlibrary(tibble)\nresult = tibble(\n  \"Generalized variance\" = gen_var,\n  \"Generalized standard deviation\" = gen_sd,\n  \"Total variance\" = total_var\n)\nprint(t(result))\n\n\n                                     [,1]\nGeneralized variance           0.00191273\nGeneralized standard deviation 0.04373476\nTotal variance                 4.57295705"
  },
  {
    "objectID": "ch3/03-MVN.html#assessing-normality",
    "href": "ch3/03-MVN.html#assessing-normality",
    "title": "3  Multivariate Normal Distribution",
    "section": "3.7 Assessing Normality",
    "text": "3.7 Assessing Normality\n\nAssessing multivariate normality is difficult.\nIf any single variable does not have a normal distribution, then the joint distribution of p random variables cannot have a normal distribution.\nWe can check normality for each variable individually. If we reject normality for any variable then the joint distribution is not multivariate normal.\nAlso look at scatter plots of pairs of variables.\nLook for outliers.\n\n\n3.7.1 Normal Q-Q plots\n\nA quantile-quantile (Q-Q) plot can also be constructed for each of the p variables.\nIn a Q-Q plot, we plot the ordered data (sample quantiles) against the quantiles that would be expected if the sample came from a standard normal distribution.\nIf the hypothesis of normality holds, the points in the plot will fall closely along a straight line.\n\n\n\n\n\n\n\nNormal Q-Q plots\n\n\n\n\nThe slope of the line passing through the points is an estimate of the population standard deviation.\nThe intercept of the estimated line is an estimate of the population mean.\nThe sample quantiles are just the sample order statistics. For a sample x_1, x_2,...,x_n, quantiles are obtained by ordering sample observations \nx_{(1)} \\leq x_{(2)} \\leq ... \\leq x_{(n)},\n where x_{(j)} is the jth smallest sample observation.\n\n\n\n\n\nR Code: Q-Q Plots\n# Normal Q-Q plots for all variables in iris[, 1:4]\npar(mfrow = c(2,2))\nfor (i in 1:4) {\n  qqnorm(iris[,i], main = colnames(iris)[i])\n  qqline(iris[,i], col = \"blue\", lwd = 2)\n}\n\n\n\n\n\nInterpretation:\n\nIf the points follow the line closely, that variable is approximately normal.\nSystematic departures from the line (curves, s-shapes, outliers) suggest non-normality.\n\n\n\nR Code: Scatterplots\n# Also look at scatterplots for pairs of variables\npairs(iris[,1:4], main = \"Scatterplots: Iris Variables\")\n\n\n\n\n\nInterpretation: - Outliers, nonlinear patterns, or heavy tails in these scatterplots suggest deviations from the multivariate normal model.\n\n\n3.7.2 Shapiro-Wilk Test\n\nThe Shapiro-Wilk test is a statistical test for normality. If the p-value is small (typically &lt;0.05), we reject the null hypothesis of normality.\nTest statistic: A weighted correlation between the x_{(j)} and the q_{(j)}: \nW = \\left( \\frac{\\sum_{i = 1}^n a_j(x_{(i)} - \\bar{x})(q_{(i)} - \\bar{q}) }{\\sqrt{\\sum_{i = 1}^n a_j^2(x_{(i)} - \\bar{x})^2}\\sqrt{\\sum_{i = 1}^n (q_{(i)} - \\bar{q})^2}}  \\right)^2.\n\nWe expect values of W to be close to one if the sample arises from a normal population.\nReject the null hypothesis that data were sampled from a normal distribution if W is too small.\nThis test has been extended to p-dimensional normal distributions.\n\n\n\n\n\n\n\nShapiro-Wilk Test\n\n\n\nThe Shapiro-Wilk test only checks marginal normality, not joint/multivariate normality when applied to each variable individually. This univariate test is implemented in the R function shapiro.test. To perform multivariate normaltiy check, we need to use the multivariate version of the Shapiro-Wilk test using the function mvShapiro.Test from the package mvShapiroTest.\n\n\n\n\nR Code: Univariate Normality Test\n# Shapiro-Wilk test for each of the four numeric iris variables\napply(iris[, 1:4], 2, shapiro.test)\n\n\n$Sepal.Length\n\n    Shapiro-Wilk normality test\n\ndata:  newX[, i]\nW = 0.97609, p-value = 0.01018\n\n\n$Sepal.Width\n\n    Shapiro-Wilk normality test\n\ndata:  newX[, i]\nW = 0.98492, p-value = 0.1012\n\n\n$Petal.Length\n\n    Shapiro-Wilk normality test\n\ndata:  newX[, i]\nW = 0.87627, p-value = 7.412e-10\n\n\n$Petal.Width\n\n    Shapiro-Wilk normality test\n\ndata:  newX[, i]\nW = 0.90183, p-value = 1.68e-08\n\n\n\n\nR Code: Multivariate Normality Test\nlibrary(mvShapiroTest)\n# multivariate Shapiro-Wilk test \nmvShapiro.Test(as.matrix(iris[, 1:4]))\n\n\n\n    Generalized Shapiro-Wilk test for Multivariate Normality by\n    Villasenor-Alva and Gonzalez-Estrada\n\ndata:  as.matrix(iris[, 1:4])\nMVW = 0.97327, p-value = 1.655e-06\n\n\n\n\n\n\n\n\nExercise: Assessing Normality with Real Data (mtcars)\n\n\n\nUsing the mtcars dataset, explore the relationship between mpg (miles per gallon) and hp (horsepower):\n\nMake a scatterplot of mpg vs hp and describe the pattern.\nOverlay bivariate normal contours (fit the mean and covariance from the data).\nDraw normal Q-Q plots for mpg and hp.\nPerform the Shapiro-Wilk test for both variables.\nInterpret your results: Does either variable appear to be normally distributed? Does the joint distribution appear approximately elliptical (as would be expected under bivariate normality)?\n\n\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nR Code: View Solution\nlibrary(mvtnorm)\nlibrary(ggplot2)\n\ndata(mtcars)\nX &lt;- mtcars[, c(\"mpg\", \"hp\")]\nmu &lt;- colMeans(X)\nSigma &lt;- cov(X)\n\n\np &lt;- ggplot(X, aes(x = mpg, y = hp)) +\n  geom_point(size = 2, alpha = 0.7) +\n  theme_minimal() +\n  labs(title = \"mtcars: mpg vs hp\")\n\nx_seq &lt;- seq(min(X$mpg) - 2, max(X$mpg) + 2, length = 100)\ny_seq &lt;- seq(min(X$hp) - 10, max(X$hp) + 10, length = 100)\ngrid &lt;- expand.grid(mpg = x_seq, hp = y_seq)\ngrid$z &lt;- dmvnorm(as.matrix(grid), mean = mu, sigma = Sigma)\n\nget_density_level &lt;- function(Sigma, p) {\n  d2 &lt;- qchisq(p, df=2)\n  detS &lt;- det(Sigma)\n  norm_const &lt;- 1/(2*pi*sqrt(detS))\n  exp_part &lt;- exp(-0.5 * d2)\n  norm_const * exp_part\n}\nlevel_50 &lt;- get_density_level(Sigma, 0.5)\nlevel_90 &lt;- get_density_level(Sigma, 0.9)\n\np + \n  geom_contour(data = grid, aes(z = z), breaks = level_50, \n               color = \"blue\", linetype = \"solid\", size = 1.2) +\n  geom_contour(data = grid, aes(z = z), breaks = level_90, \n               color = \"red\", linetype = \"dashed\", size = 1.2) +\n  labs(subtitle = \"Blue: 50% contour | Red dashed: 90% contour\")\n\n\n\n\n\nR Code: View Solution\npar(mfrow = c(1,2))\nqqnorm(X$mpg, main = \"Normal Q-Q: mpg\"); qqline(X$mpg, col = \"blue\")\nqqnorm(X$hp, main = \"Normal Q-Q: hp\"); qqline(X$hp, col = \"blue\")\n\n\n\n\n\nR Code: View Solution\npar(mfrow = c(1,1))\n\nsw1 &lt;- shapiro.test(X$mpg)\nsw2 &lt;- shapiro.test(X$hp)\ncat(\"Shapiro-Wilk p-value for mpg:\", signif(sw1$p.value, 3), \"\\n\")\n\n\nShapiro-Wilk p-value for mpg: 0.123 \n\n\nR Code: View Solution\ncat(\"Shapiro-Wilk p-value for hp:\", signif(sw2$p.value, 3), \"\\n\")\n\n\nShapiro-Wilk p-value for hp: 0.0488 \n\n\nR Code: View Solution\n# 5. multivariate Shapiro-Wilk test\nmvShapiro.Test(as.matrix(X[, c(\"mpg\", \"hp\")]))\n\n\n\n    Generalized Shapiro-Wilk test for Multivariate Normality by\n    Villasenor-Alva and Gonzalez-Estrada\n\ndata:  as.matrix(X[, c(\"mpg\", \"hp\")])\nMVW = 0.92425, p-value = 0.00639\n\n\nInterpretation:\n\nThe scatterplot shows the relationship between mpg and hp (likely negative).\nThe bivariate normal contours (fit from the data) show the estimated “ellipse” of the joint distribution.\nQ-Q plots help check whether each variable is approximately normal (look for straightness).\nShapiro-Wilk p-values: p &lt; 0.05 indicates non-normality; p &gt; 0.05 suggests no evidence against normality.\nIf either variable is non-normal or the scatterplot is strongly non-elliptical (e.g., curved, outliers), the joint distribution is not truly bivariate normal.\n\n\n\n\n\n\n3.7.3 Transformations to Near Normality\nIf observations show gross departures from normality, it might be necessary to transform some of the variables to near normality. Some recommendations are given below.\n\n\n\n\n\n\n\nOriginal scale\nTransformed scale\n\n\n\n\nRight skewed data\n\\sqrt{x}   log(x)   1/\\sqrt{x}   1/x\n\n\nx are counts\n\\sqrt{x}\n\n\nx are proportions \\hat{p}\n\\mathrm{logit}(\\hat{p}) = \\frac{1}{2} \\log\\left(\\frac{\\hat{p}}{1 - \\hat{p}}\\right)\n\n\nx are correlations r\nFisher’s z(r) = \\frac{1}{2} \\log\\left(\\frac{1 + r}{1 - r}\\right)"
  },
  {
    "objectID": "ch3/03-MVN.html#detecting-outliers",
    "href": "ch3/03-MVN.html#detecting-outliers",
    "title": "3  Multivariate Normal Distribution",
    "section": "3.8 Detecting Outliers",
    "text": "3.8 Detecting Outliers\n\nAn outlier is a measurement that appears to be much different from neighboring observations.\nIn the univariate case with adequate sample sizes, and assuming that normality holds, an outlier can be detected by:\n\nStandardizing the n measurements so that they are approximately N(0, \\ 1)\nFlagging observations with standardized values below or above 3.5 or thereabouts.\n\nIn p dimensions, detecting outliers is not so easy. A sample unit which may not appear to be an outlier in each of the marginal distributions can still be an outlier relative to the multivariate distribution.\n\n\n3.8.1 Steps for Detecting Outliers\n\nInvestigate all univariate marginal distributions by computing the standardized values z_{ji} = (x_{ji} - \\bar{x}_i) / \\sqrt{\\sigma_{ii}} for the j-th sample unit and the i-th variable.\nIf p is moderate, construct all bivariate scatter plots. There are p(p-1)/2 of them.\nFor each sample unit, calculate the squared distance d^2_j = (\\mathbf{x}_j - \\bar{\\mathbf{x}})'S^{-1}(\\mathbf{x}_j - \\bar{\\mathbf{x}}), where \\mathbf{x}_j is the p \\times 1 vector of measurements on the j-th sample unit.\nTo decide if d^2_j is extreme, recall that the d^2_j are approximately \\chi^2_p. For example, if n = 100, we would expect to observe about 5 squared distances larger than the 0.95 percentile of the \\chi^2_p distribution.\n\n\n\n3.8.2 Example: iris Data\n\n\nVisualizing Data Using Scatterplot and Marginal Dot Plots\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndf = iris\n# Bin along x\nx_proj &lt;- df %&gt;%\n  dplyr::mutate(y = 2)\n\n# Bin along y\ny_proj &lt;- df %&gt;%\n  mutate(x = 4)\n\nggplot(df, \n  aes(x = Sepal.Length, \n      y = Sepal.Width, color = Species)) +\n  geom_point(size = 1, alpha = 0.7) +\n  geom_dotplot(data = x_proj, \n               aes(x = Sepal.Length, y=2, fill = Species),\n               binaxis = \"x\", stackdir = \"down\", dotsize = 0.5,\n               position = position_nudge(y=1.5),\n               inherit.aes = FALSE) +\n  geom_dotplot(data = y_proj, \n               aes(x=3.5, y = Sepal.Width, fill = Species),\n               binaxis = \"y\", stackdir = \"down\", dotsize = 0.5,\n               position = position_nudge(x=0),\n               inherit.aes = FALSE) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\n\n\nDetecting Outliers Using Mahalanobis Distances\n## Finding mahalanobis distance\nxbar = colMeans(df[, 1:4])\nbvar = cov(df[,1:4])\ndsq = mahalanobis(x=df[,1:4], center=xbar, cov=bvar)\n\n## Cutoff value for distances from a chisquare distribution\ncutoff = qchisq(p=0.95, df=nrow(bvar)) # 95th percentile \n\n## plot observations whose distance is greater than cutoff value\nflag = dsq&gt;cutoff\ncbind(df[flag,1:4], d_sqaure=dsq[flag])\n\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width  d_sqaure\n16           5.7         4.4          1.5         0.4  9.712790\n42           4.5         2.3          1.3         0.3 11.424029\n107          4.9         2.5          4.5         1.7 10.137804\n115          5.8         2.8          5.1         2.4 11.410573\n118          7.7         3.8          6.7         2.2 12.813073\n132          7.9         3.8          6.4         2.0 13.101093\n135          6.1         2.6          5.6         1.4 12.880331\n136          7.7         3.0          6.1         2.3  9.656936\n142          6.9         3.1          5.1         2.3 12.441384\n\n\n\n\nScatterplot and Dot Plot\n# Add flag for outliers\ndf = df %&gt;%\n  mutate(flag = dsq &gt; cutoff)\n\n# Projections\nx_proj = df %&gt;% mutate(y = 2)\ny_proj = df %&gt;% mutate(x = 4)\n\nggplot(df, aes(x = Sepal.Length, y = Sepal.Width)) +\n  # Scatter plot with flagged points highlighted\n  geom_point(aes(color = Species,\n                 shape = flag,    # Different shape for flagged\n                 size = ifelse(flag, 3, 1.5)), # Larger for flagged\n             alpha = 0.7) +\n  # X-axis projection\n  geom_dotplot(\n    data = x_proj,\n    aes(x = Sepal.Length, y = 2, fill = Species),\n    binaxis = \"x\", stackdir = \"down\", binwidth = 0.15, \n    dotsize = 0.5,\n    position = position_nudge(y = 1.5),\n    inherit.aes = FALSE\n  ) +\n  # Y-axis projection\n  geom_dotplot(\n    data = y_proj,\n    aes(x = 3.5, y = Sepal.Width, fill = Species),\n    binaxis = \"y\", stackdir = \"down\", binwidth = 0.15, \n    dotsize = 0.5,\n    position = position_nudge(x = 0),\n    inherit.aes = FALSE\n  ) +\n  scale_shape_manual(\n    values = c(\n      `FALSE` = 16, \n      `TRUE` = 21)) + # open circle for outliers\n  scale_size_identity() +\n  coord_equal() +\n  theme_minimal()"
  },
  {
    "objectID": "ch3/03-MVN.html#exercises",
    "href": "ch3/03-MVN.html#exercises",
    "title": "3  Multivariate Normal Distribution",
    "section": "3.9 Exercises",
    "text": "3.9 Exercises\n\n3.9.1 Exercise 1: Perspiration Data\nThese data were taken from an example in the Johnson and Wichern book, Applied Multivariate Analysis in the file sweat.dat. Three measurements on perspiration were taken from each woman in a random sample of 20 healthy women: X_1= sweat rate, X_2= sodium concentration, X_3=potassium concentration.\n\nRead the data into R.\nCompute the sample mean and sample covariance\nTreat sample mean and sample covariance as the estimate of population parameters and evaluate the MVN pdf for a range of possible values of variables X_1, X_2.\nCompute the mean and variance of 0.5X_1+ 0.5 X_2 and X_1-X_2.\nPlot the bivariate normal density contour for X_1, X_2 using sample mean and sample covariance.\nPlot a scatterplot with points outside the 95% highlighted.\nCompute Ellipse axes via spectral decomposition.\nCompute generalized variance and total variance based on the whole dataset.\n\nPerform normality check for all the variables.\nDetect any possible outliers at level \\alpha =0.05 for all the variables.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\ndat = read.table(\"sweat.dat\", header=F, \n                 col.names=c(\"subject\", \"x1\", \"x2\", \"x3\") )\nhead(dat)\n\n\n  subject  x1   x2   x3\n1       1 3.7 48.5  9.3\n2       2 5.7 65.1  8.0\n3       3 3.8 47.2 10.9\n4       4 3.2 53.2 12.0\n5       5 3.1 55.5  9.7\n6       6 4.6 36.1  7.9\n\n\nCode\nX = as.matrix(dat[,c(2,3)])\n\n\n\n\nCode\nxbar = colMeans(X)\nS = cov(X)\nS\n\n\n          x1       x2\nx1  2.879368  10.0100\nx2 10.010000 199.7884\n\n\n\n\nCode\ngrid &lt;- expand.grid(\n  x = seq(min(X[,1]) - 0.5, max(X[,1]) + 0.5, length = 50),\n  y = seq(min(X[,2]) - 0.5, max(X[,2]) + 0.5, length = 50)\n)\ngridmat = as.matrix(grid)\ndens &lt;- mvtnorm::dmvnorm(x=gridmat, \n                         mean = xbar, \n                         sigma = S)\nhead(data.frame(grid, f = dens))\n\n\n         x  y            f\n1 1.000000 13 0.0002244281\n2 1.163265 13 0.0002563175\n3 1.326531 13 0.0002894749\n4 1.489796 13 0.0003232772\n5 1.653061 13 0.0003570021\n6 1.816327 13 0.0003898505\n\n\n\n\nCode\n# mean, var\na1 = c(0.5, 0.5)\na2 = c(1, -1)\n\n# mean and variance of 0.5*X1 + 0.5*X2\ntibble(\n  mean = sum(a1 * xbar),\n  variance = drop(t(a1) %*% solve(S) %*% a1)\n)\n\n\n# A tibble: 1 × 2\n   mean variance\n  &lt;dbl&gt;    &lt;dbl&gt;\n1  25.0   0.0961\n\n\nCode\n# mean and variance of X1 - X2\ntibble(\n  mean = sum(a2 * xbar),\n  variance = drop(t(a2) %*% solve(S) %*% a2)\n)\n\n\n# A tibble: 1 × 2\n   mean variance\n  &lt;dbl&gt;    &lt;dbl&gt;\n1 -40.8    0.469\n\n\n\n\nCode\ngrid$z = dens \nggplot(grid, aes(x = x, y = y, z = z)) +\n  geom_contour_filled(bins = 10) +\n  #coord_equal() +\n  labs(title = \"Bivariate Normal Density (Contours)\",\n       x = \"Sweat rate\", y = \"Sodium concentration\", \n       fill = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\nCode\nXc = X - matrix(1, nrow=nrow(X), ncol=1) %*% t(xbar)\nQ = diag( Xc %*% solve(S) %*% t(Xc) )\n# or use T2 = rowSums( (Xc %*% solve(S)) * Xc)\ncut = qchisq(.95, df=2)\nmean(Q&lt;=cut) # empirical coverage \n\n\n[1] 0.95\n\n\nCode\ndat1 = as.data.frame(X) %&gt;% \n  mutate(inside = Q&lt;=cut)\nggplot(dat1, aes(x=x1, y=x2, color=inside)) + \n  geom_point(size=2, alpha=.8) + \n  scale_color_manual(values = c(\"FALSE\"=\"grey60\",\n                                \"TRUE\"=\"tomato\")) + \n  labs(color = \"Inside 95% ellipse?\")\n\n\n\n\n\n\n\nCode\nE = eigen(S)\n\n# Ellipse directions\nE$vectors\n\n\n          [,1]       [,2]\n[1,] 0.0506399 -0.9987170\n[2,] 0.9987170  0.0506399\n\n\nCode\n# lengths\nsqrt(E$values) * sqrt(cut)\n\n\n[1] 34.641972  3.769698\n\n\n\n\nCode\ntibble(\n  \"Generalized variance\" = prod(E$values),\n  \"Total variance\" = sum(E$values)\n)\n\n\n# A tibble: 1 × 2\n  `Generalized variance` `Total variance`\n                   &lt;dbl&gt;            &lt;dbl&gt;\n1                   475.             203.\n\n\n\n\nCode\npar(mfrow = c(1,3))\nfor (nm in colnames(dat[,2:4])) {\n  qqnorm(dat[, nm], main = paste(\"QQ:\", nm))\n  qqline(dat[, nm])\n}\n\n\n\n\n\nCode\npar(mfrow = c(1,1))\n\napply(dat[, 2:4], 2, function(v) shapiro.test(v)$p.value)\n\n\n       x1        x2        x3 \n0.8689242 0.9861883 0.6232620 \n\n\nCode\nmvShapiroTest::mvShapiro.Test(as.matrix(dat[,2:4]))\n\n\n\n    Generalized Shapiro-Wilk test for Multivariate Normality by\n    Villasenor-Alva and Gonzalez-Estrada\n\ndata:  as.matrix(dat[, 2:4])\nMVW = 0.94446, p-value = 0.2567\n\n\n\n\nCode\ndat1 = as.matrix(dat[,2:4])\nxbar1 = colMeans(dat1)\nS1 = cov(dat1)\nXc1 = dat1 - matrix(1,nrow(dat1), 1) %*% t(xbar1)\nd2 = diag( (Xc1) %*% solve(S1) %*% t(Xc1) )\n\ncut2 = qchisq(.975, df=ncol(dat1))\nsum(d2&gt;cut2)\n\n\n[1] 0"
  },
  {
    "objectID": "ch2/02-graphs.html",
    "href": "ch2/02-graphs.html",
    "title": "2  Graphs and Data Visualization",
    "section": "",
    "text": "2.1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphs and Data Visualization</span>"
    ]
  },
  {
    "objectID": "ch1/01-intro.html",
    "href": "ch1/01-intro.html",
    "title": "1  Introduction to Multivariate Data",
    "section": "",
    "text": "1.1 Course Outline",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Multivariate Data</span>"
    ]
  },
  {
    "objectID": "ch4/04-Inference_for_Means.html#motivation",
    "href": "ch4/04-Inference_for_Means.html#motivation",
    "title": "4  Inference for Single Mean Vector",
    "section": "4.1 Motivation",
    "text": "4.1 Motivation\nLet’s look at the famous iris dataset. We’ll focus on the setosa species and two variables: Sepal Length and Sepal Width.\nA biologist might ask: “Is the mean vector of sepal measurements for setosa flowers equal to a specific standard, say \\boldsymbol{\\mu}_0 = [5.1, 3.6]'?”\n\n\nR Code: Iris Data\nlibrary(ggplot2)\nlibrary(DescTools) # For one-sample T2 test\nlibrary(ellipse)   # For plotting confidence ellipses\n\n# Isolate the setosa data for the first two variables\nsetosa_data = iris[iris$Species == \"setosa\", 1:2]\nnames(setosa_data) = c(\"Sepal.Length\", \"Sepal.Width\")\nhead(setosa_data)\n\n\n\n\n  \n\n\n\n\n\nData Visualization: iris Data\n# The hypothesized mean vector\nmu_0 = c(5.1, 3.6)\n\n# Plot the data\nggplot(setosa_data, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point(color = \"purple\", alpha = 0.7) +\n  geom_vline(xintercept = mu_0[1],\n             linetype = \"dashed\",\n             color = \"red\") +\n  geom_hline(yintercept = mu_0[2],\n             linetype = \"dashed\",\n             color = \"red\") +\n  annotate(\n    \"point\",\n    x = mu_0[1],\n    y = mu_0[2],\n    color = \"red\",\n    size = 5,\n    shape = 4,\n    stroke = 1.5\n  ) +\n  labs(\n    title = \"Sepal Measurements for Iris Setosa\",\n    subtitle = \"Red crosshairs show the hypothesized mean vector [5.1, 3.6]\",\n    x = \"Sepal Length (cm)\",\n    y = \"Sepal Width (cm)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n4.1.1 Why Not Just Run Two t-tests?\nThe “naive” approach is to test each variable separately.\n\nTest H_0: \\mu_{\\text{Length}} = 5.1 \\quad \\text{v.s.} \\quad H_1: \\mu_{\\text{Length}} \\neq 5.1\nTest H_0: \\mu_{\\text{Width}} = 3.6 \\quad \\text{v.s.} \\quad H_1: \\mu_{\\text{Width}} \\neq 3.6\n\n\n\n\n\n\n\nR Code: One-Sample t-Test\n\n\n\n\n\n\n\nR Code: One-Sample t-Test\n# Test 1: Sepal Length\nt.test(setosa_data$Sepal.Length, mu = mu_0[1])\n\n\n\n    One Sample t-test\n\ndata:  setosa_data$Sepal.Length\nt = -1.8857, df = 49, p-value = 0.06527\nalternative hypothesis: true mean is not equal to 5.1\n95 percent confidence interval:\n 4.905824 5.106176\nsample estimates:\nmean of x \n    5.006 \n\n\nR Code: One-Sample t-Test\n# Test 2: Sepal Width\nt.test(setosa_data$Sepal.Width, mu = mu_0[2])\n\n\n\n    One Sample t-test\n\ndata:  setosa_data$Sepal.Width\nt = -3.2085, df = 49, p-value = 0.002354\nalternative hypothesis: true mean is not equal to 3.6\n95 percent confidence interval:\n 3.320271 3.535729\nsample estimates:\nmean of x \n    3.428 \n\n\n\n\n\nIndividually, we failed to reject the first null hypothesis and would reject the second null hypothes at \\alpha = 0.05. But this is misleading!\n\n\n\n\n\n\nWhy do we need a multivariate test?\n\n\n\n\n\n\nThe univariate approach doesn’t answer the right question. The research question is about the mean vector \\boldsymbol{\\mu}, not the individual means. The two t-tests do not provide a single measure of evidence (one p-value) against the single hypothesis H_0: \\boldsymbol{\\mu} = \\boldsymbol{\\mu}_0.\nIt ignores the data’s structure. Sepal length and petal length are correlated. Taller flowers tend to have longer petals. The separate t-tests treat the variables as independent, which they are not. This ignores crucial information about the joint variability (or covariance) of the data.\nIt cannot quantify the joint uncertainty. The true “distance” of the sample mean vector from the hypothesized mean vector can only be properly measured by accounting for the covariance between variables. We need a method that can create a single confidence region (an ellipse) for the mean vector, which is impossible with two separate confidence intervals.\n\nThe multivariate approach provides a single, unified test for a single, unified hypothesis, while properly accounting for the relationships between the variables.\n\n\n\n\n\n4.1.2 The Family-Wise Error Rate Problem\nIf we conduct p tests, each at a significance level \\alpha, the probability of making at least one Type I error (a false positive) skyrockets.\n\n\\text{Pr}(\\text{rejecting at least one} \\, H_0: \\mu_j = \\mu_{0j} \\, | \\, \\text{all}\\, H_0\\text{'s are true})= 1 - (1 - \\alpha)^p \n\nFor our p=2 case with \\alpha=0.05, the true error rate is 1 - (1-0.05)^2 = 0.0975, nearly double our intended \\alpha!\nThis approach is very conservative (i.e., tends to reject more null hypotheses than we should).\nMore importantly, this univariate approach completely ignores the correlation between variables. Multivariate methods account for this correlation structure."
  },
  {
    "objectID": "ch4/04-Inference_for_Means.html#review-of-univariate-inference",
    "href": "ch4/04-Inference_for_Means.html#review-of-univariate-inference",
    "title": "4  Inference for Single Mean Vector",
    "section": "4.2 Review of Univariate Inference",
    "text": "4.2 Review of Univariate Inference\nThe one-sample t-test answers a very common question: “Is the average of my sample significantly different from a known or claimed value?”\nBefore we go multivariate, let’s review the one-sample t-test for a mean \\mu.\n\n\n\n\n\n\nt-Test\n\n\n\n\nHypotheses: H_0: \\mu = \\mu_0 H_1: \\mu \\neq \\mu_0\nTest Statistic:  t = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} \\sim t_{n-1} \nConfidence Interval: A 100(1-\\alpha)\\% CI for \\mu is:  \\bar{x} \\pm t_{n-1, 1-\\alpha/2} \\frac{s}{\\sqrt{n}} \n\n\n\n\n\n\n\n\n\nDecision approach\n\n\n\n\n\nTo make the decision, we could either use the rejection region approach or the p-value approach.\n\nRejection Region\n\nIf |t|&gt; |t_{n-1,1-\\alpha/2}|, we should reject H_0 at significance level \\alpha; otherwise, we fail to reject H_0 at significance level \\alpha and conclude there is no sufficient evidence to detect the difference.\n\nP-Value\n\nThe p-value is the probability of seeing a result as extreme as yours (or more extreme) if the claim (in null hypothesis) were actually true.\nSmall p-value (typically &lt; 0.05): This result is very unlikely to happen by random chance alone. We reject the null hypothesis (H_0) and conclude there’s a significant difference at level \\alpha (e.g., 0.05).\nLarge p-value (typically ≥ 0.05): This result is reasonably likely to happen by random chance. The difference you saw could just be noise. We fail to reject the null hypothesis at level \\alpha (e.g., 0.05). There is no sufficient evidence from the data to indicate the null hypothesis is wrong.\n\n\n\n\n\n\n\n\n\n\n\nExample: Coffee Roaster\n\n\n\n\n\nLet’s test the coffee roaster’s claim. We buy 10 bags and weigh them with data given by . We want to test if the true mean weight is different from the claimed 12 ounces.\n\nNull hypothesis: H_0: ______\nAlternative hypothesis H_1: ______\n\n\n\nR Code: One-Sample t Test\n# Our sample data: weights of 10 coffee bags in ounces\ncoffee_weights &lt;- c(11.8, 12.1, 11.4, 11.7, \n                    12.0, 11.6, 11.8, 12.2, \n                    11.5, 11.8)\n\n# The hypothesized mean from the null hypothesis\nmu0 &lt;- 12\n\n# Perform the one-sample t-test\ntest_result &lt;- t.test(x = coffee_weights, mu = mu0)\n\n# Print the results\nprint(test_result)\n\n\n\n    One Sample t-test\n\ndata:  coffee_weights\nt = -2.5959, df = 9, p-value = 0.02893\nalternative hypothesis: true mean is not equal to 12\n95 percent confidence interval:\n 11.607 11.973\nsample estimates:\nmean of x \n    11.79 \n\n\nInterpreting the output:\n\nt=-2.596: The sample mean is 2.596 standard errors below the hypothesized mean. That’s a moderately strong signal.\nConclusion: Since the p-value = 0.029 is less than the significant level \\alpha=0.05, we reject the null hypothesis. We conclude that the true mean weight of the coffee bags is significantly different from 12 ounces at level \\alpha=0.05.\n95 percent confidence interval: [11.61, 11.97]: We are 95% confident that the true mean weight of all bags from this roaster is between 11.61 and 11.97 ounces. Notice that this interval does not contain 12, which confirms our decision to reject the null hypothesis."
  },
  {
    "objectID": "ch4/04-Inference_for_Means.html#generalizing-to-the-multivariate-case",
    "href": "ch4/04-Inference_for_Means.html#generalizing-to-the-multivariate-case",
    "title": "4  Inference for Single Mean Vector",
    "section": "4.3 Generalizing to the Multivariate Case",
    "text": "4.3 Generalizing to the Multivariate Case\nWe upgrade our tools from scalars to vectors and matrices.\n\nThe sample mean vector \\bar{\\mathbf{x}} is a vector of the individual sample means.  \\bar{\\mathbf{x}} = \\begin{bmatrix} \\bar{x}_1 \\\\ \\vdots \\\\ \\bar{x}_p \\end{bmatrix} \nThe sample covariance matrix \\mathbf{S} contains the sample variances on its diagonal and the sample covariances on its off-diagonals.  \\mathbf{S} = \\frac{1}{n-1} \\sum_{j=1}^{n} (\\mathbf{x}_j - \\bar{\\mathbf{x}})(\\mathbf{x}_j - \\bar{\\mathbf{x}})^\\top\n  \n\n\n\nR Code: Sample Mean and Sample Covariance\nn = nrow(setosa_data)\np = ncol(setosa_data)\n\nx_bar = colMeans(setosa_data)\ncat(\"Sample Mean Vector (x_bar):\", x_bar)\n\n\nSample Mean Vector (x_bar): 5.006 3.428\n\n\nR Code: Sample Mean and Sample Covariance\nS &lt;- cov(setosa_data)\ncat(\"Sample Covariance Matrix (S):\\n\")\n\n\nSample Covariance Matrix (S):\n\n\nR Code: Sample Mean and Sample Covariance\nS\n\n\n             Sepal.Length Sepal.Width\nSepal.Length   0.12424898  0.09921633\nSepal.Width    0.09921633  0.14368980"
  },
  {
    "objectID": "ch4/04-Inference_for_Means.html#hotellings-t2-test",
    "href": "ch4/04-Inference_for_Means.html#hotellings-t2-test",
    "title": "4  Inference for Single Mean Vector",
    "section": "4.4 Hotelling’s T^2 Test",
    "text": "4.4 Hotelling’s T^2 Test\nThis is the multivariate workhorse, analogous to the squared t-statistic.\n\n4.4.1 Hotelling’s T^2 Statistic\nHotelling’s T^2 statistic measures the “distance” between the sample mean vector \\bar{\\mathbf{x}} and the hypothesized mean vector \\boldsymbol{\\mu}_0, accounting for sample size and covariance.\n T^2 = n(\\bar{\\mathbf{x}} - \\boldsymbol{\\mu}_0)^\\top \\mathbf{S}^{-1} (\\bar{\\mathbf{x}} - \\boldsymbol{\\mu}_0) \n\n\n4.4.2 The F-Distribution Connection\nThe T^2 statistic is not directly used. Instead, it is rescaled to a familiar F-statistic. Under H_0: \\boldsymbol{\\mu} = \\boldsymbol{\\mu}_0,\n \\frac{n-p}{p(n-1)} T^2 \\, \\overset{H_0}\\sim\\, F_{p, n-p} \nWe reject H_0 if our calculated F-statistic (or scaled T^2 statistic) is greater than the critical value F_{p, n-p, 1-\\alpha} at significance level \\alpha.\n\n\n4.4.3 Assumptions\n\nThe observations \\mathbf{x}_1, \\dots, \\mathbf{x}_n are independently drawn from the same population distribution.\nThe population distribution is multivariate normal (MVN): \\mathbf{x}_i \\overset{iid}{\\sim} N_p(\\boldsymbol{\\mu}, {\\Sigma}), i=1,\\ldots, n.\n\n\n\n4.4.4 Example: Sweat Data\nA study measured sweat rate, sodium content, and potassium content for 20 healthy females. Let’s test if the mean vector is \\boldsymbol{\\mu}_0 = [4, 50, 10]^\\top.\n\n# sweat data\nsweat = data.frame(\n    Rate = c(3.7, 5.7, 3.8, 3.2, 3.1, 4.6, 6.2, 3.4, 4.1, 5.5, 6.5,  \n             4.5, 3.9, 4.5, 6.2, 4.1, 5.5, 6.0, 5.2, 4.8),\n    Sodium = c(48.5, 65.1, 47.2, 31.1, 59.8, 37.8, 52.8, 43.2, 45.1,\n               50.3, 58.3, 40.2, 38.9, 48.8, 60.1, 44.5, 55.5, 59.9, \n               57.7, 51.0),\n    Potassium = c(9.3, 11.1, 9.6, 9.8, 8.0, 10.9, 12.2, 8.5, 9.2, \n                  10.4, 11.2, 9.7, 8.8, 10.1, 12.3, 9.5, 11.3, \n                  12.0, 10.8, 10.5)\n)\nmu0_sweat = c(4, 50, 10)\nnames(sweat) = c(\"x1\", \"x2\", \"x3\")\n\n\n\n\n\n\n\nStep 1: Normality Check\n\n\n\n\n\n\n\nCode\n## Q-Q plot\npar(mfrow = c(1, 3))\n\n# Loop through the first 3 column names \nfor (col_name in colnames(sweat)[1:3]) {\n  qqnorm(sweat[[col_name]], main = col_name)\n  qqline(sweat[[col_name]], col = \"red\", lwd = 2)\n}\n\n\n\n\n\nCode\n## Test normality for each variable\nsapply(colnames(sweat[ ,1:3]), function(x) {\n               shapiro.test(sweat[[x]]) } )\n\n\n          x1                            x2                           \nstatistic 0.9483909                     0.9757974                    \np.value   0.3433242                     0.8692117                    \nmethod    \"Shapiro-Wilk normality test\" \"Shapiro-Wilk normality test\"\ndata.name \"sweat[[x]]\"                  \"sweat[[x]]\"                 \n          x3                           \nstatistic 0.9759253                    \np.value   0.8714673                    \nmethod    \"Shapiro-Wilk normality test\"\ndata.name \"sweat[[x]]\"                 \n\n\nCode\n## Perform multivariate normality test\nlibrary(mvShapiroTest)\nmvShapiro.Test(as.matrix(sweat[ , 1:3]))\n\n\n\n    Generalized Shapiro-Wilk test for Multivariate Normality by\n    Villasenor-Alva and Gonzalez-Estrada\n\ndata:  as.matrix(sweat[, 1:3])\nMVW = 0.96404, p-value = 0.7893\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Calculate Sample Statistics\n\n\n\n\n\n\n\nCode\nn = nrow(sweat)\np = ncol(sweat)\nxbar = colMeans(sweat)\nS = cov(sweat)\nS_inv = solve(S)\n\n\n\n\n\n\n\n\n\n\n\nStep 3: Compute Hotelling’s T^2 Statistic\n\n\n\n\n\n\n\nCode\n# Explicit calculation \ndiff_vec &lt;- xbar - mu0_sweat\nT2_manual &lt;- n * t(diff_vec) %*% S_inv %*% diff_vec\ncat(\"Explicitly calculated T-squared:\", as.numeric(T2_manual), \"\\n\")\n\n\nExplicitly calculated T-squared: 43.50518 \n\n\nCode\n# Convert to F-statistic or scaled T-square statistic \nF_stat &lt;- as.numeric(T2_manual) * (n - p) / \n  (p * (n - 1))\ncat(\"Calculated F-statistic:\", F_stat, \"\\n\")\n\n\nCalculated F-statistic: 12.97523 \n\n\nCode\n# Find critical value\nalpha &lt;- 0.05\nF_crit &lt;- qf(1 - alpha, p, n - p)\ncat(\"Critical F-value:\", F_crit, \"\\n\")\n\n\nCritical F-value: 3.196777 \n\n\nCode\ncat(\"Decision:\", \n    ifelse(F_stat &gt; F_crit, \"Reject H0\", \"Do not reject H0\"), \"\\n\")\n\n\nDecision: Reject H0 \n\n\nConclusion: Since our F-statistic (12.98) is much larger than the critical F-value (3.197), we reject the null hypothesis at level \\alpha=0.05. The mean sweat composition is significantly different from [4, 50, 10]^\\top.\n\n\n\n\n\n\n\n\n\nHotelling’s T^2 Test\n\n\n\nThe explicit calculations in Steps 2-3 above can be completed using R package DescTools after normality check. If the data follows a multivariate normal distribution, then we can perform the Hotelling’s T^2 test; if the normality assumption is violated, the conclusion is misleading and this limitation should be acknowledged.\n\n\nR Code: Hotelling’s T2 test\n# install.packages(\"DescTools\")\nlibrary(DescTools)\nht_sweat &lt;- HotellingsT2Test(x=sweat, mu = mu0_sweat)\nprint(ht_sweat)\n\n\n\n    Hotelling's one sample T2-test\n\ndata:  sweat\nT.2 = 12.975, df1 = 3, df2 = 17, p-value = 0.0001177\nalternative hypothesis: true location is not equal to c(4,50,10)\n\n\nNote: At this point we do not know which of the two hypothesized mean values is not supported by the data.\n\n\n\n\n\n\n\n\nR Function HotellingsT2Test\n\n\n\nWhen the function HotellingsT2Test in the R package DescTools is used to perform the test, its output reports the scaled T^2 statistic (or F statistic) named T.2 (which is 12.975 in this example). This scaled T^2 statistic should not be confused with the T^2 statistic as the scaled T^2 statistic is \\frac{n-p}{p(n-1)}T^2.\n\n\n\n\n4.4.5 Exercise: Iris data\nBackground\nRonald Fisher’s iris dataset is a cornerstone of statistical analysis. Imagine a historical botanical guide from the 1930s describes the “type specimen” for the Iris setosa species as having a mean Sepal Length of 5.1 cm and a mean Sepal Width of 3.6 cm.\nYour task is to determine if the sample of 50 Iris setosa flowers from Fisher’s dataset is consistent with this historical description. You will use a Hotelling’s T² test with a significance level of \\alpha = 0.05.\n1. State the Hypotheses\nWrite the null hypothesis (H_0) and the alternative hypothesis (H_1) for this test. Let \\boldsymbol{\\mu} represent the true mean vector of [Sepal.Length, Sepal.Width].\n\n\n\n\n\n\nView Solution\n\n\n\n\n\nThe hypotheses are formulated as:\n\nNull Hypothesis (H_0): The true mean sepal measurements are equal to the historical description. H_0: \\boldsymbol{\\mu} = \\begin{bmatrix} 5.1 \\\\ 3.6 \\end{bmatrix}\nAlternative Hypothesis (H_1): The true mean sepal measurements are not equal to the historical description. H_1: \\boldsymbol{\\mu} \\neq \\begin{bmatrix} 5.1 \\\\ 3.6 \\end{bmatrix}\n\n\n\n\n2. Prepare and Visualize the Data\nLoad the necessary R packages. From the built-in iris dataset, create a final data frame containing only the Sepal.Length and Sepal.Width for the setosa species.\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nR Code: Data Preparation\n# Load the packages needed for the entire analysis\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Create the final data frame for analysis\nsetosa_data &lt;- iris %&gt;%\n  filter(Species == \"setosa\") %&gt;%\n  dplyr::select(Sepal.Length, Sepal.Width)\n\ndf = setosa_data\nn = nrow(df)\np = ncol(df)\n# Display the first few rows of the prepared data\nhead(df)\n\n\n\n\n  \n\n\n\nR Code: Data Preparation\npairs(df)\n\n\n\n\n\n\n\n\n3. Perform the Statistical Test\nRun a one-sample Hotelling’s T^2 test on your prepared data using the historical description as your hypothesized mean vector.\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nR Code: Hotelling’s T-squared Test\n# Define the hypothesized mean vector\nmu0 = c(5.1, 3.6)\n\n# Perform the test\ntest_result = DescTools::HotellingsT2Test(\n  x = df, \n  mu = mu0)\n\n# Print the results\nprint(test_result)\n\n\n\n    Hotelling's one sample T2-test\n\ndata:  df\nT.2 = 5.3116, df1 = 2, df2 = 48, p-value = 0.008244\nalternative hypothesis: true location is not equal to c(5.1,3.6)\n\n\n\n\n\n4. Interpret the Results\nWhat is the p-value from your test? Based on this and \\alpha = 0.05, do you reject or fail to reject the null hypothesis? State your conclusion in the context of the problem.\n\n\n\n\n\n\nView Solution\n\n\n\n\n\nThe p-value is 0.008244. Since this p-value is less than our significance level of \\alpha = 0.05, we should reject the null hypothesis at level 0.05.\nConclusion: There is enough statistical evidence to conclude that the mean sepal measurements of the setosa flowers in Fisher’s dataset are different from the historical description. The benchmark of 5.1 cm length and 3.6 cm width is not a plausible value for the true mean of this sample.\n\n\n\n5. Visualize the Conclusion\nCreate a scatter plot of Sepal.Width v.s. Sepal.Length. On the plot, mark the sample mean, the hypothesized mean, and add the 95% confidence ellipse. Explain how it supports your conclusion.\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nR Code: Visualization\n# Calculate the sample mean and sample covariance \nx_bar = colMeans(df)\nS = cov(df)\n\n# Calculate points for the 95% confidence ellipse\nalpha = 0.05\nFvalue = sqrt(p * (n - 1) / (n - p) * qf(1 - alpha, p, n - p))\n\nconfidence_ellipse &lt;- as.data.frame(ellipse::ellipse(\n  S,\n  centre = x_bar,\n  level = 1-alpha,\n  t = Fvalue / sqrt(n)\n))\n\n# Create the plot\nggplot(df, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point(color = \"purple\",\n             alpha = 0.6,\n             size = 2.5) +\n  geom_path(\n    data = confidence_ellipse,\n    aes(x = Sepal.Length, y = Sepal.Width),\n    color = \"blue\",\n    linewidth = 1,\n    inherit.aes = FALSE\n  ) +\n  annotate(\"point\", \n    x = x_bar[1], y = x_bar[2],\n    color = \"blue\", size = 5\n  ) +\n  annotate(\"text\",\n    x = x_bar[1], y = x_bar[2] + 0.03,\n    label = \"Sample Mean\", color = \"blue\"\n  ) +\n  annotate(\"point\",\n    x = mu0[1], y = mu0[2],\n    color = \"red\", size = 5,\n    shape = 4, stroke = 1.5\n  ) +\n  annotate(\"text\",\n    x = mu0[1], y = mu0[2] - 0.03,\n    label = \"Historical Description\",\n    color = \"red\"\n  ) +\n  labs(\n    title = \"Iris Setosa Sepal Measurements\",\n    subtitle = \"Hotelling's T² Test vs. Historical Description\",\n    x = \"Sepal Length (cm)\",\n    y = \"Sepal Width (cm)\"\n  ) +\n  theme_bw()\n\n\n\n\n\nComparison of the Iris setosa sample to a historical description.\n\n\n\n\nVisual Interpretation: The plot confirms the test result. The red cross (the historical description) lies outside the blue 95% confidence ellipse. Since the hypothesized value is outside the confidence region of plausible values for the true mean, it visually supports our conclusion to reject the null hypothesis."
  },
  {
    "objectID": "ch4/04-Inference_for_Means.html#confidence-regions-for-boldsymbolmu",
    "href": "ch4/04-Inference_for_Means.html#confidence-regions-for-boldsymbolmu",
    "title": "4  Inference for Single Mean Vector",
    "section": "4.5 Confidence Regions for \\boldsymbol{\\mu}",
    "text": "4.5 Confidence Regions for \\boldsymbol{\\mu}\nA confidence interval for a single mean becomes a confidence region (an ellipse for p=2, an ellipsoid for p&gt;2) for a mean vector. This confidence region is also known as joint confidence.\nA 100(1-\\alpha)\\% confidence region for \\boldsymbol{\\mu} is the set of all vectors \\boldsymbol \\mu satisfying:  n(\\bar{\\mathbf{x}} - \\boldsymbol{\\mu})^\\top {S}^{-1} (\\bar{\\mathbf{x}} - \\boldsymbol{\\mu}) \\le \\frac{p(n-1)}{n-p} F_{p, n-p, 1-\\alpha} \n\n\n\n\n\n\nConfidence Region\n\n\n\nThe confidence region forms an ellipsoid. Its shape and orientation are determined by the eigenvalues and eigenvectors of the sample covariance matrix {S}.\n\nThe center of the ellipsoid is \\bar{\\mathbf{x}}.\nThe distance from the center of the ellipsoid to the edge of the ellipsoid along the i-th axis is \n\\pm \\sqrt{\\lambda_i} \\sqrt{\\frac{(n-1)p}{n(n-p)}F_{(p, n-p), 1-\\alpha}}.\n\nThe directions of the eigenvectors and sizes of the eigenvalues depend on\n\nthe relative sizes of the variances of the measured variables\nthe sizes of the correlations between pairs of variables\n\n\n\n\n\n4.5.1 Plot Confidence Ellipse (p=2) via the ellipse Package\nLet’s plot the 95% confidence ellipse for our setosa variable\n\n\nR Code: Plot an Ellipse\n# Plot the points\nplot(df,\n     xlab = \"Sepal Length\",\n     ylab = \"Sepal Width\",\n     main = \"95% Confidence Ellipse for Setosa Mean\",\n     pch = 19, col = alpha(\"purple\", 0.5),\n     xlim=c(4.3, 5.8), \n     ylim=c(2, 4.5)\n)\n\n# Add the sample mean\npoints(x_bar[1], x_bar[2], pch = 19, col = \"blue\", cex = 1.5)\n\n# Add the hypothesized mean\npoints(mu0[1], mu0[2], pch = 4, col = \"red\", cex = 1.5, lwd=2)\n\n# Add the ellipse\nalpha = 0.05\nFvalue = sqrt(p * (n - 1) / (n - p) * \n                qf(1 - alpha, p, n - p) \n              )\n\n\nlines(ellipse::ellipse(S, \n              centre = x_bar, \n              level = 1-alpha, \n              t=Fvalue/sqrt(n)\n              ), \n      col = \"blue\", lwd = 2)\n\n\nlegend(\"topleft\",\n       legend = c(\"Data\", \"Sample Mean\", \n                  \"Hypothesized Mean\", \n                  \"95% Confidence Ellipse\"), \n       cex=0.8,\n       col = c(\"purple\", \"blue\", \"red\", \"blue\"),\n       pch = c(19, 19, 4, NA),\n       lty = c(NA, NA, NA, 1),\n       lwd = c(NA, NA, 2, 2)\n       )\n\n\n\n\n\n95% confidence ellipse for the true mean vector of Setosa sepal measurements. The sample mean is the blue dot, and the hypothesized mean is the red cross.\n\n\n\n\nOur hypothesized mean (red cross) falls outside the 95% confidence ellipse, which is consistent with rejecting the null hypothesis in the Hotelling’s T^2 test."
  },
  {
    "objectID": "ch4/04-Inference_for_Means.html#simultaneous-confidence-intervals",
    "href": "ch4/04-Inference_for_Means.html#simultaneous-confidence-intervals",
    "title": "4  Inference for Single Mean Vector",
    "section": "4.6 Simultaneous Confidence Intervals",
    "text": "4.6 Simultaneous Confidence Intervals\nIf we reject H_0, we want to know which variables contributed to the rejection. We need CIs that hold simultaneously for all p variables with confidence level at least 1-\\alpha.\n\n4.6.1 T^2 CIs\nThe T^2 confidence intervals are derived from the Hotelling’s T^2 statistic. For each component \\mu_i:  \\bar{x}_i \\pm \\sqrt{\\frac{p(n-1)}{n-p} F_{p, n-p, 1-\\alpha}} \\sqrt{\\frac{s_{ii}}{n}}  where s_{ii} is the (i,i)-th entry of {S}.\n\n\n4.6.2 Bonferroni CIs\nAnother way to construct individual confidence intervals for each \\mu_i is to use the t-interval, but the combined set of individual t intervals result in a simulatenous confidence level that is less than than the normal level 1-\\alpha.\nTo avoid this limitation, one can use the so-called Bonferroni confidence intervals:  \\bar{x}_i \\pm t_{n-1, 1- \\alpha/(2p)} \\sqrt{\\frac{s_{ii}}{n}}  where p is the number of variables to be estimated.\n\n\n\n\n\n\nOne-At-a-Time t CI\n\n\n\nUsing the univariate approach, we can construct t-intervals for each of mean differences, and obtain the so-called one-at-a-time t intervals: \n\\bar{x}_i \\pm t_{n-1, 1- \\alpha/2} \\sqrt{\\frac{s_{ii}}{n}}\n The one-at-a-time CI does not control the family-wise error at \\alpha or the confidence level at least at the nominal level 1-\\alpha. Bonferroni CI corrects the one-at-a-time CI and produces a set of CIs that jointly have the confidence level at least 1-\\alpha.\n\n\n\n\n4.6.3 Comparison of Confidence Region, T^2 CI and Bonferroni CI\n\n\n\n\n\n95% joint confidence region (ellipse) with rectangles for T-squared intervals (red shaped area) and Bonferroni simultaneous intervals (purple shaped area).\n\n\n\n\n\n\n4.6.4 Example: Microwave Oven Data\nManufacturers of a microwave oven are concerned about radiation emission. They measure radiation with the door closed and open from n=12 ovens. Test if the mean radiation levels are \\boldsymbol{\\mu}_0 = [0.20, 0.37]'.\n\n# Generate some plausible data\nset.seed(4750)\noven_data &lt;- data.frame(\n  Closed = rnorm(12, mean = 0.25, sd = 0.08),\n  Open = rnorm(12, mean = 0.30, sd = 0.10)\n)\ndat = oven_data\nmu0 &lt;- c(0.20, 0.37)\n\n\n\n\n\n\n\nState The Hypotheses\n\n\n\n\n\nLet \\boldsymbol \\mu = (\\mu_1, \\mu_2)^\\top represents the mean radiation levels corresponding to door closed and door open conditions. The historical standard or hypothesized mean vector is \\boldsymbol \\mu_0. The hypotheses are \nH_0: \\boldsymbol \\mu  = \\boldsymbol \\mu_0 \\quad \\text{v.s.} \\quad H_1: \\boldsymbol \\mu \\neq \\boldsymbol \\mu_0\n\n\n\n\n\n\n\n\n\n\nCompute Test Statistic\n\n\n\n\n\nIf we do a manual calculation, we need to compute the Hotelling’s T^2 test\n\n\nR Code: T^2 Statistic\nxbar = colMeans(dat)\nS = cov(as.matrix(dat))\nn = nrow(dat)\np = ncol(dat)\nT2 = n*t(xbar-mu0)%*%solve(S)%*%(xbar-mu0)\nscaledT2 = (n-p)/(p*(n-1))*drop(T2)\nF_crit =  qf(0.95, p, n-p)\nif(scaledT2&gt;F_crit){\n  cat(paste0(\" scaled T2=\", signif(scaledT2,3)),\"&gt;\", \n      paste0(\"F critical value=\", signif(F_crit,3)), \n      \", thus we reject H0 at level 0.05\")\n}else{\n  cat(paste0(\" scaled T2=\", signif(scaledT2,3)),\"&lt;\", \n      paste0(\"F critical value=\", signif(F_crit,3)), \n      \", thus we fail to reject H0 at level 0.05\")\n}\n\n\n scaled T2=4.87 &gt; F critical value=4.1 , thus we reject H0 at level 0.05\n\n\n\n\nR Code: Use HotellingsT2Test\n# First, run the T2 test\nT2result &lt;- HotellingsT2Test(x=dat, mu = mu0)\nprint(T2result)\n\n\n\n    Hotelling's one sample T2-test\n\ndata:  dat\nT.2 = 4.8713, df1 = 2, df2 = 10, p-value = 0.03334\nalternative hypothesis: true location is not equal to c(0.2,0.37)\n\n\n\n\n\n\n\n\n\n\n\nSimultaneous T^2 CIs\n\n\n\n\n\n\n\nR Code: Simultaneous T2 CIs\n# The p-value is tiny, so we reject H0. Let's find out why.\n\ns_ii &lt;- diag(S)\n\n# T2-based intervals\nT2_multiplier &lt;- sqrt(p * (n - 1) / (n - p) * F_crit)\nT2_margins &lt;- T2_multiplier * sqrt(s_ii / n)\nT2_intervals &lt;- data.frame(\n  Variable = names(oven_data),\n  Lower = xbar - T2_margins,\n  Upper = xbar + T2_margins\n)\nprint(\"95% T-squared Simultaneous CIs:\")\n\n\n[1] \"95% T-squared Simultaneous CIs:\"\n\n\n\n\n       Variable     Lower     Upper\nClosed   Closed 0.1829773 0.2941655\nOpen       Open 0.1688177 0.3802530\n\n\n\n\n\n\n\n\n\n\n\nSimultaneous Bonferroni CIs\n\n\n\n\n\n\n\nR Code: Simultaneous Bonferroni CIs\n# Bonferroni-based intervals\nbonf_multiplier &lt;- qt(1 - 0.05 / (2 * p), df = n - 1)\nbonf_margins &lt;- bonf_multiplier * sqrt(s_ii / n)\nbonf_intervals &lt;- data.frame(\n  Variable = names(dat),\n  Lower = xbar - bonf_margins,\n  Upper = xbar + bonf_margins\n)\nprint(\"95% Bonferroni Simultaneous CIs:\")\n\n\n[1] \"95% Bonferroni Simultaneous CIs:\"\n\n\n\n\n       Variable     Lower     Upper\nClosed   Closed 0.1905876 0.2865551\nOpen       Open 0.1832896 0.3657812\n\n\nConclusion: Both methods produce similar intervals. Both the T2 CI and Bonferroni CI for “Closed” Oven contain its hypothesized mean 0.2. The T2 CI for “Open” oven contains its hypothesized mean 0.37 while the Bonferroni CI does not contain its hypothesized mean 0.37. Therefore, we conclude that\n\nthere is no sufficient evidence conclude that the mean radiation level for “Closed” oven is the significantly different from the specified standard using both methods at significance level 0.05;\nfor “Open” oven, there is no sufficient evidence to conclude that the mean radiation level for “Open” oven is significantly different from the specified standard using T2 CI at significance level 0.05; while the mean radiation level for “Open” Oven is significantly higher than the specified standard using Bonferroni CI at significance level 0.05.\nNote that the Bonferroni CIs are slightly narrower (more precise) in this example."
  },
  {
    "objectID": "ch4/04-Inference_for_Means.html#sec-LinearCombinations",
    "href": "ch4/04-Inference_for_Means.html#sec-LinearCombinations",
    "title": "4  Inference for Single Mean Vector",
    "section": "4.7 Inference for Linear Combinations of Means",
    "text": "4.7 Inference for Linear Combinations of Means\nIn many scientific applications, one is interested in making statistical inference for a linear combination of the mean components (also called mean contrast) based on multivariate data. In such applications, we can test hypotheses about any linear combination of the mean components, H_0: \\mathbf{c}'\\boldsymbol{\\mu} = c_0. One can still apply previous methods to perform the test. This is useful for testing things like “Is the average of all means equal to 5?” or “Is the difference between mean 1 and mean 2 equal to zero?”\n\n\n\n\n\n\nDogs Anesthetics Study (J. & W. 2007)\n\n\n\nBackground\nA study was conducted on a sample of 19 dogs to evaluate the effect of CO2 pressure and the anesthetic halothane on heart rate. Each dog was then administered carbon dioxide CO2 at each of two pressure levels. Next, halothane (H) was added and then administration of CO2 was repeated. There was a washout period (several weeks) between the use of one anesthetic and the use of another anesthetic. The response variable is the time in milliseconds between heartbeats. The data were obtained by measuring the response under four treatment combinations: (1) high CO2 pressure, (2) low CO2 pressure, (3) high pressure + halothane, and (4) low pressure + halothane.\n\n# load data\ndogdat &lt;- read.table(\"dogs.dat\",\n            col.names=c(\"dog\", \"HighCO2\", \"LowCO2\", \n                        \"HighCO2H\", \"LowCO2H\")) %&gt;%\n  mutate(across(c(\"HighCO2\", \"LowCO2\", \n         \"HighCO2H\", \"LowCO2H\"), as.numeric))\nhead(dogdat)\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nExperimental Setup\n\n\n\n\nDesign: p=4 treatments are assigned to the same experimental unit. In total, there are n= 19 experimental units.\nObjective: our interest is to formally assess whether there is significant difference between the two corresponding mean responses.\nMean responses: Let \\mu_j denote the mean response under treatment j, where j=1,\\ldots, 4. \\boldsymbol \\mu = (\\mu_1, \\ldots, \\mu_p)^\\top is a vector of mean responses over all p treatments (high CO2, low CO2, high CO2 + halothane, low CO2 + halothane).\nData: Let \\mathbf{x}_i=(x_{i1}, x_{i2}, \\ldots, x_{ip})^\\top denote the response variable for the i-th experimental unit under all the p treatments, where i=1, \\ldots, n.\n\n\n\n\n\n\n\n\n\nResearch Questions\n\n\n\n\n\nAs we want to assess whether there is significant difference between the two corresponding mean responses, we are interested in testing three specific effects simultaneously:\n\nIs there a main effect of halothane?\nIs there a main effect of CO2?\nIs there an interaction effect?\n\n\n\n\n\n\n\n\n\n\nStep 1: Prepare and Visualize the Data\n\n\n\n\n\n\n\nR Code: Scatterplot Matrix\nX = as.matrix(dogdat[ ,2:5])\nGGally::ggpairs(X)\n\n\n\n\n\nInterpretation: The smoothed histograms on the diagonal of the scatterplot matrix show unimodal distributions, but the responses to the high CO2 anesthetic appear to be skewed left and tails of the distributions appear to be truncated relative to normal distributions. The scatter plots show nearly elliptically shaped data clouds, however, with approximately straight line trends. Consequently, the pairwise correlations appear to provide a good description of the associations between the responses to the four anesthetics.\n\n\nR Code: Boxplot\nboxplot(X)\n\n\n\n\n\nInterpretation: The boxplots indicate that variation in times between heartbeats across the 19 dogs appear to be about the same for all for anesthetics. The distributions are roughly symmetric for all four anesthetics, but the distribution of times between heartbeats may be skewed left when the high level of CO2 is used without halothane. The boxplots also indicate the mean times between heartbeats tend to be higher when halothane is used, regardless of the level of CO2, so halothane may slow heart rates in dogs. There does not appear to be much difference in the distributions of times between heartbeats for low and high levels of CO2 when halothane is used. There is also not much difference between the distributions of times between heartbeats for the high and low levels of CO2 when halothane is not used, although times between heartbeats tend to be lower when the high level of CO2 is used. The higher level of CO2 may speed up heart rates in dogs, but the Halothane effect appears to have a much larger than the CO2 effect.\n\n\n\n\n4.7.1 Repeated Measures\nA repeated measures study is a research design where the same participants are measured multiple times under different conditions or over a period of time\nKey characteristics\n\nSame participants across conditions or time points\nMeasurements may be taken:\n\nAcross treatments (within-subjects design) – e.g., each participant tries all drug dosages.\nOver time (longitudinal design) – e.g., measuring blood pressure weekly for 8 weeks.\n\nCorrelation between measurements\n\nAdvantages\n\nControls for individual differences → reduces variability and increases statistical power.\nFewer participants needed compared to a between-subjects design for the same precision.\nAbility to track changes within individuals over time or across conditions.\n\nCommon Examples\n\nBefore-and-After Studies: This is the classic setup. You measure a group’s cholesterol levels (before), put them on a new diet for three months, and measure their levels again (after).\nComparing Multiple Treatments: Each participant is given several different drugs (with a washout period in between), and their reaction to each drug is measured. The dog anesthetic example from our previous conversation is a perfect case of this.\nLongitudinal Studies: A researcher tracks the same group of children and measures their reading ability at ages 6, 8, and 10 to see how it develops over time.\n\nKey Challenge: Dependent Data\nWhen the same subject is measured on p different occasions or under p different conditions, the measurements are not independent. A person’s score at time 1 is related to their score at time 2. Because of this dependency, we cannot directly apply previous methods on the data. However, we can still analyze this data by creating difference vectors and performing a one-sample Hotelling’s T^2 test on these differences.\n\n\n4.7.2 Hotelling’s T^2 Test for Contrasts\n\n\n\n\n\n\nStep 2: State Hypotheses\n\n\n\n\n\nThe research questions can be formulated as a hypothesis testing problem to test treatment effects simultaneously:\n\nHalothane Effect: The overall effect of halothane, averaging across CO2 pressure levels: (\\mu_3+\\mu_4)/2 - (\\mu_1+\\mu_2)/2.\nCO2 Pressure Effect: The overall effect of CO2 pressure, averaging across halothane levels: (\\mu_1+\\mu_3)/2 - (\\mu_2+\\mu_4)/2\nInteraction Effect: Whether the effect of CO2 pressure depends on the presence of halothane: (\\mu_1-\\mu_2)-(\\mu_3-\\mu_4)\n\n\nNull hypothesis: The no treatment effect can be written as testing H_0: C \\boldsymbol \\mu =\\mathbf{0} for a q\\times p contrast matrix C.\n\nq is the number of contrasts. In this example, q=3.\n\n\nAlternative hypothesis: H_1: C\\boldsymbol \\mu\\neq \\mathbf{0}\n\nQuestion: How to write down the contrast matrix for the null hypothesis?\n\nExercise: If one is interested in testing the hypothesis that all four treatments have the same mean, then \\begin{equation*}\nC=\n\\left[\\begin{array}{rrrr}\n-1 & 1 & 0 & 0 \\\\ 0 & -1 & 1 & 0 \\\\ 0 & 0 & -1 & 1\n\\end{array}\\right]\n\\end{equation*}\n\n\n\n\n\n\n\n\n\n\nStep 3: Define the Contrast Matrix\n\n\n\n\n\nThe contrast matrix defined by the null hypotheses above is given by\n\\begin{equation*}\nC=\n\\begin{bmatrix}\n-.5 & -.5 & .5 & .5 \\\\\n.5 & -.5 & .5 & -.5 \\\\\n1 & -1 & -1 & 1\n\\end{bmatrix}\n\\end{equation*} because\n\n\\begin{bmatrix}\n(\\mu_3 + \\mu_4)/2 - (\\mu_1 + \\mu_2)/2 \\\\\n(\\mu_1 + \\mu_3)/2 - (\\mu_2 + \\mu_4)/2 \\\\\n(\\mu_1 - \\mu_2) - (\\mu_3-\\mu_4)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-.5 & -.5 & .5 & .5 \\\\\n.5 & -.5 & .5 & -.5 \\\\\n1 & -1 & -1 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mu_1 \\\\\n\\mu_2 \\\\\n\\mu_3 \\\\\n\\mu_4 \\end{bmatrix}\n= {C} \\boldsymbol{\\mu}\n\n\nIt is important to note that C has dimension q\\times p, where q=p-1 is the number of contrasts.\n\n\n\nCode\nC&lt;-matrix( c(.5, -.5, .5, -.5, -.5, -.5, .5, .5, -1, 1, 1, -1), \n              nrow=3, ncol=4, byrow=T)\nC\n\n\n     [,1] [,2] [,3] [,4]\n[1,]  0.5 -0.5  0.5 -0.5\n[2,] -0.5 -0.5  0.5  0.5\n[3,] -1.0  1.0  1.0 -1.0\n\n\n\n\n\n\n\n\n\n\n\nStep 4: Transform the Data Using the Contrast Matrix\n\n\n\n\n\nFor each dog, apply the contrasts defined in matrix C to their four measurements. This converts the original 4 variables into 3 new contrast variables, which will be the subject of our test.\nWe use matrix multiplication to transform our 19 \\times 4 data matrix into a 19 \\times 3 matrix of contrast scores. The formula is Y = X C^\\top.\n\n\nCode\n# Transform the original data into contrast data\nY &lt;-  X %*% t(C)\n\n# Rename columns for clarity\ncolnames(Y) &lt;- c(\"Halothane_Effect\", \"CO2_Effect\", \"Interaction_Effect\")\n\nhead(Y)\n\n\n     Halothane_Effect CO2_Effect Interaction_Effect\n[1,]           -113.5       60.5                139\n[2,]              7.0      149.0                -20\n[3,]            -41.0      -43.0                 66\n[4,]            -38.5      129.5                -79\n[5,]            -10.5       97.5                 21\n[6,]            -73.0      142.0                 82\n\n\nCode\nGGally::ggpairs(Y)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 5: Check Assumptions\n\n\n\n\n\n\n\nCode\npar(mfrow=c(2,2),pch=1)     \nfor (i in 1:3){\n  qqnorm(Y[,i],  main=\"Normal Q-Q Plot\") \n  qqline(Y[,i], col=\"red\", lwd=2)\n}\n\napply(Y,2,shapiro.test)\n\n\n$Halothane_Effect\n\n    Shapiro-Wilk normality test\n\ndata:  newX[, i]\nW = 0.97195, p-value = 0.8146\n\n\n$CO2_Effect\n\n    Shapiro-Wilk normality test\n\ndata:  newX[, i]\nW = 0.80247, p-value = 0.001242\n\n\n$Interaction_Effect\n\n    Shapiro-Wilk normality test\n\ndata:  newX[, i]\nW = 0.96055, p-value = 0.5834\n\n\nCode\nmvShapiroTest::mvShapiro.Test(Y)\n\n\n\n    Generalized Shapiro-Wilk test for Multivariate Normality by\n    Villasenor-Alva and Gonzalez-Estrada\n\ndata:  Y\nMVW = 0.92376, p-value = 0.05625\n\n\n\n\n\nConclusion: The contrast data indeed approximately follow a multivariate normal distribution as indicated by the multivariate Shapiro-Wilk test even though the original data do not follow a multivariate normal distribution.\nNote: In practice, if the multivariate normality assumption is violated based on the Shapiro-Wilk test, one needs to address this issue before applying the Hotelling’s T^2 test. Alternatively, one can use a nonparametric method (e.g., Friedman test).\n\n\n\n\n\n\n\n\n\nStep 6: Hotelling’s T^2 Test\n\n\n\n\n\n\nTo test H_0: C\\boldsymbol \\mu = 0, we can again use T^2 statistic: \nT^2 = n(C\\bar{\\mathbf{x}} - \\mathbf{0})^\\top (CS^{-1}C^\\top)^{-1} (C\\bar{\\mathbf{x}} - \\mathbf{0})\n\nThe null hypothesis is rejected at significance level \\alpha if \nT^2 \\geq \\frac{(n-1)(p-1)}{(n-p+1)} F_{p-1, n-p+1, 1-\\alpha}\n where the numerator degrees of freedom are p-1 instead of p because the null hypotheses puts only p-1 constraints on the mean responses.\n\n\n\nCode\n# Define the hypothesized mean vector (a zero vector)\nmu0 &lt;- c(0, 0, 0)\n\n# Perform the one-sample test on the contrast data\ntest_result = DescTools::HotellingsT2Test(Y, mu=mu0)\nprint(test_result)\n\n\n\n    Hotelling's one sample T2-test\n\ndata:  Y\nT.2 = 34.375, df1 = 3, df2 = 16, p-value = 3.318e-07\nalternative hypothesis: true location is not equal to c(0,0,0)\n\n\nConclusion: We clearly reject the null at significance level \\alpha=0.05, so the question now becomes whether there is a difference between CO2 pressure, between halothane levels or perhaps there is no main effect of treatment but there is still an interaction. This question can be answered by looking at confidence intervals.\n\n\n\n\n\n\n\n\n\nPaired t-Test\n\n\n\n\n\nA paired t-test is a special, simplified case of the Hotelling’s T² test on contrasts. The powerful, general framework of the F-test on contrasts simplifies to become the familiar paired t-test when you are making only one comparison between two measurements.\nThe Paired t-Test: A Simple Contrast\nA paired t-test is designed to answer one question: “Is there a significant difference between two matched measurements (e.g., before vs. after)?”\n\nThe Data: You have two measurements for each subject, X_1 and X_2.\nThe Method: For each subject, you calculate a single difference score, d = X_1 - X_2. You then perform a one-sample t-test on these difference scores to see if their mean is significantly different from 0.\nThe null hypothesis: H_0: \\mu_d = 0, which is the same as H_0: \\mu_1 - \\mu_2 = 0.\n\n\n\n\n\n\n4.7.3 Confidence Regions for Linear Combinations of Means\nA 100(1-\\alpha)\\% confidence region for any linear combination of population means, say C \\boldsymbol{\\mu}, is the set of all vectors \\boldsymbol \\mu satisfying: \nn(C\\bar{\\mathbf{x}} - C\\boldsymbol{\\mu})^\\top (CSC^\\top)^{-1} (C\\bar{\\mathbf{x}} - C\\boldsymbol{\\mu}) \\le \\frac{(p-1)(n-1)}{n-p+1} F_{p-1, n-p+1, 1-\\alpha}\n\n\n\n4.7.4 Simultaneous CIs for Linear Combinations of Means\nAfter performing a multivariate test, we are often interested in more complex comparisons than just single means. We might want to test a linear combination of the population means, which takes the general form:\n \\mathbf{c}'\\boldsymbol{\\mu} = c_1\\mu_1 + c_2\\mu_2 + \\dots + c_p\\mu_p \nWe can construct simultaneous confidence intervals for a set of m such combinations, where the k-th combination is defined by the coefficient vector \\mathbf{c}_k = [c_{k1}, c_{k2}, \\dots, c_{kp}]'.\nThe point estimate for any linear combination \\mathbf{c}_k'\\boldsymbol{\\mu} is given by \\mathbf{c}_k'\\bar{\\mathbf{x}} = \\sum_{j=1}^p c_{jk} \\mu_j, and the estimated variance is \\frac{\\mathbf{c}_k'{S}\\mathbf{c}_k}{n}. The margin of error is then determined by one of the following methods.\n\n\n\n\n\n\nT^2 CI\n\n\n\nThis method derives its critical value from the F-distribution associated with Hotelling’s T^2 test. The confidence intervals are valid simultaneously for any and all possible linear combinations.\nA 100(1-\\alpha)\\% simultaneous confidence intervals are given by:\n\n\\mathbf{c}_k'\\bar{\\mathbf{x}} \\pm \\sqrt{\\frac{(p-1)(n-1)}{n-p+1} F_{p-1, n-p+1, 1-\\alpha}} \\sqrt{\\frac{\\mathbf{c}_k'{S}\\mathbf{c}_k}{n}} \\quad \\text{for } k=1, 2, \\dots, m\n\nIn the dog anesthetics study, the T^2 CIs can be computed as follows.\n\n\nR Code: T^2 CIs\nybar = colMeans(Y)\nSy = cov(Y)  # this is the same as C%*%cov(X)%*%t(C) \nF_crit = qf(1-0.05, p-1, n-p)\n\ncval_T2 = sqrt((p-1)*(n-1)/(n-p+1) * F_crit) \nS_kk = diag(Sy)\n\nci_T2 = tibble(\n  Component = names(ybar),\n  Estimate = ybar,\n  Lower = Estimate - cval_T2 * sqrt(S_kk/n), \n  Upper = Estimate + cval_T2 * sqrt(S_kk/n)\n)\nprint(ci_T2)\n\n\n# A tibble: 3 × 4\n  Component          Estimate Lower  Upper\n  &lt;chr&gt;                 &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 Halothane_Effect      -30.0 -53.2  -6.84\n2 CO2_Effect            105.   73.4 136.  \n3 Interaction_Effect     12.8 -43.1  68.7 \n\n\nInterpretation:\n\nThe T^2 CI for the Halothane effect is shifted below zero, indicating that the mean time between heartbeats is likely to be between -53.2 and -6.84 milliseconds shorter when Halothane is used than that when Halothane is not used, averaging across the CO2 pressure levels.\nThe T^2 CI for the CO2 effect is far above 0, indicating that the mean time between heartbeats is likely to be between 73.4 and 136 milliseconds longer at high CO2 pressure than that at low CO2 pressure, averaging across the halothane effect.\n\nThe T^2 CI for the interaction effect contains 0, indicating\n\nthe effect of using halothane on the mean time between heartbeats is about the same for the two levels of CO2 pressure;\nthe effect of changing from low to high pressure of CO2 is about the same when halothane is used as when halothane is not used.\n\n\n\n\n\n\n\n\n\n\nBonferroni CI\n\n\n\nThe 100(1-\\alpha)\\% simultaneous Bonferroni confidence intervals are given by:\n \\mathbf{c}_k'\\bar{\\mathbf{x}} \\pm t_{n-1, 1-\\alpha/(2m)} \\sqrt{\\frac{\\mathbf{c}_k'{S}\\mathbf{c}_k}{n}} \\quad \\text{for } k=1, 2, \\dots, m \n\nNote the critical value is a t-statistic adjusted by m, the number of intervals being constructed.\nIt also controls the family-wise error rate at \\alpha.\n\nIn the dog anesthetics study, the Bonferroni CIs can be computed as follows.\n\n\nR Code: Bonferroni CIs\nm = 3 \n\ncval_Bon = qt(1-0.05/(2*m), n-1)\n\nci_Bon = tibble(\n  Component = names(ybar),\n  Estimate = ybar,\n  Lower = Estimate - cval_Bon * sqrt(S_kk/n), \n  Upper = Estimate + cval_Bon * sqrt(S_kk/n)\n)\nprint(ci_Bon)\n\n\n# A tibble: 3 × 4\n  Component          Estimate Lower   Upper\n  &lt;chr&gt;                 &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Halothane_Effect      -30.0 -59.4  -0.686\n2 CO2_Effect            105.   65.1 144.   \n3 Interaction_Effect     12.8 -58.0  83.6  \n\n\nInterpretation: While specific Bonferroni CI limits are slightly different than the limits of T^2 CIs, we can notice that same conclusions will be drawn for testing the Halothane effect, the CO2 effect and the interaction effect."
  },
  {
    "objectID": "ch4/04-Inference_for_Means.html#exercises",
    "href": "ch4/04-Inference_for_Means.html#exercises",
    "title": "4  Inference for Single Mean Vector",
    "section": "4.8 Exercises",
    "text": "4.8 Exercises\n\n4.8.1 Exercise 1: iris Data, Cont’d\nUsing the iris data, answer the following questions.\n\nFor the versicolor species, test if the mean vector for Sepal.Length and Petal.Length is equal to \\boldsymbol{\\mu}_0 = [6.0, 4.0]'. Use \\alpha=0.05. If you reject the null hypothesis, construct 95% Bonferroni simultaneous CIs to determine which variable(s) differ from the hypothesized values.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\ndat &lt;- iris[iris$Species == \"versicolor\", c(\"Sepal.Length\", \"Petal.Length\")]\nmu0 &lt;- c(6.0, 4.0)\n\nht &lt;- HotellingsT2Test(dat, mu = mu0)\nprint(ht)\n\n\n\n    Hotelling's one sample T2-test\n\ndata:  dat\nT.2 = 24.124, df1 = 2, df2 = 48, p-value = 5.602e-08\nalternative hypothesis: true location is not equal to c(6,4)\n\n\nConclusion: The p-value is much less than 0.05. We reject H_0 at significance level \\alpha=0.05 and conclude that the mean sepal length and the mean petal length is different from the the hypothesized values. To understand which variable would contribute to such difference, we look at the individual CIs below.\n\n\nCode\n# 3. Construct Bonferroni intervals since we rejected H0\nn &lt;- nrow(dat)\np &lt;- ncol(dat)\nxbar &lt;- colMeans(dat)\nS &lt;- cov(dat)\ns_ii &lt;- diag(S)\n\ncval_bon &lt;- qt(1 - 0.05 / (2 * p), df = n - 1)\nbonf_margins &lt;- cval_bon * sqrt(s_ii / n)\nbonf_intervals &lt;- data.frame(\n  Variable = names(dat),\n  Hypothesized_Mean = mu0,\n  Lower = xbar - bonf_margins,\n  Upper = xbar + bonf_margins\n)\n\nprint(\"95% Bonferroni Simultaneous CIs:\")\n\n\n[1] \"95% Bonferroni Simultaneous CIs:\"\n\n\nCode\nprint(bonf_intervals)\n\n\n                 Variable Hypothesized_Mean    Lower    Upper\nSepal.Length Sepal.Length                 6 5.767202 6.104798\nPetal.Length Petal.Length                 4 4.106330 4.413670\n\n\nInterpretation: The CI for Sepal.Length (5.77 to 6.10) contains the hypothesized mean of 6.0. However, the CI for Petal.Length (4.11 to 4.41) does NOT contain its hypothesized mean of 4.0. Therefore, we conclude the overall mean vector is different because the mean Petal Length is significantly greater than 4.0.\n\n\n\n\nFor the setosa species, construct simultaneous 95% confidence intervals for two linear combinations (m=2):\n\n\nDifference: \\mu_{Sepal.Length} - \\mu_{Sepal.Width}\nSum: \\mu_{Sepal.Length} + \\mu_{Sepal.Width}\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nR Code: CI Calculation\nlibrary(dplyr)\n\ndata &lt;- iris %&gt;%\n  filter(Species == \"setosa\") %&gt;%\n  select(Sepal.Length, Sepal.Width)\n\nn &lt;- nrow(data)\np &lt;- ncol(data)\nalpha &lt;- 0.05\nsample_mean &lt;- colMeans(data)\nS &lt;- cov(data)\n\n# Define the linear combinations (m=2)\n# c1 will be for the Difference, c2 for the Sum\nc1 &lt;- c(1, -1)\nc2 &lt;- c(1, 1)\n# Combine into a list for easy iteration\ncombinations &lt;- list(Difference = c1, Sum = c2)\nm &lt;- length(combinations)\n\n# Calculate intervals for each method\nresults &lt;- list()\nfor (i in 1:m) {\n  c_k &lt;- combinations[[i]]\n  combo_name &lt;- names(combinations)[i]\n  \n  point_estimate &lt;- t(c_k) %*% sample_mean\n  std_error &lt;- sqrt((t(c_k) %*% S %*% c_k) / n)\n  \n  # T-squared Method\n  T2_crit &lt;- sqrt((p * (n - 1) / (n - p)) * qf(1 - alpha, p, n - p))\n  T2_margin &lt;- T2_crit * std_error\n  \n  # Bonferroni Method (adjust alpha by m)\n  Bonf_crit &lt;- qt(1 - alpha / (2 * m), df = n - 1)\n  Bonf_margin &lt;- Bonf_crit * std_error\n\n  results[[i]] &lt;- data.frame(\n    Combination = combo_name,\n    T2_Lower = point_estimate - T2_margin,\n    T2_Upper = point_estimate + T2_margin,\n    Bonf_Lower = point_estimate - Bonf_margin,\n    Bonf_Upper = point_estimate + Bonf_margin\n  )\n}\n\n# Combine and print the results\nfinal_table &lt;- do.call(rbind, results)\nprint(final_table)\n\n\n  Combination T2_Lower T2_Upper Bonf_Lower Bonf_Upper\n1  Difference 1.482838 1.673162   1.491785   1.664215\n2         Sum 8.187499 8.680501   8.210674   8.657326\n\n\nInterpretation: As shown in the table, the Bonferroni confidence intervals for both the “Difference” and the “Sum” are narrower than their T² counterparts, offering a more precise estimate for these specific comparisons.\n\n\n\n\n\n4.8.2 Exercise 2: Dogs Anesthetics Study, Cont’d\nIn the Dogs Anesthetics study, we have tested the main effects and the interaction effect with the contrast matrix to test that all the three differences (the Halothane effect, the CO2 effect, and the interaction effect) in the mean responses have mean zero. This is equivalent to testing the null hypothesis that all four anesthetics induce the same mean times between heartbeats. In fact, this problem can be formulated by comparing any possible pairs of mean differences with zero.\n\n# load data\ndogdat &lt;- read.table(\"dogs.dat\",\n            col.names=c(\"dog\", \"HighCO2\", \"LowCO2\", \n                        \"HighCO2H\", \"LowCO2H\")) %&gt;%\n  mutate(across(c(\"HighCO2\", \"LowCO2\", \n         \"HighCO2H\", \"LowCO2H\"), as.numeric))\nhead(dogdat)\n\n\n\n  \n\n\n\n\nIn Section 4.7, we have used contrasts to test the hypothesis. Please justify why the hypothesis formulated via contrast is equivalent to test if all four anesthetics treatments induce the same mean times between heartbeats.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\nWe will use the same notation as defined in Section 4.7: \\boldsymbol{\\mu}=(\\mu_1, \\mu_2, \\mu_3, \\mu_4)^\\top represents the mean times between heartbeats under the four treatments: high CO2 pressure + no Halothane, low CO2 pressure + no Halothane, high CO2 pressure + Halothane, low CO2 pressure + Halothane.\nFrom the null hypothesis in Section 4.7, we have \n\\begin{aligned}\n\\mu_3 + \\mu_4 &= \\mu_1 + \\mu_2, \\\\\n\\mu_1 + \\mu_3 &= \\mu_2 + \\mu_4,\\\\\n\\mu_1 - \\mu_2 &=\\mu_3 - \\mu_4.\n\\end{aligned}\n Thus, after some algebra, we obtain that the above is true if and only if \\mu_1=\\mu_2=\\mu_3=\\mu_4.\nMoreover, this constraint gives three independent/unique differences as other differences are just linear combinations of these three independent differences.\nSo, the null hypothesis in Section 4.7 is also equivalent to test if all the tree independent/unique differences are zero.\n\n\n\n\nWrite down the null hypothesis (H_0) and the alternative hypothesis (H_1) using population means only.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\nNull Hypothesis (H_0): The true mean times between heartbeats for each anesthetic is the same: \nH_0: \\mu_1=\\mu_2=\\mu_3=\\mu_4\n\nAlternative Hypothesis (H_1): At least one mean is not the same as one of the other means.\n\n\n\n\n\nUse box plots to compare the distributions of the values for the differences in mean times between heartbeats.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\ndat = dogdat\ndat$diff1 &lt;- dat$HighCO2-dat$LowCO2 \ndat$diff2 &lt;- dat$HighCO2H-dat$LowCO2\ndat$diff3 &lt;- dat$LowCO2H-dat$LowCO2\nhead(dat)\n\n\n\n\n  \n\n\n\n\n\nCode\nboxplot(dat[, 6:8])\n\n\n\n\n\n\n\n\n\nCheck Assumptions.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\npar(mfrow=c(2,2),pch=1)     \nfor (i in 6:8){\n  qqnorm(dat[,i],  main=\"Normal Q-Q Plot\") \n  qqline(dat[,i], col=\"red\", lwd=2)\n}\npar(c(1,1))\n\n\nNULL\n\n\nCode\napply(dat[,6:8],2,shapiro.test)\n\n\n$diff1\n\n    Shapiro-Wilk normality test\n\ndata:  newX[, i]\nW = 0.8493, p-value = 0.006534\n\n\n$diff2\n\n    Shapiro-Wilk normality test\n\ndata:  newX[, i]\nW = 0.90513, p-value = 0.06029\n\n\n$diff3\n\n    Shapiro-Wilk normality test\n\ndata:  newX[, i]\nW = 0.8558, p-value = 0.008349\n\n\nCode\nmvShapiroTest::mvShapiro.Test(as.matrix(dat[,6:8]))\n\n\n\n    Generalized Shapiro-Wilk test for Multivariate Normality by\n    Villasenor-Alva and Gonzalez-Estrada\n\ndata:  as.matrix(dat[, 6:8])\nMVW = 0.88078, p-value = 0.001033\n\n\n\n\n\n\nBecause the p-value is quite small (0.001033) the Shapiro-Wilk test for multivariate normality rejects the null hypothesis that the joint distribution of the three differences is multivariate normal distribution.\nThe Shapiro-Wilk tests for univariate normality indicate that the distributions of the first and third sets of differences are not normal distributions. The normal Q-Q plots indicate that those distributions are skewed to the left.\n\n\n\n\n\nPerform the Hotelling T^2 test for the null hypothesis.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\nDescTools::HotellingsT2Test(dat[,6:8], mu=c(0,0,0))\n\n\n\n    Hotelling's one sample T2-test\n\ndata:  dat[, 6:8]\nT.2 = 34.375, df1 = 3, df2 = 16, p-value = 3.318e-07\nalternative hypothesis: true location is not equal to c(0,0,0)\n\n\n\n\n\n\nTo examine which mean is significant, we can investigate the simultaneous 95% confidence intervals for all six possible pairs of population mean differences: \\mu_1-\\mu_2, \\mu_1-\\mu_3,\\mu_1-\\mu_4, \\mu_2-\\mu_3, \\mu_2-\\mu_4, \\mu_3-\\mu_4.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\nTB.conf.int &lt;- function(X, level = 0.95)\n{ \n  # Convert X to a matrix, if it is not a matrix already, from\n  # vectors or data frames.\n  X &lt;- as.matrix(X)\n  \n  # Set n to the number of observations, p to the number of variables.\n  n &lt;- nrow(X)\n  p &lt;- ncol(X)\n  \n  # Stop if arguments are invalid.\n  if (!is.numeric(X))\n  {\n    stop(\"Data must be numeric\")\n  }\n  \n  if (n &lt; p)\n  {\n    stop(\"Must have at least as many observations as variables\")\n  }\n  \n  if (!is.numeric(level) || length(level) != 1 || level &lt;= 0 || level &gt;= 1)\n  {\n    stop(\"Confidence level must be between 0 and 1\")\n  }\n  \n  # Create a matrix A in which each column represents\n  # a difference between two pairs of means\n  np &lt;- p * (p - 1) / 2\n  A &lt;- matrix(c(0), ncol = np, nrow = p)\n  nc &lt;- 0\n  for (i in 1:(p - 1)) {\n    for (j in 1:(p - i)) {\n      A[i, nc + j] &lt;- 1\n      A[i + j, nc + j] &lt;- -1\n    }\n    nc &lt;- nc + (p - i)\n  }\n  \n  # Create a matrix that will hold the confidence intervals.\n  CI &lt;- matrix(NA, 2, ncol(A))\n  rownames(CI) &lt;- c(\"lower\", \"upper\")\n  colnames(CI) &lt;- colnames(A)\n  \n  CIB &lt;- matrix(NA, 2, ncol(A))\n  rownames(CIB) &lt;- c(\"lower\", \"upper\")\n  colnames(CIB) &lt;- colnames(A)\n  \n  # Find F distribution quantile for T-squared confidence intervals.\n  F &lt;- qf(level, p, n - p)\n  \n  # Find t distribution percentile for Bonferroni confidence intervals\n  alpha &lt;- (1 - level) / 2 / ncol(A)\n  levelB &lt;- 1 - alpha\n  tB &lt;- qt(levelB, n - 1)\n  t &lt;- qt(1 - (1 - level) / 2, n - 1)\n  \n  # Compute the sample covariance matrix of the original variables.\n  C &lt;- cov(X)\n  \n  # Find the confidence intervals for the specified linear combinations.\n  for (i in 1:ncol(A))\n  { \n    # Find the sample mean and variance of this linear combination.\n    m &lt;- mean(X %*% A[, i])\n    v &lt;- t(A[, i]) %*% C %*% A[, i]\n    \n    # Find the confidence interval for this difference.\n    CI[1, i] &lt;- m - sqrt((p * (n - 1) / n / (n - p)) * F * v)\n    CI[2, i] &lt;- m + sqrt((p * (n - 1) / n / (n - p)) * F * v)\n    \n    CIB[1, i] &lt;- m - tB * sqrt(v / n)\n    CIB[2, i] &lt;- m + tB * sqrt(v / n)\n    \n  }\n  \n  # Print the confidence intervals.\n  cat(\" T-squared CIs: \\n\\n\")\n  print(CI)\n  \n  cat(\"\\n\\n Bonferroni CIs: \\n\\n\")\n  print(CIB)\n}\n\nTB.conf.int(dat[, 2:5])\n\n\n T-squared CIs: \n\n           [,1]      [,2]       [,3]       [,4]       [,5]      [,6]\nlower -89.46963 -165.0961 -183.66206 -131.61501 -158.56850 -69.54636\nupper  16.62753  -57.0092  -85.70637  -17.64815  -37.95781  22.28320\n\n\n Bonferroni CIs: \n\n            [,1]       [,2]       [,3]       [,4]       [,5]      [,6]\nlower -77.460347 -152.86156 -172.57431 -118.71494 -144.91641 -59.15204\nupper   4.618242  -69.24371  -96.79411  -30.54822  -51.60991  11.88889\n\n\nInterpretation\n\nBoth the T^2 CIs and Bonferroni CIs yields the same conclusions as whenever one interval contains zero, the other also contains zero; and whenever one interval excludes zero, the other also excludes zero.\nThe CIs for \\mu_1-\\mu_2 and \\mu_3-\\mu_4 contains zeros, indicating\n\nthat there is no significant difference in mean times between heartbeats for using high or low CO2 pressure regardless of whether halothane is used or not;\nand that there appears to be no significant effect of high or low pressure of CO2 on mean heart rates, after controlling for the presence or absence of halothane.\n\n\nAll of the other confidence intervals do not contain zero indicating that mean heart rates tend to be slower (mean time between heartbeats is longer) when halothane is used than when halothane is not used, regardless of the CO2 pressure level.\nBoth sets of intervals provide at least 95% simultaneous coverage of the six differences in population means for difference between heartbeats, but the Bonferroni intervals are shorter than the T-squared intervals.\n\n\n\n\n\n\n4.8.3 Exercise 3: Baseball Player Data\nBackground\nA sports science journal from the 1950s established a “classic” physical standard for professional baseball players, claiming the ideal physique had a mean height of 72.5 inches and a mean weight of 209 pounds. The data is from the Lahman::People dataset.\nYour task is to determine if the average physique of modern players has significantly changed from this historical benchmark using a Hotelling’s T² test with a significance level of \\alpha = 0.05. In the analysis, we assume that the random vector follows a multivariate distribution and carry out the analysis, although it actually fails the multivariate Shapiro-Wilk test.\n1. State the Hypotheses\nWrite the null hypothesis (H_0) and the alternative hypothesis (H_1) for this test using proper mathematical notation. Let \\boldsymbol{\\mu} represent the true mean vector of [height, weight] for modern players.\n\n\n\n\n\n\nView Solution\n\n\n\n\n\nThe hypotheses are formulated as:\n\nNull Hypothesis (H_0): The true mean vector of modern players is equal to the historical standard. H_0: \\boldsymbol{\\mu} = \\begin{bmatrix} 72.5 \\\\ 209 \\end{bmatrix}\nAlternative Hypothesis (H_1): The true mean vector of modern players is not equal to the historical standard. H_1: \\boldsymbol{\\mu} \\neq \\begin{bmatrix} 72.5 \\\\ 209 \\end{bmatrix}\n\n\n\n\n2. Prepare the Data\nLoad the necessary R packages. From the Lahman::People dataset, create a final data frame that contains only the height and weight columns for players who debuted in the year 2010 or later, with any missing values removed.\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\nlibrary(Lahman)\nlibrary(dplyr)\nlibrary(ggplot2)\n# Create the final data frame for analysis\nplayer_data &lt;- Lahman::People %&gt;%\n  filter(debut&gt;2010) %&gt;%\n  filter(!is.na(height) & !is.na(weight)) %&gt;%\n  dplyr::select(height, weight)%&gt;%\n  mutate(across(c(height, weight), as.numeric))\n\ndf = player_data\nn = nrow(df)\np = ncol(df)\nhead(df)\n\n\n\n\n  \n\n\n\nCode\nGGally::ggpairs(df)\n\n\n\n\n\n\n\n\n3. Check Key Assumptions\nCreate and examine histograms for both the height and weight distributions in your sample. Do they appear approximately normal?\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nR Code: Normality Check\n## Q-Q plot\npar(mfrow = c(1, 3))\n\n# Loop through the column names and create a plot for each\nfor (col_name in colnames(df)) {\n  qqnorm(df[[col_name]], main = col_name)\n  qqline(df[[col_name]], col = \"red\", lwd = 2)\n}\n\n## Compute Shapiro-Wilk statistic to test normality for each variable\nsapply(colnames(df), function(x) {\n               shapiro.test(df[[x]]) } )\n\n\n          height                        weight                       \nstatistic 0.9834675                     0.9929459                    \np.value   1.309451e-19                  6.188722e-12                 \nmethod    \"Shapiro-Wilk normality test\" \"Shapiro-Wilk normality test\"\ndata.name \"df[[x]]\"                     \"df[[x]]\"                    \n\n\nR Code: Normality Check\n## Perform multivariate normality test\nmvShapiroTest::mvShapiro.Test(as.matrix(df))\n\n\n\n    Generalized Shapiro-Wilk test for Multivariate Normality by\n    Villasenor-Alva and Gonzalez-Estrada\n\ndata:  as.matrix(df)\nMVW = 0.99687, p-value = 1.045e-10\n\n\n\n\n\nInterpretation: Both distributions are mound-shaped and reasonably symmetric, supporting the assumption of approximate normality.\n\n\n\n4. Perform the Statistical Test\nRun a one-sample Hotelling’s T^2 test on your prepared data using the historical standard as your hypothesized mean vector.\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nR Code: Hotelling’s T^2 Test\n# Define the hypothesized mean vector from the 1950s standard\nmu0 &lt;- c(72.5, 209.0)\n\n# Perform the test\ntest_result &lt;- DescTools::HotellingsT2Test(x = df, mu = mu0)\n\n# Print the results\nprint(test_result)\n\n\n\n    Hotelling's one sample T2-test\n\ndata:  df\nT.2 = 589.64, df1 = 2, df2 = 3432, p-value &lt; 2.2e-16\nalternative hypothesis: true location is not equal to c(72.5,209)\n\n\n\n\n\n5. Interpret the Results\nWhat is the p-value from your test? Based on this and \\alpha = 0.05, do you reject or fail to reject the null hypothesis? State your conclusion in the context of the problem.\n\n\n\n\n\n\nView Solution\n\n\n\n\n\nThe p-value is exceptionally small and far below the significance level of \\alpha = 0.05.\nConclusion: We strongly reject the null hypothesis. There is overwhelming statistical evidence to conclude that the average physique (the mean vector of height and weight) of modern baseball players is significantly different from the historical standard of 72 inches and 190 pounds.\n\n\n\n6. Draw Conclusions Based on Confidence Region\nCheck if the hypothesized mean is in the 95% confidence ellipse. Explain how it supports your conclusion.\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nR Code: Visualization\n# Calculate the mean of our modern player sample\nxbar &lt;- colMeans(df)\nS = cov(df)\nT2 = n*t(xbar - mu0) %*%solve(S)%*%(xbar - mu0)\nscaledT2 = (n-p)/(p*(n-1)) * T2 \n# Calculate points for the 95% confidence ellipse\nalpha=0.05\nFvalue &lt;- qf(1 - alpha, p, n - p)\n\ncat(\"scaled T2 statistic is\",scaledT2, \", F critical value is\", Fvalue)\n\n\nscaled T2 statistic is 589.6427 , F critical value is 2.998349\n\n\n\n\nCode\n# this is the covariance of xbar\nSigma_ell &lt;- S / n\neig &lt;- eigen(Sigma_ell)\nA &lt;- eig$vectors %*% diag(sqrt(eig$values)) *\nsqrt((n - 1) * p / (n - p) * Fvalue)\n\ntheta &lt;- seq(0, 2 * pi, length.out = 400)\npts &lt;- t(matrix(xbar, nrow = 2, ncol = length(theta)) +\nA %*% rbind(cos(theta), sin(theta)))\n\ndf_ell &lt;- as.data.frame(pts)\ncolnames(df_ell) &lt;- names(df)\ncenter &lt;- data.frame(height = xbar[1], weight = xbar[2])\n\ngg &lt;- ggplot() +\ngeom_path(data = df_ell, aes(height, weight)) +\ngeom_point(\ndata = center,\naes(height, weight),\ncolor = \"red\",\nsize = 3\n) +\ntheme_minimal() +\nlabs(x = \"Height Difference\", y = \"Weight Difference\")\n\nprint(gg)\n\n\n\n\n\nInterpretation: This result confirms the test result. The historical standard is far outside the confidence ellipse (the 95% confidence region for the true mean of modern players), confirming that modern players are, on average, both taller and heavier. At this point we do not know which one contributes to the difference, which can be answered using simultaneous confidence intervals.\n\n\nSimultaneous T2 CIs\ncval_T2 = sqrt((n-1)*p/(n-p) * Fvalue) \nse_ii = sqrt(diag(S)/n)\n\nci_T2 = tibble(\nComponent = names(df),\nEstimate = xbar,\nLower = Estimate - cval_T2 * se_ii,\nUpper = Estimate + cval_T2 * se_ii\n)\nprint(ci_T2)\n\n\n# A tibble: 2 × 4\n  Component Estimate Lower Upper\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 height        73.7  73.6  73.8\n2 weight       209.  208.  210. \n\n\n\n\nSimultaneous Bonferroni CIs\nm = 2\ncval_Bon = qt(1-0.05/(2*m), n-1)\nci_Bon = tibble(\nComponent = names(df),\nEstimate = xbar,\nLower = Estimate - cval_Bon * se_ii,\nUpper = Estimate + cval_Bon * se_ii\n)\nprint(ci_Bon)\n\n\n# A tibble: 2 × 4\n  Component Estimate Lower Upper\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 height        73.7  73.6  73.8\n2 weight       209.  208.  210. \n\n\nBased on simultaneous CIs (T2 or Bonferroni), the results indicates that there is a significantly individual difference in height and weight at \\alpha=0.05."
  },
  {
    "objectID": "ch5/05-Inference_for_Means_II.html#two-sample-comparison",
    "href": "ch5/05-Inference_for_Means_II.html#two-sample-comparison",
    "title": "5  Inference for Multiple Mean Vectors",
    "section": "5.1 Two-Sample Comparison",
    "text": "5.1 Two-Sample Comparison\nData sources and measurements\n\nWhere the data come from\n\nTwo independent samples: Select n_1 units from population 1 and n_2 units from population 2 (samples are independent; n_1 and n_2 may differ).\nRandomized experiment: Randomly assign n_1 units to treatment 1 and n_2 units to treatment 2 (sample sizes need not be equal).\n\nWhat we measure\nFor each unit, record the same set of p variables (traits), forming a p-dimensional measurement vector.\n\n\n\n\n\n\n\nKey Assumptions\n\n\n\nThe following assumptions are needed to make inferences about the difference between two population mean vectors \\boldsymbol \\mu_1 - \\boldsymbol \\mu_2:\n\n\\mathbf{x}_{1j} \\sim N_p(\\boldsymbol{\\mu}_1, \\Sigma_1) independently for j=1,\\dots,n_1; and \\mathbf{x}_{2k} \\sim N_p(\\boldsymbol{\\mu}_2, \\Sigma_2) independently for k=1,\\dots,n_2.\n\n\\Sigma_1=\\Sigma_2=\\Sigma. (Homogeneity of Covariance)\n\\mathbf{x}_{1j}’s are independent of \\mathbf{x}_{2j}’s.\n\n\n\n\n5.1.1 Example: Two Soap Manufacturing Processes\nBackground: A consumer goods company is developing a new method for producing soap. They want to determine whether the new process (Process 2) improves product quality compared to the current standard method (Process 1). Two key performance outcomes are of interest:\n\nLather quality (x_1):\n\nA measure of how much and how long-lasting the foam is when the soap is used.\nMeasured on a continuous scale by laboratory technicians using a standardized test.\n\nMildness (x_2):\n\nA subjective measure of how gentle the soap is on skin, evaluated by a panel of trained users.\nAlso measured on a continuous scale (e.g., skin irritation score, lower is better).\n\n\n\n\n\n\n\n\nExperimental Setup\n\n\n\n\nDesign: A randomized controlled experiment.\nSample sizes:\n\nn_1 = 50 soaps produced using the current process (Process 1)\nn_2 = 50 soaps produced using the new process (Process 2)\n\nMeasurements:\n\nEach bar of soap is tested for both lather and mildness independently.\n\n\n\nn1 = 50\nn2 = 50\np = 2\nxbar1 = c(8.3, 4.1)\nS1 = matrix(c(2,1,1,6),byrow=TRUE,ncol=2)\nxbar2 = c(10.2, 3.9)\nS2 = matrix(c(2,1,1,4), byrow=TRUE, ncol=2)\n\n\n\n\n\n\n\n\n\nResearch Questions\n\n\n\n\nIs there evidence that the new process produces soaps with different overall quality, as measured by both lather and mildness?\nIf a difference exists, which outcome (lather or mildness) contributes more to that difference?\n\n\n\n\n\n5.1.2 State the Hypotheses\nLet \\boldsymbol \\mu_1 and \\boldsymbol \\mu_2 be the population mean vectors for process 1 and process 2, respectively: \\begin{align*}\n\\boldsymbol \\mu_1 = [\\text{Mean Leather}_1, \\text{Mean Mildness}_1]^\\top,\\\\\n\\boldsymbol \\mu_2 = [\\text{Mean Leather}_2, \\text{Mean Mildness}_2]^\\top.\n\\end{align*}\n\n\n\n\n\n\nHypotheses\n\n\n\n\n\n\nH_0: \\boldsymbol \\mu_1 = \\boldsymbol \\mu_2 \\quad \\text{v.s.}\\quad H_1: \\boldsymbol \\mu_1 \\neq \\boldsymbol \\mu_2\n\n\n\n\n\n\n5.1.3 Pool Covariance\n\n\n\n\n\n\nPool Covariance\n\n\n\n\nPoint estimate of \\boldsymbol \\mu_1 - \\boldsymbol \\mu_2 is \\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2.\nThe population covariance matrix of \\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2 is \n\\text{Cov}(\\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2) = \\text{Cov}(\\bar{\\mathbf{x}}_1) + \\text{Cov}(\\bar{\\mathbf{x}}_2) = \\frac{1}{n_1}\\Sigma + \\frac{1}{n_2}\\Sigma,\n\nThe pooled estimate of the population covariance matrix is \nS_{\\text{pool}} = \\frac{(n_1 - 1)}{(n_1 + n_2 - 2)} S_1 + \\frac{(n_2 - 1)}{(n_1 + n_2 - 2)} S_2\n\n\n\n\n\n\nR Code: Pooled Covariance\nDelta = xbar1 - xbar2 \nSp = (n1-1)/(n1+n2-2)*S1 + (n2-1)/(n1+n2-2)*S2\nprint(Sp)\n\n\n     [,1] [,2]\n[1,]    2    1\n[2,]    1    5\n\n\n\n\n5.1.4 Hotelling’s T^2 Statistic\n\n\n\n\n\n\nHotelling’s T^2 Statistic\n\n\n\n\n\nThe test statistic is the Hotelling’s T^2 statistic: \nT^2 = (\\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2 )^\\top\\left[\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right) S_{\\text{pool}}\\right]^{-1}(\\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2)\n\n\n\n\n\n\nR Code: T2 Statistic\nT2 = drop(t(Delta) %*% solve((1/n1+1/n2) * Sp) %*% Delta)\nprint(T2)\n\n\n[1] 52.47222\n\n\n\n\n5.1.5 Decision\n\n\n\n\n\n\nDecision\n\n\n\n\n\nWe reject H_0: \\boldsymbol \\mu_1 - \\boldsymbol \\mu_2 = 0 at level \\alpha using one of the following two approaches:\n\nCritical Region: T^2 &gt; c^2, where \nc^2:= \\frac{(n_1 + n_2 -2)p}{(n_1 + n_2 - p - 1)}F_{(p, n_1 + n_2 -p -1), 1-\\alpha}.\n\nP-value: The p-value is less than \\alpha.\n\n\n\nCode\nalpha =0.05\nc2 = (n1+n2-2)*p / (n1+n2-p-1) * qf(1-alpha, p, n1+n2-p-1)\np_val &lt;- 1 - pf((T2 * (n1 + n2 - p - 1)) / \n                  (p * (n1 + n2 - 2)), p, n1 + n2 - p - 1)\ncat(\"critical value:\", c2, \" with \", \"p-value:\", p_val, \"\\n\")\n\n\ncritical value: 6.244089  with  p-value: 9.286081e-10 \n\n\nInterpretation: Since T^2&gt;c^2, we reject H_0 at \\alpha=0.05 and conclude that the population mean measures on lather and mildness are statistically different, but at this point, we do not know which variable contributes to the difference.\n\n\n\n\n\n5.1.6 Confidence Region\n\n\n\n\n\n\nConfidence Region\n\n\n\n\n\nA 100(1-\\alpha)\\% confidence region for \\boldsymbol \\mu_1 - \\boldsymbol \\mu_2 is given by all values of \\boldsymbol \\mu_1 - \\boldsymbol \\mu_2 that satisfy \n(\\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2 - (\\boldsymbol \\mu_1-\\boldsymbol \\mu_2))'\\left[\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right) S_{\\text{pool}}\\right]^{-1}(\\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2 - (\\boldsymbol \\mu_1-\\boldsymbol \\mu_2)) \\leq c^2.\n where c^2 is defined above.\n\n\n\n\nEigenvalues and eigenvectors of the pooled covariance matrix are\n\n\n\nR Code: Eigenvalues and Eigenvectors\neig_result = eigen(Sp)\nlambda1 = eig_result$values[1]\nnames(eig_result$values) = c(\"lambda1\", \"lambda2\")\n# Eigen values are \nprint(eig_result$values) \n\n\n lambda1  lambda2 \n5.302776 1.697224 \n\n\nR Code: Eigenvalues and Eigenvectors\n# Eigenvectors are \ncolnames(eig_result$vectors) = c(\"eigenvector 1\", \"eigenvector 2\")\nprint(eig_result$vectors)\n\n\n     eigenvector 1 eigenvector 2\n[1,]     0.2897841    -0.9570920\n[2,]     0.9570920     0.2897841\n\n\n\n\nR Code: Axes Lengths\n# semi-major axis and  semi-minor axis: \naxis=c(sqrt(eig_result$values[1]) * sqrt((1/n1+1/n2)*c2),\n       sqrt(eig_result$values[2]) * sqrt((1/n1+1/n2)*c2))\nnames(axis) = c(\"axis 1\", \"axis 2\")\nprint(axis)\n\n\n   axis 1    axis 2 \n1.1508432 0.6510797 \n\n\n\nThe 95% confidence ellipse for the difference between two population mean vectors\n\nis centered at \\bar{\\mathbf{x}}_1 - \\bar{\\mathbf{x}}_2\nextends \\sqrt{\\lambda_1} \\sqrt{(1/n_1+1/n_2)c^2} = 1.15 and \\sqrt{\\lambda_2} \\sqrt{(1/n_1+1/n_2)c^2} = 0.65 units in the first eigenvector and second eigenvector directions\n\n\n\n\n\n\n\n95% Confidence Ellipse for Mean Difference\n\n\n\n\n\nInterpretation: Because the origin \\mathbf{0} is not inside the ellipse, we conclude that the populations of soaps produced by the two processes are centered at different mean vectors. There appears to be no big difference in mildness means for soaps made by the two processes, but soaps made with the second process produce more lather on average.\n\n\n\n5.1.7 Simultaneous CIs\n\n\n\n\n\n\nSimultaneous CIs\n\n\n\nAs in one population case, we can obtain simultaneous confidence intervals for any linear combination of the components of \\boldsymbol \\mu_1 - \\boldsymbol \\mu_2.\n\nSuppose we are interested in a set of p simultaneous confidence intervals: \n\\mathbf{a}'_j(\\boldsymbol \\mu_1 - \\boldsymbol \\mu_2) =\n\\begin{bmatrix}\n0 & 0 & \\cdots & 1 & \\cdots & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mu_{11} - \\mu_{21} \\\\\n\\mu_{12} - \\mu_{22} \\\\\n\\vdots \\\\\n\\mu_{1p} - \\mu_{2p}\n\\end{bmatrix} = \\mu_{1j} - \\mu_{2j}\n where the vector \\mathbf{a}_j has zeros everywhere except for the one in the jth position.\nTypically, we would be interested in m such comparisons.\n\nSimultaneous T^2 CIs for \\mu_{1j} - \\mu_{2j}:\n\n(\\bar{x}_{1j} - \\bar{x}_{2j}) \\pm\n\\sqrt{\n\\frac{(n_1 + n_2 - 2)p}{n_1 + n_2 - p - 1} \\cdot F_{p,\\; n_1 + n_2 - p - 1;\\; 1 - \\alpha}\n}\n\\cdot\n\\sqrt{\n\\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right) \\cdot S_{\\text{pool},\\, jj}\n}\n * This interval will simultaneously cover the true values of \\mu_{1j} - \\mu_{2j} with confidence at least (1-\\alpha)\\times 100\\%.\nSimultaneous Bonferroni CIs for \\mu_{1j} - \\mu_{2j}: \n(\\bar{x}_{1j} - \\bar{x}_{2j}) \\pm t_{(n_1 + n_2 -2), 1-\\alpha/(2m)} \\sqrt{\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right) S_{\\text{pool}, jj}}\n\n\n\n\n\nR Code: T2 CIs\nse_j = sqrt((1/n1+1/n2)* diag(Sp))\n# T2 CIs\ncval_T2 = sqrt(c2)\n# Bonferroni CIs\nm=2 # only two variables\ncval_Bon = qt(1-alpha/(2*m), n1+n2-2) \n\nci_T2 &lt;- tibble(\n  Component = c(\"Lather\", \"Mildness\"),\n  Estimate = Delta,\n  HalfWidth = cval_T2 * se_j,\n  Lower = Estimate - HalfWidth,\n  Upper = Estimate + HalfWidth\n)\nprint(ci_T2)\n\n\n# A tibble: 2 × 5\n  Component Estimate HalfWidth  Lower Upper\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Lather      -1.90      0.707 -2.61  -1.19\n2 Mildness     0.200     1.12  -0.918  1.32\n\n\n\n\nR Code: Bonferroni CIs\nci_Bon &lt;- tibble(\n  Component = c(\"Lather\", \"Mildness\"),\n  Estimate = Delta,\n  HalfWidth = cval_Bon * se_j,\n  Lower = Estimate - HalfWidth,\n  Upper = Estimate + HalfWidth\n)\n\nprint(ci_Bon)\n\n\n# A tibble: 2 × 5\n  Component Estimate HalfWidth  Lower Upper\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Lather      -1.90      0.644 -2.54  -1.26\n2 Mildness     0.200     1.02  -0.818  1.22\n\n\n\n\n\n\n\n95% Confidence Ellipse with T^2 and Bonferroni CIs. Blue dashed line indicates the limits of T^2 CIs and darkgree dotted line indicates the limits of Bonferroni CIs.\n\n\n\n\nInterpretation: Based on simultaneous T2 CIs and Bonferroni CIs, the results confirm previous finding based on confidence ellipse that there appears to be no big difference in mildness means for soaps made by the two processes, but soaps made with the second process produce more lather on average. We also note that T2 CIs are wider than Bonferrorni CIs.\n\n\n\n\n\n\nOne-At-a-Time CI\n\n\n\n\n\nUsing the univariate approach, we can construct t-intervals for each of mean differences, and obtain the so-called one-at-a-time t intervals \n(\\bar{x}_{1j} - \\bar{x}_{2j}) \\pm t_{(n_1 + n_2 -2), 1-\\alpha/2} \\sqrt{\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right) S_{\\text{pool}, jj}}.\n The one-at-a-time CI does not control the family-wise error at \\alpha=0.05. As shown previously, the key difference between the Bonferroni CI and the one-at-a-time CI is that Bonferroni CI controls the family-wise error by assigning \\alpha/m level to each interval when there are m comparisons.\n\n\n\n\n\n5.1.8 Exercise: Steel Tube Data\n\n\n\n\n\n\nSteel Tube Data\n\n\n\nBackground: In steel manufacturing, the rolling temperature — the temperature at which steel is shaped into tubes — can affect the material’s strength profile. To study this, engineers measured the breaking strength (yield point (ksi)) and ultimate strength (ksi) of steel tubes produced at two different rolling temperatures, where 5 samples of steel are tested with low rolling temperature independently and 7 samples of steel are tested with high rolling temperature. The objective is to determine whether changing the rolling temperature results in a change in the strength profile (i.e., yield point and ultimate strength). Based on the following data, please solve the following problems:\n\n\nCode\nsteel = readr::read_csv(file = \"steel.csv\", show_col_types = FALSE) %&gt;% \n  mutate(temp = as.factor(temp))\n\nn1 &lt;- dim(steel[steel$temp==1, -1])[1]\nn2 &lt;- dim(steel[steel$temp==2, -1])[1]\np &lt;- dim(steel[steel$temp==2, -1])[2]\nhead(steel)\n\n\n\n\n  \n\n\n\n\n\n\nStep 1. Data visualization\n\n\n\nR Code: Long Format via pivot_longer\nlibrary(dplyr)\ndf_long = steel %&gt;% \n  pivot_longer(cols=c(2:3), \n               names_to=\"var\")\nhead(df_long)\n\n\n\n\n  \n\n\n\n\n\nR Code: One variable + multiple groups\nggplot(df_long, aes(x=temp, y=value)) + \n  geom_point() + \n  facet_wrap(~var, nrow=1, \n             scales=\"free_y\")\n\n\n\n\n\n\n\nR Code: One group + multiple variables\nggplot(steel) + \n  geom_point(aes(x=yield, y=strength, col=temp)) \n\n\n\n\n\nQuestion: What do the plots tell you?\n\nStep 2. State the research question(s) and define the null and alternative hypotheses using standard notations.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\nThe research questions for this data can be stated as follows:\n\nDoes the change in rolling temperature result in the change in strength profile of the steel - that is, does it affect either or both of yield point and ultimate strength?\nIf it does affect, which variable is more sensitive to the change in rolling temperature?\n\nBased on the research questions, our objectives are to examine the data and test the null hypothesis that the vectors of means for yield point and ultimate strength are the same for the two rolling temperatures used to produce this type of steel.\n\nLet x_1 denote the yield point and x_2 denote the ultimate strength.\nLet \\mathbf{x}_{1j}: =[x_{1j}, x_{2j}]^\\top denote the measured yield point and ultimate strength under low rolling temperature for j=1,\\ldots, n_1; and \\mathbf{x}_{2j}: =[x_{1j}, x_{2j}]^\\top under high rolling temperature for j=1,\\ldots, n_2.\nLet \\boldsymbol \\mu_1, \\boldsymbol \\mu_2 be the population mean vectors of yield point and ultimate strength under low and high rolling temperature respectively: \n\\boldsymbol \\mu_1: =[\n\\text{Mean Yield}_1, \\text{Mean Strength}_1]^\\top, \\quad\n\\boldsymbol \\mu_2: =[\n\\text{Mean Yield}_2, \\text{Mean Strength}_2]^\\top\n or more precisely, \\boldsymbol \\mu_1 = E(\\mathbf{x}_{1j}) and \\boldsymbol \\mu_2 = E(\\mathbf{x}_{2j}).\n\nThe null and alternative hypothesese are given as follows \nH_0: \\boldsymbol \\mu_1 = \\boldsymbol \\mu_2 \\quad \\text{v.s.}\\quad\nH_1: \\boldsymbol \\mu_1 \\neq \\boldsymbol \\mu_2\n\n\n\n\n\nStep 3. Perform the statistical test.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\nStep 3(a): Compute summary statistics\n\n\n\nCode\n#  Compute sample mean vector and sample\n(xbar1 = sapply(steel[steel$temp==1, -1], mean))\n\n\n   yield strength \n    36.4     62.6 \n\n\nCode\n#  covariance matrix for each temperature\n(xvar1 = var(steel[steel$temp==1 , -1]))\n\n\n         yield strength\nyield      7.3      4.2\nstrength   4.2      4.3\n\n\nCode\n#  Compute sample mean vector and sample\n(xbar2 = sapply(steel[steel$temp==2, -1], mean))\n\n\n   yield strength \n39.00000 60.42857 \n\n\nCode\n#  covariance matrix for each temperature\n(xvar2 = var(steel[steel$temp==2 , -1]))\n\n\n            yield strength\nyield    8.333333 6.666667\nstrength 6.666667 7.619048\n\n\n\nStep 3(b): Check Normality Assumption\n\n\n\nCode\n# check univariate normaltiy \nfor(i in 1:p){\n  apply(steel[steel$temp == i, -1], 2, shapiro.test)\n}\n\n# check bivariate normaltiy\nmvShapiroTest::mvShapiro.Test(as.matrix(steel[ , 2:3]))\n\n\n\n    Generalized Shapiro-Wilk test for Multivariate Normality by\n    Villasenor-Alva and Gonzalez-Estrada\n\ndata:  as.matrix(steel[, 2:3])\nMVW = 0.9584, p-value = 0.8718\n\n\n\nStep 3(c): Check Homogeneity Assumption\n\n\n\nCode\n# Apply Box's M-test to test the null hypothesis of homogeneous covariance matrices. \nbiotools::boxM(steel[ , -1], steel$temp)\n\n\n\n    Box's M-test for Homogeneity of Covariance Matrices\n\ndata:  steel[, -1]\nChi-Sq (approx.) = 0.38077, df = 3, p-value = 0.9442\n\n\n\nStep 3(d): Two-Sample Hotelling’s T^2 Test\n\n\n\nCode\nT2result &lt;- DescTools::HotellingsT2Test(steel[steel$temp == 1, -1], \n             steel[steel$temp == 2, -1])\n\nT2result\n\n\n\n    Hotelling's two sample T2-test\n\ndata:  steel[steel$temp == 1, -1] and steel[steel$temp == 2, -1]\nT.2 = 10.76, df1 = 2, df2 = 9, p-value = 0.004106\nalternative hypothesis: true location difference is not equal to c(0,0)\n\n\n\n\n\n\nStep 4. Construct the confidence region and interpret the results.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\n# sample difference\nDelta = xbar1 - xbar2\n\n# compute pooled covariance matrix \nSp &lt;- ((n1-1)*xvar1 +(n2-1)*xvar2)/(n1+n2-2)\n\n# compute T2 statistic\nT2 = drop(t(Delta)%*%solve((1/n1+1/n2) * Sp)%*%Delta)\nprint(T2)\n\n\n[1] 23.91171\n\n\nCode\n# critical value\nalpha = 0.05\nc2 = (n1+n2-2)*p / (n1+n2-p-1) * qf(1-alpha, p, n1+n2-p-1)\n\n# this is the covariance of Delta \nSigma_ell &lt;-   (1/n1 + 1/n2) * Sp\neig &lt;- eigen(Sigma_ell)\nA &lt;- eig$vectors %*% diag(sqrt(eig$values)) * sqrt(c2)\n\ntheta &lt;- seq(0, 2*pi, length.out = 400)\npts &lt;- t(matrix(Delta, nrow = 2, ncol = length(theta)) + \n           A %*% rbind(cos(theta), sin(theta)))\n\ndf_ell &lt;- as.data.frame(pts)\ncolnames(df_ell) &lt;- c(\"Yield\", \"Strength\")\ncenter &lt;- data.frame(Yield = Delta[1], Strength = Delta[2])\n\ngg &lt;- ggplot() +\n  geom_path(data = df_ell, aes(Yield, Strength)) +\n  geom_point(data = center, aes(Yield, Strength), color = \"red\", size = 3) +\n  coord_equal() +\n  theme_minimal() + \n  labs(x=\"Yield Difference\", y=\"Strength Difference\")\n\nprint(gg)\n\n\n\n\n\n95% Confidence region for population mean difference \\boldsymbol \\mu_1 - \\boldsymbol \\mu_2\n\n\n\n\nInterpretation:\nThe 95\\% confidence ellipse for the difference between two population mean vectors\n\nis centered at -2.6, 2.17,\nand extends 6.45 and 2.11 units in the first and second eigenvectors directions.\n\nSince the origin 0 does not fall into the 95\\% confidence ellipse, we conclude that the change in rolling temperature indeed affect the strength profile significantly at \\alpha=0.05, which is consistent with the Hotelling’s T^2 test. However, at this point we do not know whether individual variable would contribute to this change along.\n\n\n\n\nStep 5. Construct simultaneous T^2 and Bonferroni CIs and interpret the results.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\n# confidence level\nlevel &lt;- 0.95\n\nse_j = sqrt(diag(Sp) * (1/n1 + 1/n2))\n\n# Compute degrees of freedom and the multipliers\ndf1 &lt;- p\ndf2 &lt;- n1+n2-p-1\ndf3 &lt;- n1+n2-2\n\ncval_T2 &lt;-  sqrt((n1+n2-2)*p*qf(level,df1,df2)/(n1+n2-p-1))\n\nm = p \nlevel2 &lt;- 1-(1-level)/(2*m)\ncval_Bon  &lt;- qt(level2, df3)\n\nci_T2 &lt;- tibble(\n  Component = c(\"Yield\", \"Strength\"),\n  Estimate = Delta,\n  Lower = Estimate - cval_T2 * se_j,\n  Upper = Estimate + cval_T2 * se_j\n)\ncat(\"T2 CI:\")\n\n\nT2 CI:\n\n\nCode\nprint(ci_T2)\n\n\n# A tibble: 2 × 4\n  Component Estimate Lower Upper\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Yield        -2.6  -7.67  2.47\n2 Strength      2.17 -2.35  6.69\n\n\nCode\nci_Bon &lt;- tibble(\n  Component = c(\"Yield\", \"Strength\"),\n  Estimate = Delta,\n  Lower = Estimate - cval_Bon * se_j,\n  Upper = Estimate + cval_Bon * se_j\n)\n\ncat(\"Bonferroni CI:\")\n\n\nBonferroni CI:\n\n\nCode\nprint(ci_Bon)\n\n\n# A tibble: 2 × 4\n  Component Estimate Lower Upper\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Yield        -2.6  -6.94  1.74\n2 Strength      2.17 -1.70  6.04\n\n\nInterpretation: The simultaneous T^2 CIs and Bonferroni CIs do include the origin for each population mean parameter. While we get two seemingly contradictory results, they are not wrong. The key reason is that the (joint) confidence region obtained above effectively takes into account the correlation between these two variables: yield point and ultimate strength. From the scatter plot between these two variables, we also notice that there is a strong positive correlation under both rolling temperature. Thus, it is possible that (0,0) is outside the 95\\% confidence ellipse but all individual intervals would contain 0, indicating non-significant difference for individual variables (as these variables are highly correlated.) A powerful and correct way for multivariate test is to use the Hotelling’s T^2 test or joint confidence region to detect a difference that the individual simultaneous intervals would miss."
  },
  {
    "objectID": "ch5/05-Inference_for_Means_II.html#comparing-multiple-mean-vectors",
    "href": "ch5/05-Inference_for_Means_II.html#comparing-multiple-mean-vectors",
    "title": "5  Inference for Multiple Mean Vectors",
    "section": "5.2 Comparing Multiple Mean Vectors",
    "text": "5.2 Comparing Multiple Mean Vectors\nIn multivariate analysis, we often want to test whether several groups have the same mean vector for multiple variables. This is the multivariate extension of one-way ANOVA: Multivariate Analysis of Variance (MANOVA). We can extend the comparison of mean vectors to g different groups (or treatments) or populations for p responses.\n\n\n\n\n\n\nKey Assumptions\n\n\n\nThe following assumptions are needed to make inferences about the difference between any two population mean vectors: \\boldsymbol \\mu_{\\ell_1} - \\boldsymbol \\mu_{\\ell_2} for \\ell_1\\neq \\ell_2:\n\nEach observation vector sampled from the \\ell-th population (or group) follows a multivariate normal distribution: for \\ell=1,\\ldots g, \n\\mathbf{x}_{\\ell 1}, \\mathbf{x}_{\\ell 2}, \\dots, \\mathbf{x}_{\\ell n_\\ell} \\overset{ind}{\\sim} N_p(\\boldsymbol{\\mu}_\\ell, {\\Sigma}).\n\nCovariance matrices are homogeneous: \\Sigma_\\ell = \\Sigma for every population.\nObservations from one population (or group) is independent of any observations from other populations.\n\n\n\n\n5.2.1 Example: Iris Data\nBackground\nA botanist wants to determine if the three species of iris flowers (setosa, versicolor, and virginica) have different overall morphologies. Instead of just looking at one measurement, they want to compare the species based on a complete profile of all four available measurements: Sepal.Length, Sepal.Width, Petal.Length, and Petal.Width from the iris data.\nBecause we are comparing a vector of mean responses across more than two groups, this is a classic problem for Multivariate Analysis of Variance (MANOVA).\n\nGroups (g=3): setosa, versicolor, virginica\nResponse Variables (p=4): Sepal.Length, Sepal.Width, Petal.Length, Petal.Width\nResearch Question: Are the mean vectors of these four characteristics the same across all three species?\n\n\n\n5.2.2 Load and Visualize the Data\nBefore testing, it is crucial to visualize the data. A pairs plot is excellent for this, as it shows the relationship between all variables for each species.\n\n\nR Code: Between Group plots\nlibrary(dplyr)\nlibrary(ggplot2)\ndata(iris)\n#head(iris)\ndf = iris\n\ng = length(levels(df$Species))\np = 4\n\ndf_long = df %&gt;% \n  pivot_longer(cols=c(1:4), \n               names_to=\"var\")\ng1 = ggplot(df_long) + \n  geom_point(aes(x=Species, y=value), \n             size=.8, alpha=.8) + \n  facet_wrap(~var, scales=\"free_y\")\nprint(g1)\n\n\n\n\n\nInterpretation: These panels suggest that there is a moderate variation across different species (groups) for each of the variables. Within each species (group), the observations also indicates some variations.\n\n\nR Code: Between Group boxplots\ng2 = ggplot(df_long, aes(x=Species, y=value)) + \n  geom_boxplot(fill = 'skyblue') + \n  #geom_jitter(width = 0.1) + \n  facet_wrap(~var, scales=\"free_y\") + \n  labs(y=\"\")\nprint(g2)\n\n\n\n\n\nInterpretation: All the variables across different species roughly follow symmetric distributions, except for the Petal.Width from the Setosa, whose distribution seems to be highly skewed to the right.\n\n\nR Code: Pairwise Scatterplot\nlibrary(GGally)\n\n# Create a pairs plot, colored by Species\nggpairs(\n  iris,\n  columns = 1:4,\n  ggplot2::aes(color = Species)\n) +\nlabs(title = \"Pairs Plot of Iris Measurements by Species\") +\ntheme_bw()\n\n\n\n\n\nInterpretation: The plot shows some separation among the three species, especially for the petal measurements. There are strong linear associations between Petal.Length and Petal.Width among all the three species. In general, there is also a linear association between Sepal.Length and Petal.Length, between Septal.Length and Petal.Width.\n\n\n5.2.3 State the Hypotheses\nFormulate the null and alternative hypotheses for the MANOVA test.\n\n\n\n\n\n\nHypotheses\n\n\n\n\n\nThe null hypothesis states that the true mean vectors for the full morphology profile are identical for all three species. The alternative hypothesis states that at least two of the species have different mean vectors.\n\nNull Hypothesis (H_0):  H_0: \\boldsymbol{\\mu}_{\\text{setosa}} = \\boldsymbol{\\mu}_{\\text{versicolor}} = \\boldsymbol{\\mu}_{\\text{virginica}} \nAlternative Hypothesis (H_1):  H_1: \\text{At least one } \\boldsymbol{\\mu}_{k} \\neq \\boldsymbol{\\mu}_{\\ell} \\text{ for } k \\neq \\ell \n\n\n\n\n\n\n5.2.4 Check Assumptions\nMANOVA relies on two key assumptions: multivariate normality within each group and the homogeneity of their covariance matrices.\n\n\nR Code: Normality Checks\n# check univariate normaltiy \niris %&gt;%\n  pivot_longer(\n    cols = 1:4, \n    names_to = \"Variable\", \n    values_to = \"Value\"\n  ) %&gt;%\n  # Group by both Species and the new Variable column\n  group_by(Species, Variable) %&gt;%\n  # Run the shapiro.test for each group\n  dplyr::summarise(\n    p_value = stats::shapiro.test(Value)$p.value, \n    .groups = \"drop\" \n  )\n\n\n\n\n  \n\n\n\nR Code: Normality Checks\n# check multivariate normaltiy\nmntest = c()\nfor(i in levels(iris$Species)){\nmntest[i] = mvShapiroTest::mvShapiro.Test(\n  as.matrix(iris[iris$Species == i, -5]))$p.value\n}\n# p values: \nprint(mntest)\n\n\n    setosa versicolor  virginica \n 0.0120325  0.3182897  0.9652400 \n\n\n\n\nR Code: Test Homogeneity of Covariance Matrices\n# Note: Box's M-test is very sensitive, especially with larger datasets.\nbiotools::boxM(iris[, 1:4], iris$Species)\n\n\n\n    Box's M-test for Homogeneity of Covariance Matrices\n\ndata:  iris[, 1:4]\nChi-Sq (approx.) = 140.94, df = 20, p-value &lt; 2.2e-16\n\n\nInterpretation:\n\nNormality: The p-values for all three species are relatively large (&gt;0.01), indicating that the data within each group are roughly consistent with a multivariate normal distribution.\nBox’s M-Test: The p-value is very small (p &lt; 0.001), indicating that the assumption of equal covariance matrices is violated. However, MANOVA is generally robust to this violation when group sizes are equal (as they are here, n=50 for each), so we can proceed, but we should acknowledge this limitation in a formal report.\n\n\n\n5.2.5 One-Way MANOVA\nWe will now perform the MANOVA to formally test the null hypothesis. The most common test statistic is Wilk’s \\Lambda, which essentially compares the variability within groups to the total variability. Small values of Wilk’s \\Lambda suggest that the group means are different.\nThe one-way MANOVA model is given by: \n\\mathbf{x}_{\\ell j} = \\boldsymbol{\\mu} + \\boldsymbol{\\tau}_{\\ell} + \\boldsymbol{\\epsilon}_{\\ell j}, \\quad \\ell=1,\\ldots, g; \\quad j=1,\\ldots, n_{\\ell}\n where\n\n\\boldsymbol{\\tau}_{\\ell} represents the treatment (group) effect with the constraint that \\sum_{\\ell=1}^{g} n_{\\ell} \\boldsymbol{\\tau}_{\\ell} = \\mathbf{0},\nthe error terms are independently distributed as \\boldsymbol{\\epsilon}_{\\ell j} \\sim N_p(\\mathbf{0}, {\\Sigma}),\n\\boldsymbol \\mu is the overall (or grand) mean across all populations, and \\boldsymbol \\mu_{\\ell} := \\boldsymbol \\mu + \\boldsymbol{\\tau}_{\\ell} is the group mean for the \\ellth population.\n\n\n\n\n\n\n\nMANOVA Table\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource of Variation\nMatrix of Sums of Squares and Cross-Products (SSP)\nDegrees of Freedom\n\n\n\n\nTreatment\nB = \\sum_{\\ell} n_\\ell (\\bar{\\mathbf{x}}_\\ell - \\bar{\\mathbf{x}})(\\bar{\\mathbf{x}}_\\ell - \\bar{\\mathbf{x}})'\ng - 1\n\n\nResidual\nW = \\sum_{\\ell} \\sum_{j} (\\mathbf{x}_{\\ell j} - \\bar{\\mathbf{x}}_\\ell)(\\mathbf{x}_{\\ell j} - \\bar{\\mathbf{x}}_\\ell)'\nn - g\n\n\nTotal (Corrected)\nB + W = \\sum_{\\ell} \\sum_{j} (\\mathbf{x}_{\\ell j} - \\bar{\\mathbf{x}})(\\mathbf{x}_{\\ell j} - \\bar{\\mathbf{x}})'\nn - 1\n\n\n\n\nThe within-group SSP matrix, \\mathbf{W}, can be expressed as: \\begin{align*}\n\\mathbf{W} &= \\sum_{\\ell = 1}^g \\sum_{j = 1}^{n_\\ell} (\\mathbf{x}_{\\ell j} - \\bar{\\mathbf{x}}_\\ell)(\\mathbf{x}_{\\ell j} - \\bar{\\mathbf{x}}_\\ell)' \\\\\n&= (n_1 - 1)S_1 + (n_2 - 1) S_2 + \\cdots + (n_g - 1) S_g \\\\\n&= (n-g) S_{\\text{pool}}\n\\end{align*} where n = \\sum_{\\ell=1}^g n_\\ell.\nThe pooled covariance matrix is calculated as: \nS_{\\text{pool}} = \\sum_{\\ell = 1}^g \\left[\\frac{(n_\\ell - 1)}{\\sum_{j=1}^g (n_j - 1)}\\right] S_\\ell.\n\n\n\n\n\n\n\n\n\n\n\nWilk’s \\Lambda Test Statistic\n\n\n\n\n\n\nOne test of the null hypothesis is carried out using a statistic called Wilk’s \\Lambda (a likelihood ratio test): \n\\Lambda = \\frac{|W|}{|B + W|}.\n\nIf B is “small” relative to W, then \\Lambda will be close to 1. Otherwise, \\Lambda will be small.\nWe reject the null hypothesis when \\Lambda is small.\nExact Distribution of Wilk’s \\Lambda\n\n\n\n\n\n\n\n\n\nNo. of Variables (p)\nNo. of Groups (g)\nSampling Distribution for Multivariate Normal Data\n\n\n\n\np = 1\ng \\geq 2\n\\left(\\frac{n - g}{g - 1}\\right)\\left(\\frac{1 - \\Lambda}{\\Lambda}\\right) \\sim F_{g - 1,\\ n - g}\n\n\np = 2\ng \\geq 2\n\\left(\\frac{n - g - 1}{g - 1}\\right)\\left(\\frac{1 - \\sqrt{\\Lambda}}{\\sqrt{\\Lambda}}\\right) \\sim F_{2(g - 1),\\ 2(n - g - 1)}\n\n\np \\geq 1\ng = 2\n\\left(\\frac{n - p - 1}{p}\\right)\\left(\\frac{1 - \\Lambda}{\\Lambda}\\right) \\sim F_{p,\\ n - p - 1}\n\n\np \\geq 1\ng = 3\n\\left(\\frac{n - p - 2}{p}\\right)\\left(\\frac{1 - \\sqrt{\\Lambda}}{\\sqrt{\\Lambda}}\\right) \\sim F_{2p,\\ 2(n - p - 2)}\n\n\n\n\nF Approximation to the Sampling Distribution Wilk’s \\Lambda\n\nWhen the null hypothesis of equal population mean vectors is true, the distribution of the Wilks’ Lambda statistic can be approximated by an F-distribution: \n  \\frac{ 1 - \\Lambda^{1/b}}{\\Lambda^{1/b}} \\cdot \\frac{ ab - c}{p(g - 1)} \\sim F_{p(g - 1),\\ ab - c}\n where \\begin{align*}\n  a &= (n - g) - \\frac{p - g + 2}{2} \\\\\n  b &= \\sqrt{ \\frac{p^2 (g - 1)^2 - 4}{p^2 + (g - 1)^2 - 5} } \\\\\n  c &= \\frac{p(g - 1) - 2}{2}\n\\end{align*}\n\n\n\n\n\nR Code: MANOVA Test\n# The manova() function fits the model.\n# The formula cbind(Y1, Y2, Y3, Y4) ~ Group tells R to use all four\n# measurements as the multivariate response vector.\nmanova_fit &lt;- manova(\n  cbind(Sepal.Length, Sepal.Width, \n        Petal.Length, Petal.Width) \n  ~ Species, \n  data = iris)\n\n# The summary() function generates the output from the statistical test.\n# We specify test = \"Wilks\" to get the result for Wilk's Lambda.\n# Other options include \"Pillai\", \"Hotelling-Lawley\", and \"Roy\".\nsummary(manova_fit, test = \"Wilks\")\n\n\n           Df    Wilks approx F num Df den Df    Pr(&gt;F)    \nSpecies     2 0.023439   199.15      8    288 &lt; 2.2e-16 ***\nResiduals 147                                              \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInterpret the MANOVA Results\nBased on the F-statistic and p-value from the test, what is your conclusion?\n\n\n\n\n\n\nInterpretation\n\n\n\n\n\nThe p-value is reported as &lt; 2.2e-16, which is exceptionally small. We reject the null hypothesis at significance level 0.05.\nConclusion: There is a statistically significant difference in the overall morphology (the mean vector of the four measurements) among the three iris species.\n\n\n\n\n\n5.2.6 Pairwse Comparison\nA significant MANOVA result tells us that a difference exists, but not where that difference lies. We need to perform follow-up tests to understand the result more deeply.\n\n\n\n\n\n\nMethod 1: Univariate ANOVA\n\n\n\n\n\nA simple first step is to look at the results for each response variable individually to see which ones are contributing to the overall difference.\n\n\nR Code: Univariate Follow-up\n# The summary.aov() function provides the results \n# for each dependent variable separately.\nsummary.aov(manova_fit)\n\n\n Response Sepal.Length :\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies       2 63.212  31.606  119.26 &lt; 2.2e-16 ***\nResiduals   147 38.956   0.265                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response Sepal.Width :\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies       2 11.345  5.6725   49.16 &lt; 2.2e-16 ***\nResiduals   147 16.962  0.1154                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response Petal.Length :\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies       2 437.10 218.551  1180.2 &lt; 2.2e-16 ***\nResiduals   147  27.22   0.185                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response Petal.Width :\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies       2 80.413  40.207  960.01 &lt; 2.2e-16 ***\nResiduals   147  6.157   0.042                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInterpretation: The univariate ANOVAs show extremely small p-values for all four variables (Sepal.Length, Sepal.Width, Petal.Length, and Petal.Width). This indicates that the group means are significantly different for every measurement when considered one at a time.\n\n\n\n\n\n\n\n\n\nMethod 2: Pairwise Group Comparisons\n\n\n\nTo find out which specific species are different from each other, we could perform pairwise comparisons. This could involve running two-sample Hotelling’s T² tests for each pair (e.g., setosa vs. versicolor) with a Bonferroni correction to the alpha level to control for multiple comparisons. Given the clear separation in the plots, we can be confident that all three species are significantly different from one another on their overall morphology profile.\n\nIf we wish to carry out all pairwise comparisons, there will be p g (g - 1)/2 of them.\nTo maintain a simultaneous type I error level of no more than \\alpha we can use \nt_{(n-g), 1-\\frac{\\alpha}{2m}} \\quad \\text{ where } \\quad  m = \\frac{pg(g-1)}{2}.\n\nFormulas for the simultaneous Bonferroni CIs are \n\\left( \\bar{x}_{ik} - \\bar{x}_{i \\ell} \\right)  \\ \\pm \\  t_{(n-g), 1-\\frac{\\alpha}{2m}} \\sqrt{\\left(\\frac{1}{n_k} + \\frac{1}{n_\\ell}\\right) S_{pool,ii}}\n\n\n\n\n\n\nR Code: Bonferroni CIs\nlibrary(dplyr)\nn = iris %&gt;%\n  count(Species) %&gt;%\n  pull(n, name = Species)\n\np = 4 # variables\ng = length(levels(iris$Species))\n\nlevel &lt;- 0.95\n\nm &lt;- p * g * (g - 1) / 2\nlevel2 &lt;- 1 - (1 - level) / (2 * m)\ndf &lt;- sum(n) - g\nc_bon  &lt;- qt(level2, df)\n\n# compute pooled covariance matrix\nSp = summary(manova_fit)$SS$Residuals / df\nSp_ii = diag(Sp)\n\n# sample mean\nxbar = iris %&gt;%\n  group_by(Species) %&gt;%\n  dplyr::summarise(across(where(is.numeric), mean)) %&gt;%\n  as.matrix()\n\n# Get group sample sizes (n_k) and means (x_bar_k)\niris_summaries &lt;- iris %&gt;%\n  group_by(Species) %&gt;%\n  dplyr::summarise(across(where(is.numeric), \n                          list(mean = mean, n = ~ n())), \n                   .groups = \"drop\")\n\n# Get all unique pairs of species\nspecies_pairs &lt;- combn(unique(iris$Species), \n                       2, simplify = FALSE)\n# Use purrr::map_df to loop through pairs \nall_intervals &lt;- map_df(species_pairs, function(pair) {\n  species1 &lt;- pair[1]\n  species2 &lt;- pair[2]\n  \n  # Get means and sample sizes for the two species being compared\n  summary1 &lt;- iris_summaries %&gt;% \n    filter(Species == species1)\n  summary2 &lt;- iris_summaries %&gt;% \n    filter(Species == species2)\n  \n  n1 &lt;- summary1$Sepal.Length_n\n  n2 &lt;- summary2$Sepal.Length_n\n  \n  mean_diffs &lt;- as.numeric(\n    dplyr::select(summary1, \n                  ends_with(\"_mean\"))) -\n    as.numeric(\n      dplyr::select(summary2, \n                    ends_with(\"_mean\")))\n  \n  # Calculate margin of error for this pair\n  margin_of_error &lt;- c_bon * \n    sqrt((1 / n1 + 1 / n2) * Sp_ii)\n  \n  # Create a tibble for this pair's results\n  tibble(\n    Comparison = paste(species1, \"vs.\", species2),\n    Variable = names(Sp_ii),\n    Mean_Difference = mean_diffs,\n    Lower_CI = mean_diffs - margin_of_error,\n    Upper_CI = mean_diffs + margin_of_error\n  )\n})\n\nknitr::kable(\n  all_intervals, digits = 3, \n  caption = \"Simultaneous 95% Bonferroni Confidence Intervals\")\n\n\n\nSimultaneous 95% Bonferroni Confidence Intervals\n\n\n\n\n\n\n\n\n\nComparison\nVariable\nMean_Difference\nLower_CI\nUpper_CI\n\n\n\n\nsetosa vs. versicolor\nSepal.Length\n-0.930\n-1.230\n-0.630\n\n\nsetosa vs. versicolor\nSepal.Width\n0.658\n0.460\n0.856\n\n\nsetosa vs. versicolor\nPetal.Length\n-2.798\n-3.049\n-2.547\n\n\nsetosa vs. versicolor\nPetal.Width\n-1.080\n-1.199\n-0.961\n\n\nsetosa vs. virginica\nSepal.Length\n-1.582\n-1.882\n-1.282\n\n\nsetosa vs. virginica\nSepal.Width\n0.454\n0.256\n0.652\n\n\nsetosa vs. virginica\nPetal.Length\n-4.090\n-4.341\n-3.839\n\n\nsetosa vs. virginica\nPetal.Width\n-1.780\n-1.899\n-1.661\n\n\nversicolor vs. virginica\nSepal.Length\n-0.652\n-0.952\n-0.352\n\n\nversicolor vs. virginica\nSepal.Width\n-0.204\n-0.402\n-0.006\n\n\nversicolor vs. virginica\nPetal.Length\n-1.292\n-1.543\n-1.041\n\n\nversicolor vs. virginica\nPetal.Width\n-0.700\n-0.819\n-0.581\n\n\n\nSimultaneous 95% Bonferroni CIs. Intervals not crossing the dashed line (zero) are statistically significant.\n\n\nR Code: Bonferroni CIs\nggplot(all_intervals,\n       aes(x = Mean_Difference, y = Variable, \n           color = Variable)) +\n  geom_errorbar(aes(xmin = Lower_CI, xmax = Upper_CI),\n                 width = 0.2,\n                 linewidth = 1) +\n  geom_point(size = 3.5) +\n  geom_vline(xintercept = 0,\n             linetype = \"dashed\",\n             color = \"black\") +\n  facet_wrap( ~ Comparison) +\n  labs(\n    x = \"Mean Difference\",\n    y = \"Variable\"\n  ) +\n  theme_bw(base_size = 12) +\n  theme(legend.position = \"none\",\n        axis.text.y = element_text(angle=60)) \n\n\n\n\n\nSimultaneous 95% Bonferroni CIs. Intervals not crossing the dashed line (zero) are statistically significant.\n\n\n\n\nInterpretation: This visualization makes the conclusions from our analysis immediately obvious:\n\nNo intervals cross the zero line. Every single error bar for every comparison is clearly to the left or right of the vertical dashed line.\nThis provides powerful visual evidence that, after controlling for all 12 comparisons, all three iris species are significantly different from each other on all four measured variables. For example, in the versicolor v.s. virginica panel, the point for Petal.Length is around -1.3, and its confidence interval from approximately -1.6 to -1.0 is far from zero.\nThe simultaneous Bonferroni CIs confirm previous MANOVA test and provide more details on which variables are significantly different.\n\n\n\n5.2.7 Exercise: College Student Study\nBackground: In a college student study, a sample of first-year university students was selected from three popular and critical fields of study. Each student was administered a standardized academic assessment battery upon entry. The goal is to see if the overall academic profile differs significantly among these groups. Perform detailed statistical analysis for the data below.\n\nmorel = readr::read_csv(file = \"morel.csv\", \n                        show_col_types = FALSE) %&gt;% \n  mutate(group = as.factor(group))\nhead(morel)\n\n\n\n  \n\n\n\n\nData visualization\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nR Code: Data Visualization\ndf = morel \nresponse_vars = setdiff(colnames(df), \"group\")\n\np = length(response_vars)\ng = length(levels(df$group))\n\ndf_long = df %&gt;%\n  pivot_longer(cols=c(2:5),\n               names_to=\"var\")\nggplot(df_long, aes(x=group, y=value)) + \n  geom_point() + \n  facet_wrap(~var, scales=\"free_y\") + \n  labs(y=\"\")\n\n\n\n\n\nR Code: Data Visualization\nGGally::ggpairs(\n  df,\n  columns = 2:5, # The four test score variables\n  ggplot2::aes(color = group),\n  upper = list(continuous = \"cor\"),\n  lower = list(continuous = \"points\")\n) +\nlabs(title = \"Academic Score Profiles by Field of Study\") +\ntheme_bw()\n\n\n\n\n\nThe plot suggests there may be differences. For example, the distribution of math scores for Architecture students appears shifted compared to the other groups.\n\n\n\n\nState the hypotheses using standard notations.\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\nThe null hypothesis states that the true mean vectors of test scores are the same for all three student populations. The alternative states that at least two groups have different mean vectors.\n\nNull Hypothesis (H_0):  H_0: \\boldsymbol{\\mu}_{\\text{Technology}} = \\boldsymbol{\\mu}_{\\text{Architecture}} = \\boldsymbol{\\mu}_{\\text{Medical Tech}} \nAlternative Hypothesis (H_1):  H_1: \\text{At least one } \\boldsymbol{\\mu}_{k} \\neq \\boldsymbol{\\mu}_{\\ell} \\text{ for } k \\neq \\ell \n\n\n\n\n\nCheck Assumptions\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\n# check univariate normaltiy \ndf %&gt;%\n  pivot_longer(\n    cols = 2:5, \n    names_to = \"Variable\", \n    values_to = \"Value\"\n  ) %&gt;%\n  group_by(group, Variable) %&gt;%\n  dplyr::summarise(\n    p_value = stats::shapiro.test(Value)$p.value, \n    .groups = \"drop\" \n  )\n\n\n\n\n  \n\n\n\nCode\n# check multivariate normaltiy\nmntest = c()\nfor(i in levels(df$group)){\nmntest[i] = mvShapiroTest::mvShapiro.Test(\n  as.matrix(df[df$group == i, colnames(df) !=\"group\"]))$p.value\n}\n# p values: \nprint(mntest)\n\n\n         1          2          3 \n0.00388046 0.00389287 0.01745476 \n\n\nCode\nbiotools::boxM(df[, colnames(df) !=\"group\"], df$group)\n\n\n\n    Box's M-test for Homogeneity of Covariance Matrices\n\ndata:  df[, colnames(df) != \"group\"]\nChi-Sq (approx.) = 33.493, df = 20, p-value = 0.02977\n\n\n\n\n\n\nPerform the one-way MANOVA test\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\n# fit one-way ANOVA to each of the response\nformula = paste(\"cbind(\", \n                paste(response_vars, collapse = \", \"), \n                \") ~ group\")\n\nfit.lm = manova(as.formula(formula), data = df)\n\n# fit MANOVA \nfit.manova = manova(fit.lm)\nsummary(fit.manova, test=\"Wilks\")\n\n\n          Df   Wilks approx F num Df den Df    Pr(&gt;F)    \ngroup      2 0.54345   6.7736      8    152 1.384e-07 ***\nResiduals 79                                             \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInterpretation: The p-value is less than our significance level of \\alpha = 0.05. We therefore reject the null hypothesis. We conclude that there is a statistically significant difference in the mean academic profiles among the three groups of students (Technology, Architecture, and Medical Technology).\n\n\n\n\nFollow-up analysis with pairwise comparisons\n\n\n\n\n\n\n\nView Solution\n\n\n\n\n\n\n\nCode\nn = df %&gt;%\n  count(group) %&gt;%\n  pull(n, name = group)\n\nlevel = 0.95\nm = p * g * (g - 1) / 2\nlevel2 = 1 - (1 - level) / (2 * m)\ndof = sum(n) - g\nc_bon  = qt(level2, dof)\n\n# compute pooled sample covariance\nSp = summary(fit.manova)$SS$Residuals / dof\nSp_ii = diag(Sp)\n\ndf_summaries &lt;- df %&gt;%\n  group_by(group) %&gt;%\n  dplyr::summarise(across(where(is.numeric), list(mean = mean, n = ~ n())), .groups = \"drop\")\n\n# Get all unique pairs\ngroup_pairs &lt;- combn(unique(df$group), 2, simplify = FALSE)\nall_intervals &lt;- purrr::map_df(group_pairs, function(pair) {\n  group1 &lt;- pair[1]\n  group2 &lt;- pair[2]\n  \n  summary1 &lt;- df_summaries %&gt;% filter(group == group1)\n  summary2 &lt;- df_summaries %&gt;% filter(group == group2)\n  \n  n1 &lt;- summary1$math_n\n  n2 &lt;- summary2$math_n\n  \n  mean_diffs &lt;- as.numeric(dplyr::select(summary1, ends_with(\"_mean\"))) -\n    as.numeric(dplyr::select(summary2, ends_with(\"_mean\")))\n  \n  margin_of_error &lt;- c_bon * sqrt((1 / n1 + 1 / n2) * Sp_ii)\n  \n  tibble(\n    Comparison = paste(group1, \"vs.\", group2),\n    Variable = names(Sp_ii),\n    Mean_Difference = mean_diffs,\n    Lower_CI = mean_diffs - margin_of_error,\n    Upper_CI = mean_diffs + margin_of_error\n  )\n})\n\nknitr::kable(all_intervals, digits = 3, caption = \"Simultaneous 95% Bonferroni Confidence Intervals\")\n\n\n\nSimultaneous 95% Bonferroni Confidence Intervals\n\n\nComparison\nVariable\nMean_Difference\nLower_CI\nUpper_CI\n\n\n\n\n1 vs. 2\naptitude\n-28.158\n-48.736\n-7.580\n\n\n1 vs. 2\nmath\n-3.793\n-14.388\n6.802\n\n\n1 vs. 2\nlanguage\n-6.681\n-12.732\n-0.629\n\n\n1 vs. 2\ngen_know\n-2.309\n-9.669\n5.051\n\n\n1 vs. 3\naptitude\n11.571\n-11.938\n35.081\n\n\n1 vs. 3\nmath\n9.296\n-2.808\n21.400\n\n\n1 vs. 3\nlanguage\n2.466\n-4.448\n9.380\n\n\n1 vs. 3\ngen_know\n-7.973\n-16.382\n0.436\n\n\n2 vs. 3\naptitude\n39.729\n18.550\n60.909\n\n\n2 vs. 3\nmath\n13.089\n2.185\n23.993\n\n\n2 vs. 3\nlanguage\n9.147\n2.918\n15.375\n\n\n2 vs. 3\ngen_know\n-5.664\n-13.239\n1.911\n\n\n\n\n\nCode\nggplot(all_intervals,\n       aes(x = Mean_Difference, y = Variable, color = Variable)) +\n  geom_errorbar(aes(xmin = Lower_CI, xmax = Upper_CI),\n                 width = 0.2,\n                 linewidth = 1) +\n  geom_point(size = 3.5) +\n  geom_vline(xintercept = 0,\n             linetype = \"dashed\",\n             color = \"black\") +\n  facet_wrap( ~ Comparison) +\n  labs(\n    title = \"Simultaneous 95% Bonferroni CIs\",\n    subtitle = \"Intervals not crossing the dashed line (zero) are statistically significant.\",\n    x = \"Mean Difference\",\n    y = \"Measurement Variable\"\n  ) +\n  theme_bw(base_size = 12) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "ch5/05-Inference_for_Means_II.html#permulation-test",
    "href": "ch5/05-Inference_for_Means_II.html#permulation-test",
    "title": "5  Inference for Multiple Mean Vectors",
    "section": "5.3 Permulation Test",
    "text": "5.3 Permulation Test\n\n5.3.1 Introduction\nA permutation test is a type of non-parametric statistical test. It is “distribution-free,” meaning it does not rely on assumptions that the data are drawn from a given probability distribution (like the normal distribution). This makes it an incredibly robust and versatile tool for hypothesis testing.\nWhen to Use a Permutation Test:\n\nWhen your sample size is small.\nWhen your data does not meet the assumptions of parametric tests (e.g., it’s not normally distributed).\nWhen you are working with an unusual test statistic for which the theoretical distribution is unknown.\n\n\n\n5.3.2 The Steps of a Permutation Test\nEvery permutation test follows the same fundamental logic:\n\nCalculate the Observed Statistic: Compute the test statistic on your original, unshuffled data (e.g., the difference in means between two groups).\nCreate a Null Distribution:\n\nPool all the data together.\nRepeatedly (e.g., 10,000 times) shuffle the pooled data and randomly reassign it to groups of the original sizes.\nFor each shuffle, re-calculate the test statistic. The collection of these statistics forms the null distribution—the distribution of what your statistic looks like when the null hypothesis (of no effect) is true.\n\nCalculate the p-value: The p-value is the proportion of statistics from the null distribution that are as extreme or more extreme than your originally observed statistic.\n\n\n\n5.3.3 Example: Two-Sample Comparison\nBackground: We have data on the fuel efficiency (MPG) for a sample of 4-cylinder and 8-cylinder cars from the mtcars dataset. We want to test if there is a significant difference in the mean MPG between these two groups.\n\n\nR Code: Two-Sample Permutation Test\nlibrary(dplyr)\nlibrary(ggplot2)\ndata(mtcars)\n\n# Prepare the data\ncars_data &lt;- mtcars %&gt;%\n  filter(cyl %in% c(4, 8)) %&gt;%\n  dplyr::select(mpg, cyl)\n\ngroup1 &lt;- cars_data %&gt;% filter(cyl == 4) %&gt;% pull(mpg)\ngroup2 &lt;- cars_data %&gt;% filter(cyl == 8) %&gt;% pull(mpg)\nn1 &lt;- length(group1)\nn2 &lt;- length(group2)\n\n# Calculate the OBSERVED difference in means\nobserved_diff &lt;- mean(group1) - mean(group2)\n\n# Create the Null Distribution\nset.seed(4750) # For reproducibility\nn_permutations &lt;- 10000\npermutation_diffs &lt;- numeric(n_permutations)\nall_data &lt;- c(group1, group2)\n\nfor (i in 1:n_permutations) {\n  # Shuffle the data\n  shuffled_data &lt;- sample(all_data)\n  \n  # Assign to new sham groups\n  new_group1 &lt;- shuffled_data[1:n1]\n  new_group2 &lt;- shuffled_data[(n1 + 1):(n1 + n2)]\n  \n  # Calculate and store the difference for this permutation\n  permutation_diffs[i] &lt;- mean(new_group1) - mean(new_group2)\n}\n\n# Calculate the p-value\np_value &lt;- sum(abs(permutation_diffs) &gt;= \n                 abs(observed_diff)) / n_permutations\n\n\nggplot(data.frame(diffs = permutation_diffs), \n       aes(x = diffs)) +\n  geom_histogram(aes(y = ..density..), \n                 bins = 30, fill = \"lightblue\", \n                 color = \"black\") +\n  geom_density(color = \"blue\", size = 1) +\n  geom_vline(xintercept = observed_diff, \n             color = \"red\", linetype = \"dashed\", size = 1.2) +\n  annotate(\"text\", x = observed_diff - 1.5, y = 0.1, \n           label = paste(\"Observed Difference\\np-value =\", \n                         p_value), color = \"red\") +\n  labs(\n    title = \"Permutation Test for MPG Difference (4-cyl vs. 8-cyl)\",\n    x = \"Difference in Mean MPG\",\n    y = \"Density\"\n  ) +\n  theme_bw()\n\n\n\n\n\nInterpretation: The observed difference (the red dashed line) is far out in the tail of the null distribution, and the p-value is effectively zero. This tells us that it is extremely unlikely to get a difference this large by random chance alone. We can confidently conclude that 4-cylinder cars have a significantly higher mean MPG than 8-cylinder cars.\n\n\n5.3.4 Exercise: Testing a Correlation\nBackground: Is there a significant correlation between a car’s weight (wt) and its fuel efficiency (mpg)? The null hypothesis is that the true correlation is zero.\nThe permutation logic is slightly different here: if there’s no relationship between weight and MPG, then we should be able to shuffle the order of one variable without affecting the correlation.\n\n\nR Code: Correlation Permutation Test\n# Prepare the data\nwt_data &lt;- mtcars$wt\nmpg_data &lt;- mtcars$mpg\n\n# Calculate the OBSERVED correlation\nobserved_cor &lt;- cor(wt_data, mpg_data)\n\n# Create the Null Distribution\nset.seed(123)\nn_permutations &lt;- 10000\npermutation_cors &lt;- numeric(n_permutations)\n\nfor (i in 1:n_permutations) {\n  # Shuffle ONLY one of the variables\n  shuffled_mpg &lt;- sample(mpg_data)\n  \n  # Calculate and store the correlation for this permutation\n  permutation_cors[i] &lt;- cor(wt_data, shuffled_mpg)\n}\n\n# Calculate the p-value\np_value_cor &lt;- sum(abs(permutation_cors) &gt;= \n                     abs(observed_cor)) / n_permutations\n\n\nggplot(data.frame(cors = permutation_cors), aes(x = cors)) +\n  geom_histogram(aes(y = ..density..), bins = 30, \n                 fill = \"lightgreen\", color = \"black\") +\n  geom_density(color = \"darkgreen\", size = 1) +\n  geom_vline(xintercept = observed_cor, color = \"red\", \n             linetype = \"dashed\", size = 1.2) +\n  annotate(\"text\", x = observed_cor + 0.3, y = 1.5, \n           label = paste(\"Observed Correlation =\", \n                         round(observed_cor, 2), \n                         \"\\np-value =\", p_value_cor), \n           color = \"red\") +\n  labs(\n    title = \"Permutation Test for Correlation (MPG vs. Weight)\",\n    x = \"Correlation Coefficient\",\n    y = \"Density\"\n  ) +\n  theme_bw()\n\n\n\n\n\nInterpretation: The observed correlation of -0.87 is an extreme outlier compared to the null distribution of correlations centered at zero. The p-value is effectively zero. We can conclude there is a highly significant negative correlation between a car’s weight and its fuel efficiency."
  }
]