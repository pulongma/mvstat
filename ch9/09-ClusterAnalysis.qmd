---
title: 'Cluster Analysis'
author: 'Pulong Ma'
format: 
  html:
    html-math-method: katex
    include-in-header:
      - text: |
          <script>
            window.MathJax = {
              loader: {
                load: ['[tex]/amsmath', '[tex]/amssymb', '[tex]/amsfonts'],
                '[tex]/color']
              },
              tex: {
                packages: { '[+]': ['amsmath', 'amssymb', 'amsfonts', 'color'] }
              }
            };
          </script>
    css: custom.css
    toc: true
    toc_depth: 3
    number-sections: true
    code-fold: true
    callout-appearance: default
    df-print: paged
    theme: cosmo 
execute:
  echo: true
  warning: false
  message: false
---

```{r, message=FALSE,warning=FALSE, include=FALSE}
# Preload some packages
knitr::opts_chunk$set(echo = TRUE, message=FALSE)
library(tidyverse)
library(reshape2)
library(dplyr)

```


## Learning Objectives

By the end of this lecture, you should be able to:

- Explain what **cluster analysis** is and when it is used.
- Distinguish between:
  - **Hierarchical clustering**
  - **K-means clustering**
- Compute and interpret **distances** between observations.
- Read and interpret a **dendrogram**.
- Run **hierarchical** and **K-means** clustering in R.
- Use simple criteria (Calinski–Harabasz index, Gap statistic) to help choose the number of clusters.



## What is Cluster Analysis?

**Goal:** Discover reasonable **groupings** (“clusters”) of subjects or units.

Examples:

- Group medical patients by how they use different types of services.
- Group potential customers by product preferences.
- Group environmental habitats by land cover types.
- Group foods by nutrient content.

:::{.callout-note title="Unsupervised learning" collapse="true"}
Cluster analysis is an **unsupervised** learning method:

- No response variable / labels.
- We only use similarity/dissimilarity between observations.
:::


### Clustering Example: Old Faithful Geyser

- Data: eruption durations and waiting times.

```{r}
#| label: faithful-plot
#| fig-cap: "Old Faithful geyser data"
data(faithful)
plot(faithful,
     main = "How many clusters can you see?",
     pch = 19)

```

:::{.callout-note title="Interpretation" collapse="true"}
- **Interpretation**: 
  - There seems to be two groups in the dataset
  - Short waiting, short eruption
  - Long waiting, long eruption

:::

### Data for Cluster Analysis

* Big picture: 
  * $n$ individuals (cases)
  * $p$ variables (or traits) measured on each case
$$
X_{1}=\begin{pmatrix} X_{11} \\ X_{12} \\ \vdots \\ X_{1p} \end{pmatrix}, X_{2}=\begin{pmatrix} X_{21} \\ X_{22} \\ \vdots \\ X_{2p} \end{pmatrix}, \ldots, X_{n}=\begin{pmatrix} X_{n1} \\ X_{n2} \\ \vdots \\ X_{np} \end{pmatrix}
$$

* *Goal*: We want to **group** the $X_i$'s into clusters such that
  * Within a cluster: observations are **similar**.
  * Across clusters: observations are **different**. 

A crucial factor for generating useful clusters is the initial selection of variables.

### Two Key Ingredients

Most clustering methods share two core ideas: 

* **Distance (dissimilarity) between cases**
  * Euclidean distance
  * Mahalanobis distance
  * Distances based on correlations, absolute differences, etc 
* **Linkage or grouping procedure**
  * How do we define the distance between clusters of observations?
  * This drives the hierarchy in hierarchical clustering.

:::{.callout-note title="Distances" collapse="true"}

  * **Euclidean distance:** $$d(X_{i},X_{j})=\sqrt{(X_{i}-X_{j})^{\prime}(X_{i}-X_{j})}$$
  * **Statistical distance (Mahalanobis):** $$d(X_{i},X_{j})=\sqrt{(X_{i}-X_{j})^{\prime}\Sigma^{-1}(X_{i}-X_{j})}$$ 
  * **Standardized Distance:** $$d(X_{i},X_{j})=\sqrt{(X_{i}-X_{j})^{\prime}[\text{diag}(S)]^{-1}(X_{i}-X_{j})}$$ 
  * **Absolute Values (Manhattan/City-Block):** $$d(X_{i},X_{j})=\frac{1}{p}\sum_{k=1}^{p}|X_{ki}-X_{kj}|$$ 
  * **Correlation-based:** $$d(\text{item}_{i},\text{item}_{j})=1-|r_{ij}|$$ or $$d(\text{item}_{i},\text{item}_{j})=1-r_{ij}^{2}$$
  
:::




## Hierarchical Clustering

* Hierarchical clustering does not assume the number of clusters.
* Hierarchical clustering uses **distances** between observations to build
a tree representation, called a **dendrogram**. This encodes a ***hierarchy*** of clusters. 
* It is based on an **agglomerative (bottom-up)** approach: Starts with each observation as its own cluster and merges the closest pairs/groups until all are in one cluster.


### Basic Idea



::: columns
::: {.column width="40%"}

![](./figures/HC_Data.jpg){width=85%}
:::

::: {.column width="60%"}

:::{.callout-note title="A hierarchical clustering algorithm"}
* Define a distance (or dissimilarity) between clusters 
* Initialize: each cluster contains one data point
* Repeat: 
  * Compute distances between all clusters 
  * Merge two closest clusters 
* Save both clustering and sequence of cluster operations 
* This gives us a Dendrogram. 
:::

:::
:::

:::{.callout-note title="Illustration of a hierarchical clustering algorithm" collapse="true"}
![](./figures/HC.jpg){width=95%}
:::


:::{.callout-note title="Dendrogram"}

  * Vertical axis: distance between clusters.
  * Horizontal axis: observations (or variables).
  * Leaves: individual observations.
  * Internal nodes: clusters formed by merging.
  * Height of the join indicates dissimilarity.  
  * The number of clusters can be selected according to a threshold on dissimilarity. 
:::

:::{.callout-tip title="Determine the number of clusters" collapse="true"}
To determine the number of clusters, cut the dendrogram at a certain height and count the resulting branches.
:::

### Linkage Methods

Linkage determines the dissimilarity $d(C_1, C_2)$ between two clusters, $C_1$ and $C_2$. The key R function for hierarchical clustering is `hclust` in the `stats` package.

| Linkage Method | Formula | Description |
| :--- | :--- | :--- |
| **Single Linkage** (Nearest-Neighbor)  | $$d(C_{1},C_{2})=\min_{i\in C_{1},j\in C_{2}}d_{ij}$$  | Distance between closest points in the clusters. |
| **Complete Linkage** (Furthest-Neighbor)  | $$d(C_{1},C_{2})=\max_{i\in C_{1},j\in C_{2}}d_{ij}$$  | Distance between  farthest points in the clusters. |
| **Average Linkage**  | $$d(C_{1},C_{2})=\frac{1}{|C_{1}|\cdot|C_{2}|}\sum_{i\in C_{1},j\in C_{2}}d_{ij}$$ | Average of all pairwise distances. |
| **Centroid Method**  | $$d(\overline{X}_{A},\overline{X}_{B})=\bigl\|\overline{X}_{A}-\overline{X}_{B}\bigr\|^2_2$$  | Distance between centroids (means) of two clusters. |
| **Ward's Method**  | $$\Delta_{SST} = \frac{n_{A}n_{B}}{n_{A}+n_{B}}\bigl\|\overline{X}_{A}-\overline{X}_{B}\bigr\|^2_2$$  | Merges two clusters that result in the smallest increase in the total within-cluster sum of squared Euclidean distances (SST). |


```{r, echo=FALSE}
#| fig-align: center
#| fig-width: 8
#| fig-height: 6 
library(ggplot2)
library(dplyr)

set.seed(4750)

# -------------------------------------------------------------------
# Data: two clusters
# -------------------------------------------------------------------
red <- data.frame(
  x = c(1.1, 1.4, 1.8, 1.2, 1.6),
  y = c(1.0, 1.6, 1.3, 0.7, 0.9),
  cluster = "red"
)

blue <- data.frame(
  x = c(3.5, 3.8, 4.2, 3.9, 3.6),
  y = c(2.2, 2.8, 2.5, 1.9, 2.3),
  cluster = "blue"
)

df <- bind_rows(red, blue)

# Centroids
centroids <- df %>%
  group_by(cluster) %>%
  summarize(x = mean(x), y = mean(y), .groups = "drop")

# -------------------------------------------------------------------
# Pairwise distances between red and blue points
# -------------------------------------------------------------------
dist_mat  <- as.matrix(dist(df[, 1:2]))
nr        <- nrow(red)
nb        <- nrow(blue)
# distances only between red and blue rows
cross_dist <- dist_mat[1:nr, (nr + 1):(nr + nb)]

# helper: get coordinates for a (i,j) pair index
get_pair_coords <- function(i, j) {
  data.frame(
    x1 = red$x[i],
    y1 = red$y[i],
    x2 = blue$x[j],
    y2 = blue$y[j]
  )
}

# -------------------------------------------------------------------
# 1) Single linkage: nearest neighbor edge
# -------------------------------------------------------------------
idx_min   <- which(cross_dist == min(cross_dist), arr.ind = TRUE)
edge_single <- get_pair_coords(idx_min[1, 1], idx_min[1, 2])

g1 <- ggplot(df, aes(x, y, color = cluster)) +
  geom_point(size = 3) +
  geom_point(data = centroids,
             aes(x, y),
             color = c("red", "blue"),
             shape = 3, size = 5, stroke = 1.3) +
  geom_segment(data = edge_single,
               aes(x = x1, y = y1, xend = x2, yend = y2),
               color = "forestgreen", linewidth = 1.5) +
  scale_color_manual(values = c(red = "red", blue = "blue")) +
  coord_cartesian(xlim = c(0.5, 5), ylim = c(0.5, 3)) +
  theme_bw(base_size = 14) +
  theme(
    legend.position = "none",
    plot.title = element_text(size = 14, face = "bold"),
    axis.title = element_blank()
  ) +
  ggtitle("Nearest Neighbour \n(Single Linkage)")



# -------------------------------------------------------------------
# 2) Complete linkage: furthest neighbor edge
# -------------------------------------------------------------------
idx_max <- which(cross_dist == max(cross_dist), arr.ind = TRUE)
edge_complete <- get_pair_coords(idx_max[1, 1], idx_max[1, 2])

g2 <- ggplot(df, aes(x, y, color = cluster)) +
  geom_point(size = 3) +
  geom_point(data = centroids,
             aes(x, y),
             color = c("red", "blue"),
             shape = 3, size = 5, stroke = 1.3) +
  geom_segment(data = edge_complete,
               aes(x = x1, y = y1, xend = x2, yend = y2),
               color = "forestgreen", linewidth = 1.5) +
  scale_color_manual(values = c(red = "red", blue = "blue")) +
  coord_cartesian(xlim = c(0.5, 5), ylim = c(0.5, 3)) +
  theme_bw(base_size = 14) +
  theme(
    legend.position = "none",
    plot.title = element_text(size = 14, face = "bold"),
    axis.title = element_blank()
  ) +
  ggtitle("Farthest Neighbour \n(Complete Linkage)")



# -------------------------------------------------------------------
# 3) Average linkage: all cross-cluster distances shown
# -------------------------------------------------------------------
# Build all pairwise edges between red and blue
edge_average <- expand.grid(
  i = 1:nr,
  j = 1:nb
) %>%
  mutate(
    x1 = red$x[i],
    y1 = red$y[i],
    x2 = blue$x[j],
    y2 = blue$y[j]
  )

g3 <- ggplot(df, aes(x, y, color = cluster)) +
  geom_point(size = 3) +
  # draw all cross-cluster distances as thin grey segments
  geom_segment(data = edge_average,
               aes(x = x1, y = y1, xend = x2, yend = y2),
               inherit.aes = FALSE,
               color = "grey60", linewidth = 0.6) +
  geom_point(data = centroids,
             aes(x, y),
             color = c("red", "blue"),
             shape = 3, size = 5, stroke = 1.3) +
  scale_color_manual(values = c(red = "red", blue = "blue")) +
  coord_cartesian(xlim = c(0.5, 5), ylim = c(0.5, 3)) +
  theme_bw(base_size = 14) +
  theme(
    legend.position = "none",
    plot.title = element_text(size = 14, face = "bold"),
    axis.title = element_blank()
  ) +
  ggtitle("Average Linkage")


# -------------------------------------------------------------------
# 4) Centroid Method: distance between centroids
# ----------------------------------------------------------------
red_centroid  <- centroids[centroids$cluster == "red", ]
blue_centroid <- centroids[centroids$cluster == "blue", ]


edge_centroid <- data.frame(
  x1 = red_centroid$x,
  y1 = red_centroid$y,
  x2 = blue_centroid$x,
  y2 = blue_centroid$y
)

g4 <- ggplot(df, aes(x, y, color = cluster)) +
  geom_point(size = 3) +
  geom_point(data = centroids,
             aes(x, y),
             color = c("red", "blue"),
             shape = 3, size = 5, stroke = 1.3) +
   geom_segment(data = edge_centroid,
                aes(x = x1, y = y1, xend = x2, yend = y2),
                color = "forestgreen", linewidth = 1.5) +
  scale_color_manual(values = c(red = "red", blue = "blue")) +
  coord_cartesian(xlim = c(0.5, 5), ylim = c(0.5, 3)) +
  theme_bw(base_size = 14) +
  theme(
    legend.position = "none",
    plot.title = element_text(size = 14, face = "bold"),
    axis.title = element_blank()
  ) +
  ggtitle("Centroid Method")


#patchwork::wrap_plots(g1,g2,g3,g4,nrow=2)
```

```{r,echo=FALSE}
#| fig-align: center
#| fig-width: 9
#| fig-height: 6 

library(ggplot2)
library(dplyr)
library(patchwork)


within_ss <- function(dat) {
  cen <- colMeans(dat[, c("x", "y")])
  sum(rowSums( sweep(dat[, c("x","y")], 2, cen)^2 ))
}

W_red   <- within_ss(red)
W_blue  <- within_ss(blue)
W_before <- W_red + W_blue

# merged cluster
merged  <- df[, c("x","y")]
W_after <- within_ss(merged)
delta_W <- W_after - W_before

# check Ward’s formula (optional)
nA <- nrow(red); nB <- nrow(blue)
cent_diff <- as.numeric(red_centroid[ ,c("x","y")] - 
                        blue_centroid[,c("x","y")])
delta_formula <- nA * nB / (nA + nB) * sum(cent_diff^2)
#c(delta_W = delta_W, delta_formula = delta_formula)


seg_before <- df %>%
  left_join(
    centroids |> rename(cx = x, cy = y),
    by = "cluster"
  ) %>%
  transmute(x1 = cx, y1 = cy, x2 = x, y2 = y, cluster)

g_before <- ggplot(df, aes(x, y, color = cluster)) +
  geom_segment(data = seg_before,
               aes(x = x1, y = y1, xend = x2, yend = y2),
               inherit.aes = FALSE,
               color = "grey70", linewidth = 0.6) +
  geom_point(size = 3) +
  geom_point(data = centroids,
             aes(x, y),
             color = c("red","blue"),
             shape = 3, size = 5, stroke = 1.3) +
  scale_color_manual(values = c(red = "red", blue = "blue")) +
  coord_cartesian(xlim = c(0.5, 5), ylim = c(0.5, 3)) +
  theme_bw(base_size = 14) +
  theme(
    legend.position = "none",
    axis.title      = element_blank()
  ) +
  ggtitle(
    paste0("Ward's Method \n(Before merge)\n",
           "W = ", round(W_before, 2))
  )


merged_centroid <- colMeans(merged)
cent_merged_df  <- data.frame(
  x = merged_centroid[1],
  y = merged_centroid[2]
)

seg_after <- df %>%
  mutate(cx = merged_centroid[1],
         cy = merged_centroid[2]) %>%
  transmute(x1 = cx, y1 = cy, x2 = x, y2 = y, cluster)

g_after <- ggplot(df, aes(x, y, color = cluster)) +
  geom_segment(data = seg_after,
               aes(x = x1, y = y1, xend = x2, yend = y2),
               inherit.aes = FALSE,
               color = "grey70", linewidth = 0.6) +
  geom_point(size = 3) +
  geom_point(data = cent_merged_df,
             aes(x, y),
             shape = 3, size = 5, stroke = 1.3, color = "black") +
  scale_color_manual(values = c(red = "red", blue = "blue")) +
  coord_cartesian(xlim = c(0.5, 5), ylim = c(0.5, 3)) +
  theme_bw(base_size = 14) +
  theme(
    legend.position = "none",
    axis.title      = element_blank()
  ) +
  ggtitle(
    paste0("Ward's method \n(After merge)\n",
           "W = ", round(W_after, 2),
           "   ΔW = ", round(delta_W, 2))
  )


#g_ward_demo <- g_before + g_after
#g_ward_demo

patchwork::wrap_plots(g1,g2,g3,g4,g_before,g_after, nrow=2)

```


### Insect Community Data Example 

:::{.callout-note title="Insect Community Data"}
The data set (`newpbi.csv`) consist of insect counts collected on 30 Iowa prairies with 44 types of insects. Counts of these insects were taken periodically over one summer at the 30 sites. The first column (`Site`) identifies the sample location, and the second column (`Type`) is likely a categorical environmental or habitat variable. The remaining columns are abundance counts for different arthropod groups (e.g., Araneae, ACari).   

```{r}
#| code-summary: "R Code: Load Data"
df = read.csv("newpbi.csv")
head(df)
```

:::

:::{.callout-note title="Research Questions" collapse="true"}
* Community Structure 
  * Question 1: Do distinct communities exist in the sampled areas, and if so, how many? This can discover if the sites fall into natural groupings based purely on the species/taxa they contain. This could reveal previously unrecognized ecological zones or biotypes
  * Question 2: Are the natural community clusters consistent with the recorded habitat types (`Type`)?
* Taxonomic and Functional Groups
  * Question 3: Which specific arthropod taxa (e.g., specific beetle or fly families) tend to co-occur across sites?
  
:::




```{r}
#| code-summary: "R Code: Visualize the data"
#| code-fold: true

abundance_matrix = as.matrix(df[,-c(1:2)])

heatmap(abundance_matrix, col=cm.colors(256), 
               Rowv=NA, Colv=NA) 

```

* A heatmap displays:
  * Observations (or variables) on rows.
  * Variables (or observations) on columns.
  * Color intensity = value size.


:::{.callout-note title="heatmap" collapse="true"}

```{}
heatmap(x, Rowv = NULL, Colv = if(symm)"Rowv" else NULL,
        distfun = dist, hclustfun = hclust,
        reorderfun = function(d, w) reorder(d, w),
        add.expr, symm = FALSE, revC = identical(Colv, "Rowv"),
        scale = c("row", "column", "none"), na.rm = TRUE,
        margins = c(5, 5), ColSideColors, RowSideColors,
        cexRow = 0.2 + 1/log10(nr), cexCol = 0.2 + 1/log10(nc),
        labRow = NULL, labCol = NULL, main = NULL,
        xlab = NULL, ylab = NULL,
        keep.dendro = FALSE, verbose = getOption("verbose"), ...)
```

* By default, `heatmap` performs clustering for both rows and columns based on the clustering function `hclust`. Unless we know the default clustering uses an appropriate distance/dissimilarity metric, we can use the clustered heatmap for interpretation. 

:::




```{r}
#| code-summary: "R Code: Calculate Bray-Curtis dissimilarity via `vegdist`"
#| code-fold: true 
library(vegan)
D = vegdist(abundance_matrix, method = "bray")
```

```{r}
#| code-summary: "R Code: Hierarchical Clustering via `hclust`"
#| code-fold: true
hca1 = hclust(D, method="single")
hca2 = hclust(D, method="complete")
hca3 = hclust(D, method="average")
hca4 = hclust(D, method="ward.D2")

```

```{r}
#| code-fold: true
#| code-summary: "R Code: Dendrograms"
#| fig-width: 10
#| fig-height: 8
#| fig-align: "center"
 
library(factoextra)
p1=fviz_dend(hca1)
p2=fviz_dend(hca2)
p3=fviz_dend(hca3)
p4=fviz_dend(hca4)

patchwork::wrap_plots(p1 + ggtitle("single"),
                      p2 + ggtitle("complete"),
                      p3 + ggtitle("average"),
                      p4 + ggtitle("ward"),ncol=2)
```

* **Interpretation**: The height at which two branches join represents the dissimilarity (Bray-Curtis distance) between them. Short branches that merge low down indicate highly similar communities.
  

## K-Mean Clustering 

* K-means clustering is a type non-hierarchical clustering algorithm

* No tree structure exists

* One often needs to pre-determine the number of clusters in the algorithm to find the clusters

### Basic Idea


::: columns
::: {.column width="60%" height="90%"}

:::{.callout-note title="A K-means clustering algorithm"}
* **Initialize**: Pick $K$ points randomly as cluster centers.  
* Repeat: 
  * Assign points to closet cluster center 
  * Update cluster center location to the mean of the assigned points 
* Stop when no points change cluster assignment (convergence) 
:::

:::

::: {.column width="40%" height="80%"}

![](./figures/K-means0.png){width=80% height=60%}


:::
:::



:::{.callout-note title="Illustration of K-means clustering" collapse="true"}

:::{.callout-note title="Iteration 1" collapse="true"}
![](./figures/K-means1.png){width=95%}
:::

:::{.callout-note title="Iteration 2" collapse="true"}
![](./figures/K-means1.png){width=95%}
:::

:::{.callout-note title="Iteration 3" collapse="true"}
![](./figures/K-means1.png){width=95%}
:::

::: 


### A Toy Example 

This small data set has only four cases (A, B, C, and D) and two variables are measured on each case. The data vectors are: 
$$A=\begin{pmatrix}5\\ 3\end{pmatrix} \quad B=\begin{pmatrix}-1\\ 1\end{pmatrix} \quad C=\begin{pmatrix}1\\ -2\end{pmatrix} \quad D=\begin{pmatrix}-3\\ -2\end{pmatrix} $$

Start with initial clusters $\mathbf{(A, B)}$ and $\mathbf{(C, D)}$. The centroids for the initial clusters are
$$C_{1}=\begin{pmatrix}2\\ 2\end{pmatrix} \quad \text{and} \quad C_{2}=\begin{pmatrix}-1\\ -2\end{pmatrix}
$$

:::{.callout-note title="Step-by-step K-means iteration" collapse="true"}

In each iteration, we need to compute the distance of each case from each centroid. Here we use Euclidean distance. Then we update cluster centroids.

:::{.callout-note title="Iteration 1" collapse="true"}
* distances: 
  * **Case A**:  $d(A,C_{1})=\sqrt{(5-2)^{2}+(3-2)^{2}}=\sqrt{10}$, and 
$d(A,C_{2})=\sqrt{(5-(-1))^{2}+(3-(-2))^{2}}=\sqrt{61}$

  * **Case B**: 
  $d(B,C_{1})=\sqrt{(-1-2)^{2}+(1-2)^{2}}=\sqrt{10}$, and $d(B,C_{2})=\sqrt{(-1-(-1))^{2}+(1-(-2))^{2}}=\sqrt{9}$
  * **Case C**:
   $d(C,C_{1})=\sqrt{(1-2)^{2}+(-2-2)^{2}}=\sqrt{17}$, and $d(C,C_{2})=\sqrt{(1-(-1))^{2}+(-2-(-2))^{2}}=\sqrt{4}$
  * **Case D**: 
  $d(D,C_{1})=\sqrt{(-3-2)^{2}+(-2-2)^{2}}=\sqrt{41}$, and $d(D,C_{2})=\sqrt{(-3-(-1))^{2}+(-2-(-2))^{2}}=\sqrt{4}$


* The **new clusters** are $\mathbf{(A)}$ and $\mathbf{(B, C, D)}$
  * The new centroids are: $$C_{1}=\begin{pmatrix}5\\ 3\end{pmatrix} \quad \text{and} \quad C_{2}=\begin{pmatrix}-1\\ -1\end{pmatrix} $$

* Check convergence: for example, specify a tolerance for the difference between two consecutive centroids. 

:::


:::{.callout-note title="Iteration 2" collapse="true"}
* Distances: 
  * **Case A**: $d(A,C_{1})=\sqrt{(5-5)^{2}+(3-3)^{2}}=\sqrt{0}$ and $d(A,C_{2})=\sqrt{(5-(-1))^{2}+(3-(-1))^{2}}=\sqrt{52}$
  
  * **Case B**:
  $d(B,C_{1})=\sqrt{(-1-5)^{2}+(1-3)^{2}}=\sqrt{40}$ and $d(B,C_{2})=\sqrt{(-1-(-1))^{2}+(1-(-1))^{2}}=\sqrt{4}$
  * **Case C**:
  $d(C,C_{1})=\sqrt{(1-5)^{2}+(-2-3)^{2}}=\sqrt{41}$ and $d(C,C_{2})=\sqrt{(1-(-1))^{2}+(-2-(-1))^{2}}=\sqrt{5}$ 
  * **Case D**:
  $d(D,C_{1})=\sqrt{(-3-5)^{2}+(-2-3)^{2}}=\sqrt{89}$ and $d(D,C_{2})=\sqrt{(-3-(-1))^{2}+(-2-(-1))^{2}}=\sqrt{5}$ 
  * The new clusters are $\mathbf{(A)}$ and $\mathbf{(B, C, D)}$ and the clusters did not change: 
$$C\_{1}=\begin{pmatrix}5\\ 3\end{pmatrix} \quad \text{and} \quad C\_{2}=\begin{pmatrix}-1\\ -1\end{pmatrix}
$$

* **Stop**: The K-means algorithm has converged to produce clusters $\mathbf{(A)}$ and $\mathbf{(B, C, D)}$ 

::: 



::: 



### How to Choose the Number of Clusters?

:::{.callout-note title="Review of notations" collapse="true"}
*  $n$ is the number of observations in the data.
*  $p$ is the number of variables measured on each observation.
* $C_k$ represents the $k$-th cluster at a particular step of the 
 clustering algorithm and $n_k$ is the number of observations in $C_k$.
*  $SS_{T}$ is the  total sum of squared distances from the overall vector of means for all $n$ observations in the data set:
$$
  SS_T = \sum_{i=1}^n (\mathbf{x}_i-\bar{\mathbf{x}})^{\prime}  (\mathbf{x}_i-\bar{\mathbf{x}})
$$

:::

:::{.callout-note title="Cluster validation metrics" collapse="true"}

We define the components used in cluster validation metrics:

* ${G}$: The number of groups or clusters. The goal of these methods is to find the optimal ${G}$.
* $\mathbf{w}_k$: The sum of squared distances for all points within a single cluster, $C_k$.
$$
\mathbf{w}_k:= \sum_{i \text{ in } C_k} (\mathbf{x}_i-\bar{\mathbf{x}}_k)^{\prime}  (\mathbf{x}_i-\bar{\mathbf{x}}_k)
$$
$$
* $\mathbf{W}_G$: The total sum of squared distances across all $\mathbf{G}$ clusters. This is the **Within-Group Sum of Squares (WCSS)**:
    $$
    \mathbf{W}_G = \sum_{k=1}^{G} \mathbf{w_k}
    $$
* $\mathbf{B}_G$: The **Between-Group Sum of Squares (BCSS)**, which measures the separation of the group centroids. It is defined based on the Total Sum of Squares ($\mathbf{SS_T}$):
    $$
    \mathbf{B}_G = \mathbf{SS}_T - \mathbf{W}_G
    $$
::: 


:::{.callout-note title="Comparative methods" collapse="true"}
| Method | Core Concept | Optimal $\mathbf{G}$ Rule | Mathematical Metric |
| :--- | :--- | :--- | :--- |
| **Elbow (WCSS)** | Measures total **compactness** ($\mathbf{W_G}$) as $\mathbf{G}$ increases. | Choose the $\mathbf{G}$ where the decrease in $\mathbf{W_G}$ starts to slow significantly (the "bend"). | Plot $\mathbf{W_G}$ vs. $\mathbf{G}$ |
| **Calinski-Harabasz Index** | Maximizes the ratio of **separation** ($\mathbf{B_G}$) to **compactness** ($\mathbf{W_G}$). | Choose the $\mathbf{G}$ that **maximizes** the CH index (highest peak). | $$\text{CH}(G) = \frac{\mathbf{B_G}}{\mathbf{W_G}} \cdot \frac{n-G}{G-1}$$ |
| **Gap Statistic** | Compares the observed compactness ($\mathbf{W_G}$) to the expected compactness ($\mathbf{W_G^*}$) under a random **null distribution**. | Choose the smallest $\mathbf{G}$ that **maximizes** the Gap statistic. | $$\text{Gap}(G) = E_{n}^{*} \{\log(W_{G}^{*})\} - \log(\mathbf{W_G})$$ |

::: 


### Working Example 

```{r}
#| code-summary: "`USArests` data"
library(factoextra)
library(cluster)
#load data
df <- USArrests

#remove rows with missing values
df <- na.omit(df)

df <- scale(df)

head(df)
```

```{r}
#| code-summary: "Elbow method"

fviz_nbclust(df, kmeans, method = "wss")

```

```{r}
#| code-summary: "Gap statistic"

fviz_nbclust(df, kmeans, method = "gap_stat")

```


```{r}
#| code-summary: "K-mean clustering"

set.seed(4750)

#perform k-means clustering with k = 4 clusters
km = kmeans(df, 
     centers = 4, 
     nstart = 25 # choose 25 initial centers
     )

print(km)

```


```{r}
#| code-summary: "Visualiazing clusters"
fviz_cluster(km, data = df)

```

```{r}
#| code-summary: "Compute cluster means"
aggregate(USArrests, by=list(cluster=km$cluster), mean)
```

```{r}
#add cluster assigment to original data
final_data = cbind(USArrests, cluster = km$cluster)

#view final data
head(final_data)
```
